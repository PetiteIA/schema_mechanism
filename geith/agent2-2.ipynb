{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eebb8ac-264e-4c67-92bf-320effd22e69",
   "metadata": {},
   "source": [
    "# AGENT2: L'agent qui simulait longtemps (en construction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48264d7-4120-4cb6-904a-1d7f34f56e26",
   "metadata": {},
   "source": [
    "Ce notebook présente un agent qui simule plusieurs cycle à l'avance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b81b5d6-8d74-448c-8dda-c4606449d209",
   "metadata": {},
   "source": [
    "# L'environnement Small Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49339f-e86c-4e5b-b1d1-7c0cf388e43d",
   "metadata": {},
   "source": [
    "Ajoutons la méthode `clone()` pour permettre de faire plusieurs simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "656da749-2ac1-4a25-83f3-f8a2cdc38b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"sav\"\n",
    "\n",
    "FORWARD = 0\n",
    "FEEL_FRONT = 1\n",
    "FEEL_LEFT = 2    # Not used\n",
    "FEEL_RIGHT = 3   # Not used\n",
    "TURN_LEFT = 4  # Turn 180°\n",
    "TURN_RIGHT = 5   # Not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "58ecd7a0-317a-4032-8b5d-fa0321f6432a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from ipywidgets import Button, HBox,VBox, Output\n",
    "from IPython.display import display\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "EMPTY = 0\n",
    "WALL = 1\n",
    "FEELING_EMPTY = 2\n",
    "FEELING_WALL = 3\n",
    "BUMPING = 4\n",
    "\n",
    "colors = [\"#b0b0b0\", '#b0b0b0', '#ffffff', '#535865', \"#F93943\"]  # Hidden environment\n",
    "colors = [\"#D6D6D6\", '#5C946E', '#FAE2DB', '#535865', \"#F93943\", \n",
    "          \"#eeeeee\", \"#85A48F\", '#FAE2DB', '#535865', \"#F93943\"]  # Simulator\n",
    "agent_color = \"#1976D2\"\n",
    "\n",
    "class SmallLoop():\n",
    "    def __init__(self, position, direction):\n",
    "        self.environment_grid = np.array([[WALL, EMPTY, EMPTY, EMPTY, WALL]])\n",
    "        self.display_grid = np.zeros((2, self.environment_grid.shape[1]), dtype=int)\n",
    "        self.display_grid[0, :] = self.environment_grid[0, :]\n",
    "        self.position = np.array(position) \n",
    "        self.direction = direction\n",
    "        self.cmap = ListedColormap(colors)\n",
    "        # self.norm = BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5], self.cmap.N)\n",
    "        self.norm = BoundaryNorm(np.arange(-0.5, len(colors) + 0.5, 1.0), self.cmap.N)\n",
    "        self.marker_size = 400\n",
    "        self.marker_map = {LEFT: '<', DOWN: 'v', RIGHT: '>', UP: '^'}\n",
    "        self.marker_color = agent_color\n",
    "        self.directions = np.array([\n",
    "            [0, -1],  # Left\n",
    "            [1, 0],   # Down\n",
    "            [0, 1],   # Right\n",
    "            [-1, 0]   # Up\n",
    "            ])\n",
    "\n",
    "    def outcome(self, action):\n",
    "        \"\"\"Update the grid. Return the outcome of the action.\"\"\"\n",
    "        result = 0\n",
    "        # x, y = self.position\n",
    "\n",
    "        if action == FORWARD:  \n",
    "            target_position = self.position + self.directions[self.direction]\n",
    "            if self.environment_grid[tuple(target_position)] == EMPTY:\n",
    "                self.position[:] = target_position\n",
    "            else:\n",
    "                result = 1\n",
    "                self.display_grid[tuple(target_position)] = BUMPING\n",
    "        \n",
    "        elif action == TURN_RIGHT:\n",
    "            self.direction = {LEFT: UP, DOWN: LEFT, RIGHT: DOWN, UP: RIGHT}[self.direction]\n",
    "        \n",
    "        elif action == TURN_LEFT:\n",
    "            # self.direction = {LEFT: DOWN, DOWN: RIGHT, RIGHT: UP, UP: LEFT}[self.direction]\n",
    "            self.direction = {LEFT: RIGHT, DOWN: UP, RIGHT: LEFT, UP: DOWN}[self.direction]\n",
    "        \n",
    "        elif action == FEEL_FRONT:\n",
    "            feeling_position = self.position + self.directions[self.direction]\n",
    "            if self.environment_grid[tuple(feeling_position)] == EMPTY:\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                result = 1\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_WALL\n",
    "        \n",
    "        elif action == FEEL_LEFT:\n",
    "            feeling_position = self.position + self.directions[(self.direction + 1) % 4]\n",
    "            if self.environment_grid[tuple(feeling_position)] == EMPTY:\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                result = 1\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_WALL\n",
    "        \n",
    "        elif action == FEEL_RIGHT:\n",
    "            feeling_position = self.position + self.directions[self.direction - 1]\n",
    "            if self.environment_grid[tuple(feeling_position)] == EMPTY:\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                result = 1\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_WALL\n",
    "\n",
    "        # print(f\"Line: {self.position[0]}, Column: {self.position[1]}, direction: {self.direction}\")\n",
    "        return result  \n",
    "    \n",
    "    def display(self, simulator=None):\n",
    "        \"\"\"Display the grid in the notebook\"\"\"\n",
    "        out.clear_output(wait=True)\n",
    "        with out:\n",
    "            fig, ax = plt.subplots()\n",
    "            if simulator is not None:\n",
    "                plt.scatter(simulator.position[1], 1, s=self.marker_size, marker=self.marker_map[simulator.direction], c=\"#aaaaaa\")\n",
    "                self.display_grid[1, :] = simulator.display_grid[0, :] + 5\n",
    "            ax.imshow(self.display_grid, cmap=self.cmap, norm=self.norm)\n",
    "            plt.scatter(self.position[1], self.position[0], s=self.marker_size, marker=self.marker_map[self.direction], c=self.marker_color)\n",
    "            ax.text(4.5, 0, f\"{step:>3}\", fontsize=12, color='White')\n",
    "            plt.show()\n",
    "    \n",
    "    def save(self, step):\n",
    "        \"\"\"Save the display as a PNG file\"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.axis('off')\n",
    "        ax.imshow(self.display_grid, cmap=self.cmap, norm=self.norm)\n",
    "        plt.scatter(self.position[1], self.position[0], s=self.marker_size, marker=self.marker_map[self.direction], c=self.marker_color)\n",
    "        ax.text(4.5, 0, f\"{step:>4}\", fontsize=12, color='White')\n",
    "        plt.savefig(f\"{save_dir}/{step:04}.png\", bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    def clear(self, clear):\n",
    "        \"\"\"Clear the grid display\"\"\"\n",
    "        if clear:\n",
    "            self.display_grid[0, :] = self.environment_grid\n",
    "\n",
    "    def clone(self):\n",
    "        \"\"\"Return a copy of the environment in its current state\"\"\"\n",
    "        return SmallLoop(self.position, self.direction)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db93a7-8c36-4a14-8d83-1fd97777f6d9",
   "metadata": {},
   "source": [
    "# L'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bb1dd589-53a3-4793-9bbf-4a02d2bd1b36",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, action, outcome, valence):\n",
    "        self.action = action\n",
    "        self.outcome = outcome\n",
    "        self.valence = valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self.action}{self.outcome}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self.action}{self.outcome}:{self.valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        return self.key() == other.key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125dea66-b78a-44d9-b4d7-a0e0c6f8f191",
   "metadata": {},
   "source": [
    "L'agent est instancié avec un simularteur interne qui lui est passé en argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8f99d491-f59d-42ff-a068-853e32649bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Creating our agent\"\"\"\n",
    "    def __init__(self, interactions, simulator):\n",
    "        \"\"\" Initialize the dictionary of interactions\"\"\"\n",
    "        self._interactions = {interaction.key(): interaction for interaction in interactions}\n",
    "        self._intended_interaction = self._interactions[\"00\"]\n",
    "        self._actions = [i.action for i in interactions if i.outcome == 0]\n",
    "        self.action_df = pd.DataFrame({\"action\": [i.action for i in interactions if i.outcome == 0]}) # , columns=['action', 'outcome', 'valence'])\n",
    "        self._simulator = simulator\n",
    "\n",
    "    def select_action(self): \n",
    "        \"\"\"Select the action that yeilds the highest valence\"\"\"\n",
    "        # Roll the actions to try different actions when all outcome are equal\n",
    "        self.action_df = pd.concat([self.action_df.tail(1), self.action_df.head(len(self.action_df) - 1)], ignore_index=True)\n",
    "        # self.action_df = pd.concat([self.action_df.iloc[1:], self.action_df.iloc[[0]]], ignore_index=True)\n",
    "        # Try every action in a clone of the simulator\n",
    "        self.action_df[\"outcome\"] = self.action_df.apply(lambda row: self._simulator.clone().outcome(row[\"action\"]), axis=1)\n",
    "        # Record the expected valence for each resulting interaction\n",
    "        self.action_df[\"valence\"] = self.action_df.apply(lambda row: self._interactions[f\"{row[\"action\"]}{row[\"outcome\"]}\"].valence, axis=1)\n",
    "        # Sort by descending valence\n",
    "        self.action_df = self.action_df.sort_values(by=['valence'], ascending=[False]).reset_index(drop=True)\n",
    "        # print(self.action_df)\n",
    "        # Return the action that yeilds the highest valence\n",
    "        return self.action_df.loc[0, \"action\"]\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\" Tracing the previous cycle \"\"\"\n",
    "        previous_interaction = self._interactions[f\"{self._intended_interaction.action}{_outcome}\"]\n",
    "        print(f\"Action: {self._intended_interaction.action}, Prediction: {self._intended_interaction.outcome}, Outcome: {_outcome}, \" \n",
    "              f\"Prediction: {self._intended_interaction.outcome == _outcome}, Valence: {previous_interaction.valence})\")\n",
    "\n",
    "        \"\"\" Computing the next interaction to try to enact \"\"\"\n",
    "        # Select the next action\n",
    "        intended_action = self.select_action()\n",
    "        \n",
    "        # Predict the outcome based on simulation\n",
    "        intended_outcome = self._simulator.outcome(intended_action)\n",
    "        # Memorize the intended interaction\n",
    "        self._intended_interaction = self._interactions[f\"{intended_action}{intended_outcome}\"]\n",
    "        return intended_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc3d53-873c-41a7-b359-6acef6d6bcb5",
   "metadata": {},
   "source": [
    "# Run the simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed57f2f-1337-4821-8c93-9f9a05e126d2",
   "metadata": {},
   "source": [
    "Nous utilisons un instance de l'environnement lui même comme simulateur passé à l'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fbad721a-46f2-412b-8dd6-7ee0e18e8cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f364891f9dc54bd9a7115b165947b4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciate the small loop environment\n",
    "e = SmallLoop([0, 1], 0)\n",
    "\n",
    "# Instanciate the agent \n",
    "interactions = [\n",
    "    Interaction(FORWARD, 0, 5),\n",
    "    Interaction(FORWARD, 1, -10),\n",
    "    Interaction(FEEL_FRONT, 0, -1),\n",
    "    Interaction(FEEL_FRONT, 1, -1),\n",
    "    Interaction(TURN_LEFT, 0, -3),\n",
    "    Interaction(TURN_LEFT, 1, -3)\n",
    "]\n",
    "simulator = SmallLoop([0, 1], 0)\n",
    "a = Agent(interactions, simulator)\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0\n",
    "\n",
    "# Display\n",
    "out = Output()\n",
    "e.display()\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9b187590-6d57-42ba-966f-cce41f56e933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>outcome</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action  outcome  valence\n",
       "0       1        1       -1\n",
       "1       4        0       -3\n",
       "2       0        1      -10"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "action = a.action(outcome)\n",
    "e.display()\n",
    "outcome = e.outcome(action)\n",
    "a.action_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f477f-aa9a-4742-9f35-413638cafd9b",
   "metadata": {},
   "source": [
    "L'agent sélectionne toujours l'action `feel` car c'est celle qui produit l'interaction qui a la valence la moins basse (-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c88b4-879f-432d-ad25-b4e19e2ab778",
   "metadata": {},
   "source": [
    "# Agent2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d0e6e-9586-4749-a791-d89aca260fdd",
   "metadata": {},
   "source": [
    "Implémentons l'Agent2 qui simule plusieurs coups à l'avance.\n",
    "\n",
    "On passe l'environnement à l'agent pour pouvoir visualiser la simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "69036914-5f6d-42bb-94a0-61db733c5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class Agent2(Agent):\n",
    "    def __init__(self, interactions, simulator):\n",
    "        super().__init__(interactions, simulator)\n",
    "        self.simulation = simulator  # Used to display the current state of the simulation\n",
    "        pairs = list(itertools.product(self._actions, repeat=2))\n",
    "        self.default_df = pd.DataFrame(pairs, columns=['action1', 'action2'])\n",
    "        self.default_df['outcome1'] = 0\n",
    "        self.default_df['step'] = 0\n",
    "        self.default_df['outcome2'] = 0\n",
    "        self.default_df['valence'] = 0\n",
    "        self.action_df = self.default_df.copy()\n",
    "        self.selection_df = None\n",
    "        self.simulation_step = 0\n",
    "        self._s1 = None\n",
    "        self._valence1 = 0\n",
    "\n",
    "    def simulate(self): \n",
    "        \"\"\"Compute the next row in action_df. Return True during the simulation and False when all the rows are computed\"\"\"\n",
    "        # Simulate two steps ahead\n",
    "        a1, step, a2 = self.action_df.loc[self.simulation_step, ['action1', 'step', 'action2']]\n",
    "        if step == 0:\n",
    "            self._s1 = self._simulator.clone()\n",
    "            o1 = self._s1.outcome(a1)\n",
    "            self.action_df.loc[self.action_df['action1'] == a1, 'outcome1'] = o1\n",
    "            self.simulation = self._s1\n",
    "            self.action_df.loc[self.simulation_step, 'step'] = 1\n",
    "            self._valence1 = self._interactions[f\"{a1}{o1}\"].valence\n",
    "            # print(self.action_df)\n",
    "            return True\n",
    "        else:\n",
    "            s2 = self._s1.clone()\n",
    "            o2 = s2.outcome(a2)\n",
    "            valence = self._valence1 + self._interactions[f\"{a2}{o2}\"].valence\n",
    "            self.action_df.loc[(self.action_df['action1'] == a1) & (self.action_df['action2'] == a2), ['outcome2', 'valence']] = [o2, valence]\n",
    "            # self.action_df.loc[(self.action_df['action1'] == a1) & (self.action_df['action2'] == a2), 'valence'] = valence\n",
    "            self.simulation = s2\n",
    "            self.simulation_step += 1\n",
    "            # print(self.action_df)\n",
    "            return self.simulation_step < len(self.action_df)\n",
    "\n",
    "    def update(self, outcome):\n",
    "        \"\"\" Tracing the previous cycle \"\"\"\n",
    "        previous_interaction = self._interactions[f\"{self._intended_interaction.action}{outcome}\"]\n",
    "        print(f\"Action: {self._intended_interaction.action}, Prediction: {self._intended_interaction.outcome}, Outcome: {outcome}, \" \n",
    "              f\"Prediction: {self._intended_interaction.outcome == outcome}, Valence: {previous_interaction.valence})\")\n",
    "        # Prepare the next simulation\n",
    "        self.simulation_step = 0\n",
    "        self.action_df = self.default_df.copy()\n",
    "\n",
    "    def action(self):\n",
    "        \"\"\" Select the next interaction to try to enact \"\"\"\n",
    "        # Sort by descending valence\n",
    "        self.selection_df = self.action_df.sort_values(by=['valence'], ascending=[False]).reset_index(drop=True)\n",
    "        # Return the action that yeilds the highest valence\n",
    "        intended_action, intended_outcome = self.selection_df.loc[0, ['action1', 'outcome1']]\n",
    "        self._intended_interaction = self._interactions[f\"{intended_action}{intended_outcome}\"]\n",
    "        # Update the simulator\n",
    "        self._simulator.outcome(intended_action)\n",
    "        self.simulation = self._simulator\n",
    "        return self._intended_interaction.action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c6dd1-257c-44b8-9581-3c9b6df95b6e",
   "metadata": {},
   "source": [
    "## Testons l'Agent2 en lui passant le Small Loop comme simulateur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc605d-cfb0-4aa9-b78b-fec49063409d",
   "metadata": {},
   "source": [
    "Le simulateur est initialisée avec une position différente de celle de l'environnement. \n",
    "Le simulateur se synchronise rapidement avec l'environnement quand l'agent est bloqué par un mur. \n",
    "\n",
    "La ligne du haut de l'affichage représent l'environnement. La ligne du bas représente la simulation que l'agent fait avant chaque décision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6f6b5c27-ddca-4f4a-98a3-0603e05fb3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd42014658e64f96ad2aa664a3ca37b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciate the small loop environment\n",
    "e = SmallLoop([0, 1], 0)\n",
    "\n",
    "# Instanciate the agent \n",
    "interactions = [\n",
    "    Interaction(FORWARD, 0, 5),\n",
    "    Interaction(FORWARD, 1, -10),\n",
    "    Interaction(FEEL_FRONT, 0, -1),\n",
    "    Interaction(FEEL_FRONT, 1, -1),\n",
    "    Interaction(TURN_LEFT, 0, -3),\n",
    "    Interaction(TURN_LEFT, 1, -3)\n",
    "]\n",
    "simulator = SmallLoop([0, 2], 0)\n",
    "agent = Agent2(interactions, simulator)\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0\n",
    "\n",
    "# Display\n",
    "out = Output()\n",
    "e.display(agent.simulation)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ffaf9a17-b16b-421a-b23c-e394343641a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5\n",
      "Action: 4, Prediction: 0, Outcome: 0, Prediction: True, Valence: -3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action1</th>\n",
       "      <th>action2</th>\n",
       "      <th>outcome1</th>\n",
       "      <th>step</th>\n",
       "      <th>outcome2</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action1  action2  outcome1  step  outcome2  valence\n",
       "0        4        0         0     1         0        2\n",
       "1        1        1         1     1         1       -2\n",
       "2        1        4         1     1         0       -4\n",
       "3        4        1         0     1         0       -4\n",
       "4        4        4         0     1         0       -6\n",
       "5        1        0         1     1         1      -11\n",
       "6        0        1         1     1         1      -11\n",
       "7        0        4         1     1         0      -13\n",
       "8        0        0         1     1         1      -20"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step += 1\n",
    "print(f\"Step {step}\")\n",
    "while agent.simulate():\n",
    "    e.display(agent.simulation)\n",
    "action = agent.action()\n",
    "outcome = e.outcome(action)\n",
    "e.display(agent.simulation)\n",
    "e.clear(True)\n",
    "agent.update(outcome)\n",
    "agent.selection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c8070-8b6c-4f23-9cde-0bc692d44177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e78f3-fc82-482a-8267-f1fab583be7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyton 3.12",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
