{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eebb8ac-264e-4c67-92bf-320effd22e69",
   "metadata": {},
   "source": [
    "# AGENT1: l'agent qui simulait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48264d7-4120-4cb6-904a-1d7f34f56e26",
   "metadata": {},
   "source": [
    "Ce notebook présente un agent qui prend une décision basée sur une simulation interne de ses actions.\n",
    "\n",
    "C'est le premier agent Sartrien qui une mémoire binaire: l'être ou le néant :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b81b5d6-8d74-448c-8dda-c4606449d209",
   "metadata": {},
   "source": [
    "# Environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7b081-ec11-465e-b2e8-23f5cf01a50c",
   "metadata": {},
   "source": [
    "* L'action `set` mets passe l'état de l'environnement à `1` et renvoie outcome `0`\n",
    "* L'action `reset` mets passe l'état de l'environnement à `0` et renvoie outcome `0`\n",
    "* L'action `feel` renvoie un outcome correspondant à l'état de l'environnement\n",
    "\n",
    "L'environnement possède une méthode `clone()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58ecd7a0-317a-4032-8b5d-fa0321f6432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SET = 0\n",
    "ACTION_RESET = 1\n",
    "ACTION_FEEL = 2\n",
    "OUTCOME_EMPTY = 0\n",
    "OUTCOME_FULL = 1\n",
    "\n",
    "class Environment1:\n",
    "    def __init__(self, state=0):\n",
    "        self.state = state\n",
    "    def outcome(self, action):\n",
    "        if action == ACTION_FEEL:\n",
    "            outcome = self.state\n",
    "        else:\n",
    "            if action == ACTION_SET:\n",
    "                self.state = 1\n",
    "            else:\n",
    "                self.state = 0\n",
    "            outcome = 0\n",
    "        return outcome\n",
    "    def clone(self):\n",
    "        return Environment1(self.state)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db93a7-8c36-4a14-8d83-1fd97777f6d9",
   "metadata": {},
   "source": [
    "# L'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb1dd589-53a3-4793-9bbf-4a02d2bd1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, action, outcome, valence):\n",
    "        self.action = action\n",
    "        self.outcome = outcome\n",
    "        self.valence = valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self.action}{self.outcome}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self.action}{self.outcome}:{self.valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        return self.key() == other.key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125dea66-b78a-44d9-b4d7-a0e0c6f8f191",
   "metadata": {},
   "source": [
    "L'agent est instancié avec un simularteur interne qui lui est passé en argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f99d491-f59d-42ff-a068-853e32649bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Creating our agent\"\"\"\n",
    "    def __init__(self, _interactions, simulator):\n",
    "        \"\"\" Initialize the dictionary of interactions\"\"\"\n",
    "        self._interactions = {interaction.key(): interaction for interaction in _interactions}\n",
    "        self._intended_interaction = self._interactions[\"00\"]\n",
    "        self.memory = 0\n",
    "        self.action_df = pd.DataFrame({\"action\": [i.action for i in _interactions if i.outcome == OUTCOME_EMPTY]}) # , columns=['action', 'outcome', 'valence'])\n",
    "        self.simulator = simulator\n",
    "\n",
    "    def select_action(self):\n",
    "        \"\"\"Select the next action\"\"\"\n",
    "        # Roll the actions\n",
    "        self.action_df = pd.concat([self.action_df.tail(1), self.action_df.head(len(self.action_df) - 1)], ignore_index=True)\n",
    "        # self.action_df = pd.concat([self.action_df.iloc[1:], self.action_df.iloc[[0]]], ignore_index=True)\n",
    "        # Select the first action in the action_df\n",
    "        return self.action_df.loc[0, \"action\"]\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\" Tracing the previous cycle \"\"\"\n",
    "        previous_interaction = self._interactions[f\"{self._intended_interaction.action}{_outcome}\"]\n",
    "        print(f\"Action: {self._intended_interaction.action}, Prediction: {self._intended_interaction.outcome}, Outcome: {_outcome}, \" \n",
    "              f\"Prediction: {self._intended_interaction.outcome == _outcome}, Valence: {previous_interaction.valence})\")\n",
    "\n",
    "        \"\"\" Computing the next interaction to try to enact \"\"\"\n",
    "        # Select the next action\n",
    "        intended_action = self.select_action()\n",
    "        \n",
    "        # Predict the outcome based on simulation\n",
    "        intended_outcome = self.simulator.outcome(intended_action)\n",
    "        # Memorize the intended interaction\n",
    "        self._intended_interaction = self._interactions[f\"{intended_action}{intended_outcome}\"]\n",
    "        return intended_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc3d53-873c-41a7-b359-6acef6d6bcb5",
   "metadata": {},
   "source": [
    "# Run the simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed57f2f-1337-4821-8c93-9f9a05e126d2",
   "metadata": {},
   "source": [
    "Nous utilisons un instance de l'environnement lui même comme simulateur passé à l'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fbad721a-46f2-412b-8dd6-7ee0e18e8cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n"
     ]
    }
   ],
   "source": [
    "interactions = [\n",
    "    Interaction(ACTION_SET,OUTCOME_EMPTY,-1),\n",
    "    Interaction(ACTION_SET,OUTCOME_FULL,1),\n",
    "    Interaction(ACTION_RESET,OUTCOME_EMPTY,-1),\n",
    "    Interaction(ACTION_RESET,OUTCOME_FULL,1),\n",
    "    Interaction(ACTION_FEEL,OUTCOME_EMPTY,-1),\n",
    "    Interaction(ACTION_FEEL,OUTCOME_FULL,1),\n",
    "]\n",
    "\n",
    "simulator = Environment1()\n",
    "a = Agent(interactions, simulator)\n",
    "e = Environment1()\n",
    "\n",
    "outcome = 0\n",
    "for i in range(10):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c88b4-879f-432d-ad25-b4e19e2ab778",
   "metadata": {},
   "source": [
    "# Agent1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d0e6e-9586-4749-a791-d89aca260fdd",
   "metadata": {},
   "source": [
    "Implémentons l'Agent1 pour qu'il choisisse l'action qui conduit à l'interaction qui a la meilleure valence.\n",
    "\n",
    "Notons qu'il faut effectuer la simulation de chaque action dans un clone différent du simulateur car celui-ci peut changer d'état suite à la simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69036914-5f6d-42bb-94a0-61db733c5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent1(Agent):\n",
    "    def select_action(self): \n",
    "        \"\"\"Select the action that yeilds the highest valence\"\"\"\n",
    "        # Roll the actions to try different actions when all outcome are equal\n",
    "        self.action_df = pd.concat([self.action_df.tail(1), self.action_df.head(len(self.action_df) - 1)], ignore_index=True)\n",
    "        # self.action_df = pd.concat([self.action_df.iloc[1:], self.action_df.iloc[[0]]], ignore_index=True)\n",
    "        # Try every action in a clone of the simulator\n",
    "        self.action_df[\"outcome\"] = self.action_df.apply(lambda row: self.simulator.clone().outcome(row[\"action\"]), axis=1)\n",
    "        # Record the expected valence for each resulting interaction\n",
    "        self.action_df[\"valence\"] = self.action_df.apply(lambda row: self._interactions[f\"{row[\"action\"]}{row[\"outcome\"]}\"].valence, axis=1)\n",
    "        # Sort by descending valence\n",
    "        self.action_df = self.action_df.sort_values(by=['valence'], ascending=[False]).reset_index(drop=True)\n",
    "        print(self.action_df)\n",
    "        # Return the action that yeilds the highest valence\n",
    "        return self.action_df.loc[0, \"action\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c6dd1-257c-44b8-9581-3c9b6df95b6e",
   "metadata": {},
   "source": [
    "## Testons l'Agent1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f6b5c27-ddca-4f4a-98a3-0603e05fb3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "   action  outcome  valence\n",
      "0       2        0       -1\n",
      "1       0        0       -1\n",
      "2       1        0       -1\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "   action  outcome  valence\n",
      "0       1        0       -1\n",
      "1       2        0       -1\n",
      "2       0        0       -1\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "   action  outcome  valence\n",
      "0       0        0       -1\n",
      "1       1        0       -1\n",
      "2       2        0       -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       0        0       -1\n",
      "2       1        0       -1\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       1        0       -1\n",
      "2       0        0       -1\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       0        0       -1\n",
      "2       1        0       -1\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       1        0       -1\n",
      "2       0        0       -1\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       0        0       -1\n",
      "2       1        0       -1\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       1        0       -1\n",
      "2       0        0       -1\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       0        0       -1\n",
      "2       1        0       -1\n"
     ]
    }
   ],
   "source": [
    "simulator = Environment1()\n",
    "a = Agent1(interactions, simulator)\n",
    "e = Environment1()\n",
    "\n",
    "outcome = 0\n",
    "for i in range(10):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2757b60-1504-4615-9328-591ccf96fd79",
   "metadata": {},
   "source": [
    "Notons que si le simulateur est initialisé dans un état différent que l'environnement, il se synchronise tout seul par les actions `set` ou `reset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0834d934-8b1c-4514-9658-61c071f161a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "   action  outcome  valence\n",
      "0       2        0       -1\n",
      "1       0        0       -1\n",
      "2       1        0       -1\n",
      "Step 1\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction: False, Valence: 1)\n",
      "   action  outcome  valence\n",
      "0       1        0       -1\n",
      "1       2        0       -1\n",
      "2       0        0       -1\n",
      "Step 2\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "   action  outcome  valence\n",
      "0       0        0       -1\n",
      "1       1        0       -1\n",
      "2       2        0       -1\n",
      "Step 3\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       0        0       -1\n",
      "2       1        0       -1\n",
      "Step 4\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       1        0       -1\n",
      "2       0        0       -1\n",
      "Step 5\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       0        0       -1\n",
      "2       1        0       -1\n",
      "Step 6\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       1        0       -1\n",
      "2       0        0       -1\n",
      "Step 7\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       0        0       -1\n",
      "2       1        0       -1\n",
      "Step 8\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       1        0       -1\n",
      "2       0        0       -1\n",
      "Step 9\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "   action  outcome  valence\n",
      "0       2        1        1\n",
      "1       0        0       -1\n",
      "2       1        0       -1\n"
     ]
    }
   ],
   "source": [
    "simulator = Environment1()\n",
    "a = Agent1(interactions, simulator)\n",
    "e = Environment1(1)\n",
    "\n",
    "outcome = 0\n",
    "for i in range(10):\n",
    "    print(\"Step\", i)\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf9a17-b16b-421a-b23c-e394343641a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyton 3.12",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
