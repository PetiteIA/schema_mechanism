{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eebb8ac-264e-4c67-92bf-320effd22e69",
   "metadata": {},
   "source": [
    "# AGENT1: l'agent qui simulait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48264d7-4120-4cb6-904a-1d7f34f56e26",
   "metadata": {},
   "source": [
    "Simulation du premier agent Sartrien qui une mémoire binaire: l'être ou le néant :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b81b5d6-8d74-448c-8dda-c4606449d209",
   "metadata": {},
   "source": [
    "# Environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7b081-ec11-465e-b2e8-23f5cf01a50c",
   "metadata": {},
   "source": [
    "Si action `feel` alors l'environnement renvoie l'outcome correspondant à la position courate. \n",
    "Si action `moove`alors l'environnement passe à la position suivante et renvoie outcome 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ecd7a0-317a-4032-8b5d-fa0321f6432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SET = 0\n",
    "ACTION_RESET = 1\n",
    "ACTION_FEEL = 2\n",
    "OUTCOME_EMPTY = 0\n",
    "OUTCOME_FULL = 1\n",
    "\n",
    "class Environment1:\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "    def outcome(self, action):\n",
    "        if action == ACTION_FEEL:\n",
    "            outcome = self.state\n",
    "        else:\n",
    "            if action == ACTION_SET:\n",
    "                self.state = 1\n",
    "            else:\n",
    "                self.state = 0\n",
    "            outcome = 0\n",
    "        return outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db93a7-8c36-4a14-8d83-1fd97777f6d9",
   "metadata": {},
   "source": [
    "# L'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb1dd589-53a3-4793-9bbf-4a02d2bd1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, action, outcome, valence):\n",
    "        self.action = action\n",
    "        self.outcome = outcome\n",
    "        self.valence = valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self.action}{self.outcome}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self.action}{self.outcome}:{self.valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        return self.key() == other.key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125dea66-b78a-44d9-b4d7-a0e0c6f8f191",
   "metadata": {},
   "source": [
    "l'agent a un simulateur interne. \n",
    "Si action `set` il mets la mémoire à 1. \n",
    "Si action `reset` il mets la mémoire à 0. \n",
    "Si action `feel` il renvoie l'état de la mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f99d491-f59d-42ff-a068-853e32649bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Creating our agent\"\"\"\n",
    "    def __init__(self, _interactions, simulator):\n",
    "        \"\"\" Initialize the dictionary of interactions\"\"\"\n",
    "        self._interactions = {interaction.key(): interaction for interaction in _interactions}\n",
    "        self._intended_interaction = self._interactions[\"00\"]\n",
    "        self.memory = 0\n",
    "        self.action_df = pd.DataFrame({\"action\": [i.action for i in _interactions if i.outcome == OUTCOME_EMPTY]})\n",
    "        self.simulator = simulator\n",
    "\n",
    "    def simulate(self, action):\n",
    "        if action == ACTION_FEEL: \n",
    "            return self.memory\n",
    "        else:\n",
    "            if action == ACTION_SET:\n",
    "                self.state = 1\n",
    "            else:\n",
    "                self.state = 0\n",
    "            return 0\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\" Tracing the previous cycle \"\"\"\n",
    "        previous_interaction = self._interactions[f\"{self._intended_interaction.action}{_outcome}\"]\n",
    "        print(f\"Action: {self._intended_interaction.action}, Prediction: {self._intended_interaction.outcome}, Outcome: {_outcome}, \" \n",
    "              f\"Prediction: {self._intended_interaction.outcome == _outcome}, Valence: {previous_interaction.valence})\")\n",
    "\n",
    "        \"\"\" Computing the next interaction to try to enact \"\"\"\n",
    "        # Select the next action\n",
    "        # Roll the actions\n",
    "        self.action_df = pd.concat([self.action_df.tail(1), self.action_df.head(len(self.action_df) - 1)], ignore_index=True)\n",
    "        # self.action_df = pd.concat([self.action_df.iloc[1:], self.action_df.iloc[[0]]], ignore_index=True)\n",
    "        # Select the first action in the action_df\n",
    "        intended_action = self.action_df.loc[0, \"action\"]\n",
    "        \n",
    "        # Predict the outcome based on simulation\n",
    "        # intended_outcome = self.simulate(intended_action)\n",
    "        intended_outcome = self.simulator.outcome(intended_action)\n",
    "        # Memorize the intended interaction\n",
    "        self._intended_interaction = self._interactions[f\"{intended_action}{intended_outcome}\"]\n",
    "        return intended_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc3d53-873c-41a7-b359-6acef6d6bcb5",
   "metadata": {},
   "source": [
    "# Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbad721a-46f2-412b-8dd6-7ee0e18e8cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 2, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1)\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1)\n"
     ]
    }
   ],
   "source": [
    "interactions = [\n",
    "    Interaction(ACTION_SET,OUTCOME_EMPTY,-1),\n",
    "    Interaction(ACTION_SET,OUTCOME_FULL,1),\n",
    "    Interaction(ACTION_RESET,OUTCOME_EMPTY,-1),\n",
    "    Interaction(ACTION_RESET,OUTCOME_FULL,1),\n",
    "    Interaction(ACTION_FEEL,OUTCOME_EMPTY,-1),\n",
    "    Interaction(ACTION_FEEL,OUTCOME_FULL,1),\n",
    "]\n",
    "\n",
    "e = Environment1()\n",
    "a = Agent(interactions, e)\n",
    "\n",
    "outcome = 0\n",
    "for i in range(10):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d19158-bfba-49d7-ad6b-118436c7977c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyton 3.12",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
