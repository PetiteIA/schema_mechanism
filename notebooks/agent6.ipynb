{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f34084-d4b8-4160-89ad-a927a0919615",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PetiteIA/schema_mechanism/blob/master/notebooks/agent6.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec58b3-e210-4a78-9610-f6615254de29",
   "metadata": {},
   "source": [
    "# THE AGENT WHO USED A LONGER CONTEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ef995-42ce-44ba-9f3b-8bfa808b08fe",
   "metadata": {},
   "source": [
    "# Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14a47d-ca87-4080-8f84-afcd2bc83170",
   "metadata": {},
   "source": [
    "Upon completing this lab, you will be able to implement a developmental agent that reinforces simple behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16db8-1bd6-4e82-9cf6-2b37d996ffe8",
   "metadata": {},
   "source": [
    "## Define the necessary classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44155c4a-d038-40b3-be33-e8cf77c5f976",
   "metadata": {},
   "source": [
    "Let's improve the Interaction class a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e96c3637-4161-49aa-9f1d-0d3342de92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, action, outcome, valence):\n",
    "        self._action = action\n",
    "        self._outcome = outcome\n",
    "        self._valence = valence\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_outcome(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._outcome\n",
    "\n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self._action}{self._outcome}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self._action}{self._outcome}:{self._valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.key() == other.key()\n",
    "        else:\n",
    "            return False            "
   ]
  },
  {
   "cell_type": "raw",
   "id": "be99690f-4199-4438-afdd-13c7ee0b65fe",
   "metadata": {},
   "source": [
    "Let's improve the CompositeInteraction class a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ef3243d6-1de3-418e-9236-8af0287131da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeInteraction:\n",
    "    \"\"\"A composite interaction is a tuple (pre_interaction, post_interaction) and a weight\"\"\"\n",
    "    def __init__(self, pre_interaction, post_interaction):\n",
    "        self.pre_interaction = pre_interaction\n",
    "        self.post_interaction = post_interaction\n",
    "        self.weight = 1\n",
    "        self.isActivated = False\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action of the post interaction\"\"\"\n",
    "        return self.post_interaction.get_action()\n",
    "    \n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the valence of the pre_interaction plus the valence of the post_interaction\"\"\"\n",
    "        return self.pre_interaction.get_valence() + self.post_interaction.get_valence()\n",
    "\n",
    "    def reinforce(self):\n",
    "        \"\"\"Increment the composite interaction's weight\"\"\"\n",
    "        self.weight += 1\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictionary is the string '<pre_interaction><post_interaction>'. \"\"\"\n",
    "        return f\"({self.pre_interaction.key()},{self.post_interaction.key()})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print the interaction in the Newick tree format (pre_interaction, post_interaction: valence) \"\"\"\n",
    "        return f\"({self.pre_interaction}, {self.post_interaction}: {self.weight})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same pre and post interactions \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return (self.pre_interaction == other.pre_interaction) and (self.post_interaction == other.post_interaction)\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152512a3-ef97-473a-91a7-9a36fe70ba33",
   "metadata": {},
   "source": [
    "## Define the Agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b81c3-5671-4ec6-bb7d-9db4f99c7c0b",
   "metadata": {},
   "source": [
    "We will use a Pandas DataFrame to compute the selection of the next intended interaction and to predict its most likely outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7e5da-0188-4924-92f4-3ecbfc2fce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d65a49-9e8a-4834-b064-d9fff651abdc",
   "metadata": {},
   "source": [
    "Let's implement a base Agent that has the functionnalities of Agent5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "3f0bf369-48ef-48d7-a92b-3c7ef8dc1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, _interactions):\n",
    "        \"\"\" Initialize our agent \"\"\"\n",
    "        self._interactions = {interaction.key(): interaction for interaction in _interactions}\n",
    "        self._composite_interactions = {}\n",
    "        self._intended_interaction = self._interactions[\"00\"]\n",
    "        self._last_interaction = None\n",
    "        self._previous_interaction = None\n",
    "        self._last_composite_interaction = None\n",
    "        # Create a dataframe of default primitive interactions \n",
    "        default_interactions = [interaction for interaction in _interactions if interaction.get_outcome() == 0]\n",
    "        data = {'post_action': [i.get_action() for i in default_interactions],\n",
    "                'weight': [0] * len(default_interactions),\n",
    "                'proclivity': [0] * len(default_interactions),\n",
    "                'post_interaction': [i.key() for i in default_interactions]}\n",
    "        self.primitive_df = pd.DataFrame(data)\n",
    "        # Store the selection dataframe as a class attribute so we can display it in the notebook\n",
    "        self.selection_df = None\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\"Implement the agent's policy\"\"\"\n",
    "        # tracing the previous cycle\n",
    "        self._previous_interaction = self._last_interaction\n",
    "        self._last_interaction = self._interactions[f\"{self._intended_interaction.get_action()}{_outcome}\"]\n",
    "        print(f\"Action: {self._intended_interaction.get_action()}, Prediction: {self._intended_interaction.get_outcome()}, \"\n",
    "              f\"Outcome: {_outcome}, Prediction_correct: {self._intended_interaction.get_outcome() == _outcome}, \"\n",
    "              f\"Valence: {self._last_interaction.get_valence()}\")\n",
    "\n",
    "        # Call the learning mechanism\n",
    "        self.learn()\n",
    "        \n",
    "        # Create a dataframe from the activated composite interaction \n",
    "        activated_keys = [composite_interaction.key() for composite_interaction in self._composite_interactions.values() \n",
    "                          if composite_interaction.pre_interaction == self._last_interaction or \n",
    "                          composite_interaction.pre_interaction == self._last_composite_interaction]\n",
    "        data = {'composite': activated_keys,\n",
    "                'weight': [self._composite_interactions[k].weight for k in activated_keys],\n",
    "                'post_valence': [self._composite_interactions[k].post_interaction.get_valence() for k in activated_keys],\n",
    "                'post_action': [self._composite_interactions[k].post_interaction.get_action() for k in activated_keys],\n",
    "                'post_interaction': [self._composite_interactions[k].post_interaction.key() for k in activated_keys]\n",
    "                }\n",
    "        activated_df = pd.DataFrame(data)\n",
    "\n",
    "        # Create the selection dataframe from the primitive and the activated dataframes\n",
    "        df = pd.concat([self.primitive_df, activated_df], ignore_index=True)\n",
    "\n",
    "        # Compute the proclivity for each action\n",
    "        df['proclivity'] = df['weight'] * df['post_valence']\n",
    "        grouped_df = df.groupby('post_action').agg({'proclivity': 'sum'}).reset_index()\n",
    "        df = df.merge(grouped_df, on='post_action', suffixes=('', '_sum'))\n",
    "\n",
    "        # Find the most probable outcome for each action\n",
    "        max_weight_df = df.loc[df.groupby('post_action')['weight'].idxmax(), ['post_action', 'post_interaction']].reset_index(drop=True)\n",
    "        max_weight_df.columns = ['post_action', 'intended']\n",
    "        df = df.merge(max_weight_df, on='post_action')\n",
    "\n",
    "        # Find the first row that has the highest proclivity\n",
    "        max_index = df['proclivity_sum'].idxmax()\n",
    "        intended_interaction = df.loc[max_index, ['intended']].values[0]\n",
    "        self._intended_interaction = self._interactions[intended_interaction]\n",
    "        print(\"Intended\", self._intended_interaction)\n",
    "\n",
    "        # Store the selection dataframe for printing\n",
    "        self.selection_df = df.copy()\n",
    "        \n",
    "        return self._intended_interaction.get_action()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Recording previous composite interaction\"\"\"\n",
    "        if self._previous_interaction is not None:\n",
    "            composite_interaction = CompositeInteraction(self._previous_interaction, self._last_interaction)\n",
    "            if composite_interaction.key() not in self._composite_interactions:\n",
    "                self._composite_interactions[composite_interaction.key()] = composite_interaction\n",
    "                print(f\"Learning {composite_interaction}\")\n",
    "            else:\n",
    "                self._composite_interactions[composite_interaction.key()].reinforce()\n",
    "                print(f\"Reinforcing {self._composite_interactions[composite_interaction.key()]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019f138-a218-44b2-964a-8095fbe891a7",
   "metadata": {},
   "source": [
    "# PRELIMINARY EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f2924-f597-4d34-bc96-3602dc4460b7",
   "metadata": {},
   "source": [
    "Let's test this agent in Environment4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5bf9a4ba-2703-49e9-a63f-b72c08b8663c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Environment4:\n",
    "    \"\"\" Environm4 \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializing Environment4 \"\"\"\n",
    "        self.step = 0\n",
    "\n",
    "    def outcome(self, _action):\n",
    "        \"\"\"Take the action and generate the next outcome \"\"\"\n",
    "        self.step += 1\n",
    "        # Behave like environment1 during the first 10 steps\n",
    "        if self.step < 10:\n",
    "            if _action == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1            \n",
    "        # Behave like Environment2 after the first 10 steps\n",
    "        else: \n",
    "            if _action == 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ce510-9438-47d1-ac88-ab79c063d592",
   "metadata": {},
   "source": [
    "Initialize the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6f6d76df-89e4-4ab3-a327-afa813a27325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a new agent\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1),\n",
    "    Interaction(2,0,-1),\n",
    "    Interaction(2,1,1)\n",
    "]\n",
    "a = Agent(interactions)\n",
    "e = Environment4()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf97eb-7aca-4157-ba8e-3e97f2b85ae1",
   "metadata": {},
   "source": [
    "Run the simulation step by step to see the Selection DataFrame. Use `Ctrl+Enter` to run the cell bellow and stay on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "5b044b4d-162e-4491-929c-a73eb584dcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction_correct: True, Valence: 1\n",
      "Reinforcing (01:1, 01:1: 10)\n",
      "Intended 01:1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>composite</th>\n",
       "      <th>weight</th>\n",
       "      <th>post_valence</th>\n",
       "      <th>post_action</th>\n",
       "      <th>proclivity</th>\n",
       "      <th>proclivity_sum</th>\n",
       "      <th>intended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(01,01)</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  composite  weight  post_valence  post_action  proclivity  proclivity_sum  \\\n",
       "0       NaN       0           NaN            0         NaN            10.0   \n",
       "1       NaN       0           NaN            1         NaN             0.0   \n",
       "2       NaN       0           NaN            2         NaN             0.0   \n",
       "3   (01,01)      10           1.0            0        10.0            10.0   \n",
       "\n",
       "  intended  \n",
       "0       01  \n",
       "1       10  \n",
       "2       20  \n",
       "3       01  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.selection_df[['composite', 'weight', 'post_valence', 'post_action', 'proclivity', 'proclivity_sum', 'intended']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9daa278-434e-43ce-8520-8fabef69cd83",
   "metadata": {},
   "source": [
    "Observe the Selection DataFrame above as you run the agent step by step. \n",
    "Each activated composite interaction proposes its post_action with proclity equals to the composite interaction's weight multiplied by the post interactions' valence. \n",
    "\n",
    "The proclivities are summed for each action. The action that has the highest sum proclivity is selected.\n",
    "\n",
    "* During the first 10 steps, the composite interaction (11,11) is progressively reinforced as the agent learns that, in the context of intreaction 11, it can again enact interation 11.\n",
    "* After Step 10, the agent learns the composite interaction (01,01), which tells that, in the context of interaction, 01, the agent can again enact 01."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50d96e-4f12-4531-a122-9d13d8bd8639",
   "metadata": {},
   "source": [
    "# ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199faee9-a546-4d86-8e09-484bf2212523",
   "metadata": {},
   "source": [
    "Implement Agent6 that learns higher levels of composite interactions as shown in Figure 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e95995-8077-46ed-a03d-e318623cb82b",
   "metadata": {},
   "source": [
    "![Agent5](img/Figure_1_Agent6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742aecbc-5e94-4cc6-bf5d-1b6c12bde814",
   "metadata": {},
   "source": [
    "Figure 1: Agent6 records and reinforces two levels of composite interactions. as tuples $(i_{t-2}, i_{t-1}: weight)$. \n",
    "\n",
    "The last enacted interaction $i_{t-1}$ activates previously-learned composite interactions that propose the action of their post interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2e150c60-5315-4a49-822f-2681fb22a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent6(Agent):\n",
    "    def learn(self):\n",
    "        # Implement the learning mechanism of Agent6 here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7fa66-bc71-44bf-85ff-716842a24aad",
   "metadata": {},
   "source": [
    "## Test Agent6 in the turtle environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991de95f-e2d4-4638-a7f8-55de36eb3617",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @title Install the turtle environment\n",
    "!pip3 install ColabTurtle\n",
    "from ColabTurtle.Turtle import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f35913a4-4426-4e50-ba74-6790e579fb80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @title Initialize the turtle environment\n",
    "\n",
    "BORDER_WIDTH = 20\n",
    "\n",
    "class ColabTurtleEnvironment:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Creating the Turtle window \"\"\"\n",
    "        bgcolor(\"lightGray\")\n",
    "        penup()\n",
    "        goto(window_width() / 2, window_height()/2)\n",
    "        face(0)\n",
    "        pendown()\n",
    "        color(\"green\")\n",
    "\n",
    "    def outcome(self, action):\n",
    "        \"\"\" Enacting an action and returning the outcome \"\"\"\n",
    "        _outcome = 0\n",
    "        for i in range(10):\n",
    "            # _outcome = 0\n",
    "            if action == 0:\n",
    "                # move forward\n",
    "                forward(10)\n",
    "            elif action == 1:\n",
    "                # rotate left\n",
    "                left(4)\n",
    "                forward(2)\n",
    "            elif action == 2:\n",
    "                # rotate right\n",
    "                right(4)\n",
    "                forward(2)\n",
    "\n",
    "            # Bump on screen edge and return outcome 1\n",
    "            if xcor() < BORDER_WIDTH:\n",
    "                goto(BORDER_WIDTH, ycor())\n",
    "                _outcome = 1\n",
    "            if xcor() > window_width() - BORDER_WIDTH:\n",
    "                goto(window_width() - BORDER_WIDTH, ycor())\n",
    "                _outcome = 1\n",
    "            if ycor() < BORDER_WIDTH:\n",
    "                goto(xcor(), BORDER_WIDTH)\n",
    "                _outcome = 1\n",
    "            if ycor() > window_height() - BORDER_WIDTH:\n",
    "                goto(xcor(), window_height() -BORDER_WIDTH)\n",
    "                _outcome = 1\n",
    "\n",
    "            # Change color\n",
    "            if _outcome == 0:\n",
    "                color(\"green\")\n",
    "            else:\n",
    "                # Finit l'interaction\n",
    "                color(\"red\")\n",
    "                # if action == 0:\n",
    "                #     break\n",
    "                if action == 1:\n",
    "                    for j in range(10):\n",
    "                        left(4)\n",
    "                elif action == 2:\n",
    "                    for j in range(10):\n",
    "                        right(4)\n",
    "                break\n",
    "\n",
    "        return _outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4b183-a761-47c8-a3a5-957467dd5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run the turtle environment\n",
    "import pandas as pd\n",
    "initializeTurtle()\n",
    "\n",
    "# Parameterize the rendering\n",
    "bgcolor(\"lightGray\")\n",
    "penup()\n",
    "goto(window_width() / 2, window_height()/2)\n",
    "face(0)\n",
    "pendown()\n",
    "color(\"green\")\n",
    "speed(10)\n",
    "\n",
    "# Some valences to avoid bumping into walls\n",
    "interactions = [\n",
    "    Interaction(0,0,3),\n",
    "    Interaction(0,1,-3),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,-1),\n",
    "    Interaction(2,0,-2),\n",
    "    Interaction(2,1,-2)\n",
    "]\n",
    "\n",
    "a = Agent6(interactions)\n",
    "e = ColabTurtleEnvironment()\n",
    "\n",
    "outcome = 0\n",
    "for i in range(10):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15a134-34a6-4039-9b43-f8fd76d93b5e",
   "metadata": {},
   "source": [
    "## Report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e661c15-c1ed-4512-a1d9-0fbdb8f91879",
   "metadata": {},
   "source": [
    "Explain what you programmed and what results you observed. Export this document as PDF including your code, the traces you obtained, and your explanations below (no more than a few paragraphs):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
