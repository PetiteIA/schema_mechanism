{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f34084-d4b8-4160-89ad-a927a0919615",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PetiteIA/schema_mechanism/blob/master/notebooks/agent6.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec58b3-e210-4a78-9610-f6615254de29",
   "metadata": {},
   "source": [
    "# THE AGENT WHO FOLLOWED ANCIENT CLUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ef995-42ce-44ba-9f3b-8bfa808b08fe",
   "metadata": {},
   "source": [
    "# Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14a47d-ca87-4080-8f84-afcd2bc83170",
   "metadata": {},
   "source": [
    "Upon completing this lab, you will be able to implement a developmental agent that learns sequences of interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16db8-1bd6-4e82-9cf6-2b37d996ffe8",
   "metadata": {},
   "source": [
    "## Define the necessary classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44155c4a-d038-40b3-be33-e8cf77c5f976",
   "metadata": {},
   "source": [
    "Let's improve the Interaction class a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96c3637-4161-49aa-9f1d-0d3342de92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, action, outcome, valence):\n",
    "        self._action = action\n",
    "        self._outcome = outcome\n",
    "        self._valence = valence\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_outcome(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._outcome\n",
    "\n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self._action}{self._outcome}\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        return self.key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self._action}{self._outcome}:{self._valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.key() == other.key()\n",
    "        else:\n",
    "            return False            "
   ]
  },
  {
   "cell_type": "raw",
   "id": "be99690f-4199-4438-afdd-13c7ee0b65fe",
   "metadata": {},
   "source": [
    "Let's improve the CompositeInteraction class a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3243d6-1de3-418e-9236-8af0287131da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeInteraction:\n",
    "    \"\"\"A composite interaction is a tuple (pre_interaction, post_interaction) and a weight\"\"\"\n",
    "    def __init__(self, pre_interaction, post_interaction):\n",
    "        self.pre_interaction = pre_interaction\n",
    "        self.post_interaction = post_interaction\n",
    "        self.weight = 1\n",
    "        self.isActivated = False\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action of the post interaction\"\"\"\n",
    "        return self.post_interaction.get_action()\n",
    "    \n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the valence of the pre_interaction plus the valence of the post_interaction\"\"\"\n",
    "        return self.pre_interaction.get_valence() + self.post_interaction.get_valence()\n",
    "\n",
    "    def reinforce(self):\n",
    "        \"\"\"Increment the composite interaction's weight\"\"\"\n",
    "        self.weight += 1\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictionary is the string '<pre_interaction><post_interaction>'. \"\"\"\n",
    "        return f\"({self.pre_interaction.key()},{self.post_interaction.key()})\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key of the pre_interaction\"\"\"\n",
    "        return self.pre_interaction.pre_key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print the interaction in the Newick tree format (pre_interaction, post_interaction: valence) \"\"\"\n",
    "        return f\"({self.pre_interaction}, {self.post_interaction}: {self.weight})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same pre and post interactions \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return (self.pre_interaction == other.pre_interaction) and (self.post_interaction == other.post_interaction)\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152512a3-ef97-473a-91a7-9a36fe70ba33",
   "metadata": {},
   "source": [
    "## Define the Agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b81c3-5671-4ec6-bb7d-9db4f99c7c0b",
   "metadata": {},
   "source": [
    "We will use a Pandas DataFrame to compute the selection of the next intended interaction and to predict its most likely outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7e5da-0188-4924-92f4-3ecbfc2fce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d65a49-9e8a-4834-b064-d9fff651abdc",
   "metadata": {},
   "source": [
    "Let's implement a base Agent that has the functionnalities of Agent5 implemented using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f0bf369-48ef-48d7-a92b-3c7ef8dc1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, _interactions):\n",
    "        \"\"\" Initialize our agent \"\"\"\n",
    "        self._interactions = {interaction.key(): interaction for interaction in _interactions}\n",
    "        self._composite_interactions = {}\n",
    "        self._intended_interaction = self._interactions[\"00\"]\n",
    "        self._last_interaction = None\n",
    "        self._previous_interaction = None\n",
    "        self._penultimate_interaction = None\n",
    "        self._last_composite_interaction = None\n",
    "        self._previous_composite_interaction = None\n",
    "        # Create a dataframe of default primitive interactions \n",
    "        default_interactions = [interaction for interaction in _interactions if interaction.get_outcome() == 0]\n",
    "        data = {'post_action': [i.get_action() for i in default_interactions],\n",
    "                'weight': [0] * len(default_interactions),\n",
    "                'proclivity': [0] * len(default_interactions),\n",
    "                'post_interaction': [i.key() for i in default_interactions]}\n",
    "        self.primitive_df = pd.DataFrame(data)\n",
    "        # Store the selection dataframe as a class attribute so we can display it in the notebook\n",
    "        self.selection_df = None\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\"Implement the agent's policy\"\"\"\n",
    "        # tracing the previous cycle\n",
    "        self._previous_composite_interaction = self._last_composite_interaction\n",
    "        self._penultimate_interaction = self._previous_interaction\n",
    "        self._previous_interaction = self._last_interaction\n",
    "        self._last_interaction = self._interactions[f\"{self._intended_interaction.get_action()}{_outcome}\"]\n",
    "        print(f\"Action: {self._intended_interaction.get_action()}, Prediction: {self._intended_interaction.get_outcome()}, \"\n",
    "              f\"Outcome: {_outcome}, Prediction_correct: {self._intended_interaction.get_outcome() == _outcome}, \"\n",
    "              f\"Valence: {self._last_interaction.get_valence()}\")\n",
    "\n",
    "        # Call the learning mechanism\n",
    "        self.learn()\n",
    "        \n",
    "        # Create a dataframe from the activated composite interactions \n",
    "        activated_keys = [composite_interaction.key() for composite_interaction in self._composite_interactions.values() \n",
    "                          if composite_interaction.pre_interaction == self._last_interaction or \n",
    "                          composite_interaction.pre_interaction == self._last_composite_interaction]\n",
    "        data = {'composite': activated_keys,\n",
    "                'weight': [self._composite_interactions[k].weight for k in activated_keys],\n",
    "                'post_valence': [self._composite_interactions[k].post_interaction.get_valence() for k in activated_keys],\n",
    "                'post_action': [self._composite_interactions[k].post_interaction.get_action() for k in activated_keys],\n",
    "                'post_interaction': [self._composite_interactions[k].post_interaction.pre_key() for k in activated_keys]\n",
    "                }\n",
    "        activated_df = pd.DataFrame(data)\n",
    "\n",
    "        # Create the selection dataframe from the primitive and the activated dataframes\n",
    "        df = pd.concat([self.primitive_df, activated_df], ignore_index=True)\n",
    "\n",
    "        # Compute the proclivity for each action\n",
    "        df['proclivity'] = df['weight'] * df['post_valence']\n",
    "        grouped_df = df.groupby('post_action').agg({'proclivity': 'sum'}).reset_index()\n",
    "        df = df.merge(grouped_df, on='post_action', suffixes=('', '_sum'))\n",
    "\n",
    "        # Find the most probable outcome for each action\n",
    "        max_weight_df = df.loc[df.groupby('post_action')['weight'].idxmax(), ['post_action', 'post_interaction']].reset_index(drop=True)\n",
    "        max_weight_df.columns = ['post_action', 'intended']\n",
    "        df = df.merge(max_weight_df, on='post_action')\n",
    "\n",
    "        # Find the first row that has the highest proclivity\n",
    "        max_index = df['proclivity_sum'].idxmax()\n",
    "        intended_interaction_key = df.loc[max_index, ['intended']].values[0]\n",
    "        self._intended_interaction = self._interactions[intended_interaction_key]\n",
    "        print(\"Intended\", self._intended_interaction)\n",
    "\n",
    "        # Store the selection dataframe for printing\n",
    "        self.selection_df = df.copy()\n",
    "        \n",
    "        return self._intended_interaction.get_action()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Recording previous composite interaction\"\"\"\n",
    "        if self._previous_interaction is not None:\n",
    "            composite_interaction = CompositeInteraction(self._previous_interaction, self._last_interaction)\n",
    "            if composite_interaction.key() not in self._composite_interactions:\n",
    "                self._composite_interactions[composite_interaction.key()] = composite_interaction\n",
    "                print(f\"Learning {composite_interaction}\")\n",
    "                self._last_composite_interaction = composite_interaction\n",
    "            else:\n",
    "                self._composite_interactions[composite_interaction.key()].reinforce()\n",
    "                print(f\"Reinforcing {self._composite_interactions[composite_interaction.key()]}\")\n",
    "                self._last_composite_interaction = self._composite_interactions[composite_interaction.key()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019f138-a218-44b2-964a-8095fbe891a7",
   "metadata": {},
   "source": [
    "# PRELIMINARY EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f2924-f597-4d34-bc96-3602dc4460b7",
   "metadata": {},
   "source": [
    "Let's test this agent in Environment4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bf9a4ba-2703-49e9-a63f-b72c08b8663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment4:\n",
    "    \"\"\" Environment4 \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializing Environment4 \"\"\"\n",
    "        self.step = 0\n",
    "\n",
    "    def outcome(self, _action):\n",
    "        \"\"\"Take the action and generate the next outcome \"\"\"\n",
    "        self.step += 1\n",
    "        # Behave like environment1 during the first 10 steps\n",
    "        if self.step < 10:\n",
    "            if _action == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1            \n",
    "        # Behave like Environment2 after the first 10 steps\n",
    "        else: \n",
    "            if _action == 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ce510-9438-47d1-ac88-ab79c063d592",
   "metadata": {},
   "source": [
    "Initialize the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f6d76df-89e4-4ab3-a327-afa813a27325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a new agent\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1),\n",
    "    Interaction(2,0,-1),\n",
    "    Interaction(2,1,1)\n",
    "]\n",
    "a = Agent(interactions)\n",
    "e = Environment4()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf97eb-7aca-4157-ba8e-3e97f2b85ae1",
   "metadata": {},
   "source": [
    "Run the simulation step by step to see the Selection DataFrame. Use `Ctrl+Enter` to run the cell bellow and stay on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b044b4d-162e-4491-929c-a73eb584dcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1\n",
      "Intended 00:-1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>composite</th>\n",
       "      <th>weight</th>\n",
       "      <th>post_valence</th>\n",
       "      <th>post_action</th>\n",
       "      <th>proclivity</th>\n",
       "      <th>proclivity_sum</th>\n",
       "      <th>intended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   composite  weight  post_valence  post_action  proclivity  proclivity_sum  \\\n",
       "0        NaN     0.0           NaN          0.0         NaN             0.0   \n",
       "1        NaN     0.0           NaN          1.0         NaN             0.0   \n",
       "2        NaN     0.0           NaN          2.0         NaN             0.0   \n",
       "\n",
       "  intended  \n",
       "0       00  \n",
       "1       10  \n",
       "2       20  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.selection_df[['composite', 'weight', 'post_valence', 'post_action', 'proclivity', 'proclivity_sum', 'intended']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9daa278-434e-43ce-8520-8fabef69cd83",
   "metadata": {},
   "source": [
    "Observe the Selection DataFrame above as you run the agent step by step. \n",
    "Each activated composite interaction proposes its post_action with proclity equals to the composite interaction's weight multiplied by the post interactions' valence. \n",
    "\n",
    "The proclivities are summed for each action according to the formula given in Agent5: \n",
    "\n",
    "$\\displaystyle proclivity_a = \\sum_{c \\in A_a} w_c \\cdot v_{i}$\n",
    "\n",
    "in which $A_a$ is the set of activated composite interactions $c$ that propose action $a$, $w_c$ is the weight of $c$, $v_i$ is the valence of the interaction proposed by $c$.\n",
    "\n",
    "The action that has the highest `proclivity_sum` $proclivity_a$ is selected.\n",
    "\n",
    "* During the first 10 steps, the composite interaction (11,11) is progressively reinforced as the agent learns that, in the context of intreaction 11, it can again enact interation 11.\n",
    "* After Step 10, the agent learns the composite interaction (01,01), which tells that, in the context of interaction, 01, the agent can again enact 01."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ab9c1-4c9a-4cdc-9350-cdab5144ee25",
   "metadata": {},
   "source": [
    "## Remark on the relation between proclivity and expected reward "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b645b0-62d6-4246-8a99-0cb32b45100c",
   "metadata": {},
   "source": [
    "Notably,  $proclivity_a$ relates to the \"expected reward\" traditionally used in reward optimisation under uncertainty. \n",
    "The expected reward balances the reward of each sequence with the probability of successfully enacting the sequence.\n",
    "In the reinforcement learning literature, the expected reward for action $a$ is sometimes noted $\\mathbb{E}(R_a)$ and its estimation $\\hat{\\mathbb{E}}(R_a)$ .\n",
    "\n",
    "In our case, however, the agent ignores the probability of successfully enacting the post interaction $i$. \n",
    "The agent can only compute an estimated expected valence $\\hat{v}_a$ based on an estimated probability $\\hat{p}_{ai}$ of enacting the post interaction $i$ when selecting action $a$ :\n",
    "\n",
    "$\\displaystyle \\hat{v}_a = \\sum_{i \\in I} \\hat{p}_{ia} \\cdot v_{i}$\n",
    "\n",
    "in which $I$ is the set of primitive interactions that can result from action $a$.\n",
    "\n",
    "If there is at least one activated composite interaction proposing $a$ then, for any interaction $i$ whose action is $a$, the estimated probability $\\hat{p}_{ia}$ can be computed as the ratio of the weight of composite interactions proposing $i$ over the total weight of composite interactions proposing $a$ :\n",
    "\n",
    "$\\displaystyle \\hat{p}_{ia} = \\frac{\\sum_{c \\in A_{ia}} w_c }{\\sum_{c \\in A_a} w_c }$ \n",
    "\n",
    "in which $A_a$ is the set of activated composite interactions proposing $a$, and $A_{ia} \\subset A_a$ is the set of activated composite interactions proposing interaction $i$ whose action is $a$. \n",
    "\n",
    "The proclivy $proclivity_a$ for selecting action $a$ is thus equal to the estimated expected valence resulting from $a$ multiplied by the sum of the weight of the activated composite interactions proposing $a$ :\n",
    "\n",
    "$\\displaystyle proclivity_a = \\hat{v}_a \\cdot \\sum_{c \\in A_a} w_c $\n",
    "\n",
    "Two observations follow: \n",
    "* The $\\sum_{c \\in A_a} w_c$ factor, called the \"proposal weight\" increases the chances of selecting actions that yield positive valence over time. The more an action has been selected, the more it will be selected in the future, leading the agent to learn habits of interactions that yield positive valences.\n",
    "* It is well possible that the estimation $\\hat{v}_a$ is inacurate because the estimated probabilities $\\hat{p}_{ai}$ is biased by the fact that the agent preferably tries actions that already have a positive expected valence.\n",
    "\n",
    "In future agents, we may want to compute the proposal weight differently, and improve the estimation of expected valences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7bde1-d58a-40e5-8b55-45af157ae077",
   "metadata": {},
   "source": [
    "## Environment5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ef47c-b298-41cd-9372-aac913a3e7f8",
   "metadata": {},
   "source": [
    "Let's define Environment5 in which the agent must perform the same action twice in a row in order to get an outcome of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "77a6dc83-5bbc-4e72-a1a2-8242ada3e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment5:\n",
    "    \"\"\" Environment5 \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializing Environment4 \"\"\"\n",
    "        self._previous_action = 0\n",
    "        self._last_action = 0\n",
    "\n",
    "    def outcome(self, _action):\n",
    "        \"\"\"Take the action and generate the next outcome \"\"\"\n",
    "        if _action == self._last_action and _action == self._previous_action:\n",
    "            # If same action during the last 3 steps then outcome 0\n",
    "            _outcome = 0\n",
    "        else:\n",
    "            # If different action then outcome 1\n",
    "            _outcome = 1\n",
    "        self._previous_action = self._last_action\n",
    "        self._last_action = _action\n",
    "        return _outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "91a00992-7b54-433f-a2a0-027c905a56cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a new agent\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1),\n",
    "]\n",
    "a = Agent(interactions)\n",
    "e = Environment5()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d9817644-f5e8-4935-a4ee-bac1dc04ccbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction_correct: True, Valence: 1\n",
      "Reinforcing (01:1, 01:1: 5)\n",
      "Intended 01:1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>composite</th>\n",
       "      <th>weight</th>\n",
       "      <th>post_valence</th>\n",
       "      <th>post_action</th>\n",
       "      <th>proclivity</th>\n",
       "      <th>proclivity_sum</th>\n",
       "      <th>intended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(01,01)</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(01,00)</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  composite  weight  post_valence  post_action  proclivity  proclivity_sum  \\\n",
       "0       NaN       0           NaN            0         NaN             1.0   \n",
       "1       NaN       0           NaN            1         NaN             0.0   \n",
       "2   (01,01)       5           1.0            0         5.0             1.0   \n",
       "3   (01,00)       4          -1.0            0        -4.0             1.0   \n",
       "\n",
       "  intended  \n",
       "0       01  \n",
       "1       10  \n",
       "2       01  \n",
       "3       01  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.selection_df[['composite', 'weight', 'post_valence', 'post_action', 'proclivity', 'proclivity_sum', 'intended']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d304f1-f549-40b2-8737-b4b627e35b2e",
   "metadata": {},
   "source": [
    "Observe that this agent cannot get a positive valence each time. \n",
    "To do so, he would need to select an action base on the last two previous interactions. \n",
    "We are going to design Agent6 that can do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50d96e-4f12-4531-a122-9d13d8bd8639",
   "metadata": {},
   "source": [
    "# ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199faee9-a546-4d86-8e09-484bf2212523",
   "metadata": {},
   "source": [
    "Implement Agent6 that learns a higher level of composite interactions as shown in Figure 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e95995-8077-46ed-a03d-e318623cb82b",
   "metadata": {},
   "source": [
    "![Agent5](img/Figure_1_Agent6.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742aecbc-5e94-4cc6-bf5d-1b6c12bde814",
   "metadata": {},
   "source": [
    "Figure 1: Agent6 records and reinforces two levels of composite interactions:\n",
    "* First-level composite interaction $c_{t-1} = (i_{t-2}, i_{t-1}: weight)$, \n",
    "* Second-level composite interaction $(c_{t-2}, i_{t-1}: weight)$.\n",
    "\n",
    "The last enacted primitive interaction $i_{t-1}$ and the last enacted composite interaction $c_{t-1}$ activates previously-learned composite interactions that propose the action of their post interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c6b9a-dbb0-4420-bab9-49005a8f71a5",
   "metadata": {},
   "source": [
    "## Implement the learning mechanism of Agent6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22833230-917e-4cc0-ad71-46331fb8543d",
   "metadata": {},
   "source": [
    "In the Agent's learning method below, add the learning of the second-level composite interaction similar to the first level.\n",
    "\n",
    "* Its pre_interaction will be `self._previous_composite_interaction`\n",
    "* Its post_interaction will be `self._last_interaction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8955379f-f99a-409d-a57c-ab2757d0574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent6(Agent):\n",
    "    def learn(self):\n",
    "        # Recording previous composite interaction\n",
    "        if self._previous_interaction is not None:\n",
    "            # Record or reinforce the first level composite interaction\n",
    "            composite_interaction = CompositeInteraction(self._previous_interaction, self._last_interaction)\n",
    "            if composite_interaction.key() not in self._composite_interactions:\n",
    "                self._composite_interactions[composite_interaction.key()] = composite_interaction\n",
    "                print(f\"Learning {composite_interaction}\")\n",
    "                self._last_composite_interaction = composite_interaction\n",
    "            else:\n",
    "                self._composite_interactions[composite_interaction.key()].reinforce()\n",
    "                print(f\"Reinforcing {self._composite_interactions[composite_interaction.key()]}\")\n",
    "                self._last_composite_interaction = self._composite_interactions[composite_interaction.key()]\n",
    "            # *** Implement the learning mechanism of the second level composite interaction here ****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4fec51-19aa-447d-a456-893249acc994",
   "metadata": {},
   "source": [
    "## Test your Agent6 in Environment5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac7f79-9722-4bc7-85f4-a025b43fedd4",
   "metadata": {},
   "source": [
    "It is ok if your Agent6 requires several tens of interaction to learn, but we expect it to eventually obtain a positive valence on every cicle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "74c423e8-de20-479c-84f7-8d64b3985433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a new agent\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1),\n",
    "]\n",
    "a = Agent6(interactions)\n",
    "e = Environment5()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0159e4d4-f851-432b-8127-63403d7a45c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction_correct: True, Valence: 1\n",
      "Reinforcing (00:-1, 11:1: 3)\n",
      "Intended 01:1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>composite</th>\n",
       "      <th>weight</th>\n",
       "      <th>post_valence</th>\n",
       "      <th>post_action</th>\n",
       "      <th>proclivity</th>\n",
       "      <th>proclivity_sum</th>\n",
       "      <th>intended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(11,01)</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  composite  weight  post_valence  post_action  proclivity  proclivity_sum  \\\n",
       "0       NaN       0           NaN            0         NaN             2.0   \n",
       "1       NaN       0           NaN            1         NaN             0.0   \n",
       "2   (11,01)       2           1.0            0         2.0             2.0   \n",
       "\n",
       "  intended  \n",
       "0       01  \n",
       "1       10  \n",
       "2       01  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.selection_df[['composite', 'weight', 'post_valence', 'post_action', 'proclivity', 'proclivity_sum', 'intended']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0316ab43-7b94-47d2-9f0b-6762f2f11657",
   "metadata": {},
   "source": [
    "You should observe that activated composite interactions in column `composite` may have a pre_interaction that is itself a composite interaction. \n",
    "This allows Agent6 to select behavior that is adapted to the previous two interaction, and thus learn to get a positive valence every time in Environment5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43101d7-0805-49fc-9f17-8f84cde2308c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
