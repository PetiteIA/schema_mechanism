{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f34084-d4b8-4160-89ad-a927a0919615",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PetiteIA/schema_mechanism/blob/master/notebooks/agent13.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec58b3-e210-4a78-9610-f6615254de29",
   "metadata": {},
   "source": [
    "# L'AGENT QUI AFFICHE SA SIMULATION (en construction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ef995-42ce-44ba-9f3b-8bfa808b08fe",
   "metadata": {},
   "source": [
    "# Objectifs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8d107-0a5d-4c8a-868a-9317e823fd1c",
   "metadata": {},
   "source": [
    "Dans ce notebook nous examinons la possibilité de dotter l'agent d'un simulateur interne de ses sequances d'interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16db8-1bd6-4e82-9cf6-2b37d996ffe8",
   "metadata": {},
   "source": [
    "# Préparons les classes CompositeInteraction et Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d018f-39ae-471d-9976-0737298bc1a0",
   "metadata": {},
   "source": [
    "Nous conservons la même classe `CompositeInteraction` que l'Agent11 sauf que nous ajoutons la méthode `series()` qui renvoie la séquence des tokens des interactions primitive sous forme d'une liste.\n",
    "\n",
    "les tokens sont construits par `action * BASE + outcome`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b289ae-d5c7-45d2-8b46-9446d3de00f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3243d6-1de3-418e-9236-8af0287131da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CompositeInteraction:\n",
    "    \"\"\"A composite interaction is a tuple (pre_interaction, post_interaction) and a weight\"\"\"\n",
    "    def __init__(self, pre_interaction, post_interaction):\n",
    "        self.pre_interaction = pre_interaction\n",
    "        self.post_interaction = post_interaction\n",
    "        self.weight = 1\n",
    "        self._step = 1\n",
    "\n",
    "    def get_decision(self):\n",
    "        \"\"\"Return the flatten sequence of intermediary primitive interactions terminated with the final decision\"\"\"\n",
    "        return f\"{self.pre_interaction.sequence()}{self.post_interaction.get_decision()}\"\n",
    "\n",
    "    def get_actions(self):\n",
    "        \"\"\"Return the flat sequence of the decisions of this interaction as a string\"\"\"\n",
    "        return f\"{self.pre_interaction.get_actions()}{self.post_interaction.get_actions()}\"\n",
    "    \n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the valence of the pre_interaction plus the valence of the post_interaction\"\"\"\n",
    "        return self.pre_interaction.get_valence() + self.post_interaction.get_valence()\n",
    "\n",
    "    def reinforce(self):\n",
    "        \"\"\"Increment the composite interaction's weight\"\"\"\n",
    "        self.weight += 1\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictionary is the string '<pre_interaction><post_interaction>'. \"\"\"\n",
    "        return f\"({self.pre_interaction.key()},{self.post_interaction.key()})\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key of the pre_interaction\"\"\"\n",
    "        return self.pre_interaction.key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print the interaction in the Newick tree format (pre_interaction, post_interaction: valence) \"\"\"\n",
    "        return f\"({self.pre_interaction}, {self.post_interaction}: {self.weight})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same pre and post interactions \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return (self.pre_interaction == other.pre_interaction) and (self.post_interaction == other.post_interaction)\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_length(self):\n",
    "        \"\"\"Return the length of the number of primitive interactions in this composite interaction\"\"\"\n",
    "        return self.pre_interaction.get_length() + self.post_interaction.get_length()\n",
    "\n",
    "    def increment(self, interaction, interactions):\n",
    "        \"\"\"Increment the step of the appropriate sub-interaction. Return the enacted interaction if it is over, or None if it is ongoing.\"\"\"\n",
    "        # First step \n",
    "        if self._step == 1:\n",
    "            interaction = self.pre_interaction.increment(interaction, interactions)\n",
    "            # Ongoing pre-interaction. Return None\n",
    "            if interaction is None:\n",
    "                return None\n",
    "            # Pre-interaction succeeded. Increment the step and return None\n",
    "            elif interaction == self.pre_interaction:\n",
    "                self._step = 2\n",
    "                return None\n",
    "            # Pre-interaction failed. Reset the step and return the enacted interaction\n",
    "            else:\n",
    "                self._step = 1\n",
    "                return interaction\n",
    "        # Second step\n",
    "        else:\n",
    "            interaction = self.post_interaction.increment(interaction, interactions)\n",
    "            # Ongoing post-interaction. Return None\n",
    "            if interaction is None:\n",
    "                return None\n",
    "            # Post-interaction succeeded. Reset the step and return this interaction\n",
    "            elif interaction == self.post_interaction:\n",
    "                self._step = 1\n",
    "                return self\n",
    "            # Post-interaction failed. Reset the step and return the enacted interaction\n",
    "            else:\n",
    "                self._step = 1\n",
    "                composite_interaction = CompositeInteraction(self.pre_interaction, interaction)\n",
    "                if composite_interaction.key() not in interactions:\n",
    "                    # Add the enacted composite interaction to memory\n",
    "                    interactions[composite_interaction.key()] = composite_interaction\n",
    "                    print(f\"Learning {composite_interaction}\")\n",
    "                    return composite_interaction\n",
    "                else:\n",
    "                    # Reinforce the existing composite interaction and return it\n",
    "                    interactions[composite_interaction.key()].reinforce()\n",
    "                    print(f\"Reinforcing {interactions[composite_interaction.key()]}\")\n",
    "                    return interactions[composite_interaction.key()]\n",
    "\n",
    "    def current(self):\n",
    "        \"\"\"Return the current intended primitive interaction\"\"\"\n",
    "        # Step 1: the current primitive interaction of the pre-interaction\n",
    "        if self._step == 1:\n",
    "            return self.pre_interaction.current()\n",
    "        # Step 2: The current primitive interaction of the post-interaction\n",
    "        else:\n",
    "            return self.post_interaction.current()\n",
    "\n",
    "    def sequence(self):\n",
    "        \"\"\"Return the flat sequence of primitive interactions of this composite interaction\"\"\"\n",
    "        return f\"{self.pre_interaction.sequence()}{self.post_interaction.sequence()}\"\n",
    "\n",
    "    def get_post_interactions(self):\n",
    "        \"\"\"Return the list of the hierarchy of the sub post_interactions\"\"\"\n",
    "        return [self.post_interaction] + self.post_interaction.get_post_interactions()\n",
    "\n",
    "    def series(self):\n",
    "        \"\"\"Return the series of tokens of the primitive interactions\"\"\"\n",
    "        series = self.pre_interaction.series()\n",
    "        series.extend(self.post_interaction.series())\n",
    "        return series\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf913b2-97bf-4bfb-9647-26a18ebbf549",
   "metadata": {},
   "source": [
    "Nous conservons la mêmes classe `Interaction ` que pour l'Agent10 a part la méthode `get_post_interactions()` ajoutée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7fd8bc-9dee-4aef-a217-3ed77844e2f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, _action, _outcome, _valence):\n",
    "        self._action = _action\n",
    "        self._outcome = _outcome\n",
    "        self._valence = _valence\n",
    "        self.weight = 10\n",
    "        \n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_actions(self):\n",
    "        \"\"\"Return the action as a string for compatibilty with CompositeInteraction\"\"\"\n",
    "        return str(self._action)\n",
    "\n",
    "    def get_decision(self):\n",
    "        \"\"\"Return the decision key\"\"\"\n",
    "        return f\"{self._action}\"\n",
    "        # return f\"a{self._action}\"\n",
    "\n",
    "    def get_outcome(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._outcome\n",
    "\n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self._action}{self._outcome}\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key. Used for compatibility with CompositeInteraction\"\"\"\n",
    "        return \"\"  # self.key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self._action}{self._outcome}:{self._valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.key() == other.key()\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_length(self):\n",
    "        \"\"\"The length of the sequence of this interaction\"\"\"\n",
    "        return 1\n",
    "\n",
    "    def increment(self, interaction, interactions):\n",
    "        \"\"\"Return the enacted interaction for compatibility with composite interactions\"\"\"\n",
    "        return interaction\n",
    "\n",
    "    def current(self):\n",
    "        \"\"\"Return itself for compatibility with composite interactions\"\"\"\n",
    "        return self\n",
    "\n",
    "    def sequence(self):\n",
    "        \"\"\"Return the key. Use for compatibility with composite interactions\"\"\"\n",
    "        return self.key()\n",
    "\n",
    "    def get_post_interactions(self):\n",
    "        \"\"\"Return the empty list for compatibility with composite interactions\"\"\"\n",
    "        return []\n",
    "\n",
    "    def series(self):\n",
    "        \"\"\"Return the token of this primitive interactions in a list\"\"\"\n",
    "        return [self._action * BASE + self._outcome]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afad0a0-b639-4168-8a62-c97a323b67e8",
   "metadata": {},
   "source": [
    "# L'environnement SmallLoop "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db5ef4-5601-4278-a751-4a5eeb755bb6",
   "metadata": {},
   "source": [
    "Implémentons une version de l'environnement qui nous permet d'afficher le simulateur interne de l'agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef6497c-c828-49c6-9288-847b23474da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"sav\"\n",
    "\n",
    "FORWARD = 1\n",
    "FEEL_FRONT = 0\n",
    "FEEL_LEFT = 2\n",
    "FEEL_RIGHT = 3\n",
    "TURN_LEFT = 4\n",
    "TURN_RIGHT = 5\n",
    "\n",
    "ENV_HIGHT = 6\n",
    "ENV_WIDTH = 6\n",
    "SIM_HIGHT = 9\n",
    "SIM_WIDTH = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c153ef60-2a2c-4cfe-9896-92b47ba234e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from ipywidgets import Button, HBox,VBox, Output\n",
    "from IPython.display import display\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "EMPTY = 0\n",
    "WALL = 1\n",
    "FEELING_EMPTY = 2\n",
    "FEELING_WALL = 3\n",
    "BUMPING = 4\n",
    "UNKNOWN = 5\n",
    "\n",
    "colors = [\"#b0b0b0\", '#b0b0b0', '#ffffff', '#535865', \"#F93943\"]  # Hidden environment\n",
    "colors = [\"#D6D6D6\", '#5C946E', '#FAE2DB', '#535865', \"#F93943\", \n",
    "          \"#BAC9E1\", '#BAC9E1', '#FAE2DB', '#535865', \"#F93943\", '#BAC9E1']  # Simulator\n",
    "agent_color = \"#1976D2\"\n",
    "prediction_error_color = \"#f62dae\"\n",
    "agent_size = 200\n",
    "\n",
    "class SmallLoop():\n",
    "    def __init__(self, position, direction, grid):\n",
    "        self.environment_grid = np.array(grid)\n",
    "        self.display_grid = np.full((SIM_HIGHT, ENV_WIDTH + SIM_WIDTH), WALL, dtype=int)\n",
    "        self.display_grid[0:ENV_HIGHT, 0:ENV_WIDTH] = self.environment_grid[0:6, 0:6]\n",
    "        self.position = np.array(position) \n",
    "        self.direction = direction\n",
    "        self.cmap = ListedColormap(colors)\n",
    "        self.norm = BoundaryNorm(np.arange(-0.5, len(colors) + 0.5, 1.0), self.cmap.N)\n",
    "        self.marker_size = agent_size\n",
    "        self.marker_map = {LEFT: '<', DOWN: 'v', RIGHT: '>', UP: '^'}\n",
    "        self.marker_color = agent_color\n",
    "        self.directions = np.array([\n",
    "            [0, -1],  # Left\n",
    "            [1, 0],   # Down\n",
    "            [0, 1],   # Right\n",
    "            [-1, 0]   # Up\n",
    "            ])\n",
    "\n",
    "    def outcome(self, action):\n",
    "        \"\"\"Update the grid. Return the outcome of the action.\"\"\"\n",
    "        result = 0\n",
    "        self.display_grid[0:6, 0:6] = self.environment_grid\n",
    "\n",
    "        if action == FORWARD:  \n",
    "            target_position = self.position + self.directions[self.direction]\n",
    "            if self.environment_grid[tuple(target_position)] == EMPTY:\n",
    "                self.position[:] = target_position\n",
    "            else:\n",
    "                result = 1\n",
    "                self.display_grid[tuple(target_position)] = BUMPING\n",
    "        \n",
    "        elif action == TURN_RIGHT:\n",
    "            self.direction = {LEFT: UP, DOWN: LEFT, RIGHT: DOWN, UP: RIGHT}[self.direction]\n",
    "        \n",
    "        elif action == TURN_LEFT:\n",
    "            self.direction = {LEFT: DOWN, DOWN: RIGHT, RIGHT: UP, UP: LEFT}[self.direction]\n",
    "        \n",
    "        elif action == FEEL_FRONT:\n",
    "            feeling_position = self.position + self.directions[self.direction]\n",
    "            if self.environment_grid[tuple(feeling_position)] == EMPTY:\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                result = 1\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_WALL\n",
    "        \n",
    "        elif action == FEEL_LEFT:\n",
    "            feeling_position = self.position + self.directions[(self.direction + 1) % 4]\n",
    "            if self.environment_grid[tuple(feeling_position)] == EMPTY:\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                result = 1\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_WALL\n",
    "        \n",
    "        elif action == FEEL_RIGHT:\n",
    "            feeling_position = self.position + self.directions[self.direction - 1]\n",
    "            if self.environment_grid[tuple(feeling_position)] == EMPTY:\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                result = 1\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_WALL\n",
    "\n",
    "        # print(f\"Line: {self.position[0]}, Column: {self.position[1]}, direction: {self.direction}\")\n",
    "        return result  \n",
    "    \n",
    "    def display(self, simulator=None):\n",
    "        \"\"\"Display the grid in the notebook\"\"\"\n",
    "        out.clear_output(wait=True)\n",
    "        with out:\n",
    "            fig, ax = plt.subplots()\n",
    "            if simulator is not None:\n",
    "                plt.scatter(4 + 6, 4, s=simulator.marker_size, marker='^', c=\"#ffffff\")\n",
    "                plt.scatter(simulator.position[1] + 6, simulator.position[0], s=simulator.marker_size, marker=self.marker_map[simulator.direction], \n",
    "                            c=simulator.marker_color)\n",
    "                self.display_grid[0:SIM_HIGHT, ENV_WIDTH:(ENV_WIDTH + SIM_WIDTH + 1)] = simulator.display_grid[0:SIM_HIGHT, 0:SIM_WIDTH] + 5\n",
    "            ax.imshow(self.display_grid, cmap=self.cmap, norm=self.norm)\n",
    "            plt.scatter(self.position[1], self.position[0], s=self.marker_size, marker=self.marker_map[self.direction], c=self.marker_color)\n",
    "            ax.text(4.5, 0, f\"{step:>3}\", fontsize=10, color='White')\n",
    "            plt.show()\n",
    "    \n",
    "    def save(self, step, simulator):\n",
    "        \"\"\"Save the display as a PNG file\"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.axis('off')\n",
    "        ax.imshow(self.display_grid, cmap=self.cmap, norm=self.norm)\n",
    "        plt.scatter(self.position[1], self.position[0], s=self.marker_size, marker=self.marker_map[self.direction], c=self.marker_color)\n",
    "        plt.scatter(simulator.position[1] + 6, simulator.position[0], s=simulator.marker_size, marker=self.marker_map[simulator.direction], \n",
    "                    c=simulator.marker_color)\n",
    "        ax.text(4.5, 0, f\"{step:>4}\", fontsize=10, color='White')\n",
    "        plt.savefig(f\"{save_dir}/{step:04}.png\", bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "        plt.close(fig)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7d99c-84e6-4017-b560-ffa28752cbc6",
   "metadata": {},
   "source": [
    "# Créons le simulateur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157bb981-d999-4c55-b0a3-c764393f5785",
   "metadata": {},
   "source": [
    "Ce simulateur visualise les interactions en fonction de leur outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f3e184-18e7-434b-a9de-bdfa9131afe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator():\n",
    "    def __init__(self, position, direction):\n",
    "        self.display_grid = np.full((SIM_HIGHT, SIM_WIDTH), UNKNOWN, dtype=int)\n",
    "        self.initial_position = np.array(position) \n",
    "        self.position = np.array(position) \n",
    "        self.initial_direction = direction\n",
    "        self.direction = direction\n",
    "        self.marker_size = agent_size\n",
    "        self.marker_color = agent_color\n",
    "        self.directions = np.array([\n",
    "            [0, -1],  # Left\n",
    "            [1, 0],   # Down\n",
    "            [0, 1],   # Right\n",
    "            [-1, 0]   # Up\n",
    "            ])\n",
    "    def outcome(self, i):\n",
    "        \"\"\"Update the grid. Return the outcome of the action.\"\"\"\n",
    "        if i._action == FORWARD:  \n",
    "            target_position = self.position + self.directions[self.direction]\n",
    "            if i._outcome == EMPTY:\n",
    "                self.position[:] = target_position\n",
    "            else:\n",
    "                self.display_grid[tuple(target_position)] = BUMPING\n",
    "        elif i._action == TURN_RIGHT:\n",
    "            self.direction = {LEFT: UP, DOWN: LEFT, RIGHT: DOWN, UP: RIGHT}[self.direction]\n",
    "        elif i._action == TURN_LEFT:\n",
    "            self.direction = {LEFT: DOWN, DOWN: RIGHT, RIGHT: UP, UP: LEFT}[self.direction]\n",
    "        elif i._action == FEEL_FRONT:\n",
    "            feeling_position = self.position + self.directions[self.direction]\n",
    "            if i._outcome == EMPTY:\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_WALL\n",
    "        elif i._action == FEEL_LEFT:\n",
    "            feeling_position = self.position + self.directions[(self.direction + 1) % 4]\n",
    "            if i._outcome == EMPTY:\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_WALL\n",
    "        elif i._action == FEEL_RIGHT:\n",
    "            feeling_position = self.position + self.directions[self.direction - 1]\n",
    "            if i._outcome == EMPTY:\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                self.display_grid[tuple(feeling_position)] = FEELING_WALL\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Reset the simulator\"\"\"\n",
    "        self.position[:] = self.initial_position\n",
    "        self.direction = self.initial_direction \n",
    "        self.display_grid[:] = np.full((SIM_HIGHT, SIM_WIDTH), EMPTY, dtype=int)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0957e7b6-d560-4e4e-a8bf-0a7b62817a7a",
   "metadata": {},
   "source": [
    "# Implémentons l'agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf545ea9-a6b5-4c2a-8179-4145894e26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, _interactions):\n",
    "        \"\"\" Initialize our agent \"\"\"\n",
    "        # Les intéreactions doivent être triées dans l'ordre de leurs tokens\n",
    "        self._interactions = dict(sorted({interaction.key(): interaction for interaction in _interactions}.items()))\n",
    "        self._primitive_intended_interaction = self._interactions[\"00\"]\n",
    "        self._intended_interaction = None\n",
    "        self.simulator = Simulator([4, 4], UP)\n",
    "\n",
    "        # The context\n",
    "        self._penultimate_interaction = None\n",
    "        self._previous_interaction = None\n",
    "        self._last_interaction = None\n",
    "        self._penultimate_composite_interaction = None\n",
    "        self._previous_composite_interaction = None\n",
    "        self._last_composite_interaction = None\n",
    "        self._enacted_interaction = self._primitive_intended_interaction\n",
    "        \n",
    "        # Prepare the dataframe of proposed interactions\n",
    "        default_interactions = [interaction for interaction in _interactions if interaction.get_outcome() == 0]\n",
    "        data = {'activated': [\"\"] * len(default_interactions),\n",
    "                'weight': [0] * len(default_interactions),\n",
    "                'actions': [i.get_actions() for i in default_interactions],\n",
    "                'intention': [i.key() for i in default_interactions],\n",
    "                'valence': [i.get_valence() for i in default_interactions],\n",
    "                'decision': [i.get_decision() for i in default_interactions],\n",
    "                'length': [1] * len(default_interactions),\n",
    "                'pre': [\"\"] * len(default_interactions)} \n",
    "        self._default_df = pd.DataFrame(data)\n",
    "        self.proposed_df = None\n",
    "        self.decision_df = None\n",
    "\n",
    "    def action(self):\n",
    "        \"\"\"Implement the agent's policy\"\"\"\n",
    "        # If the intended interaction is over (completely enacted or aborted)\n",
    "        if self._enacted_interaction is not None:\n",
    "            self.simulator.clear()\n",
    "            # Memorize the context\n",
    "            self._penultimate_composite_interaction = self._previous_composite_interaction\n",
    "            self._previous_composite_interaction = self._last_composite_interaction\n",
    "            self._penultimate_interaction = self._previous_interaction\n",
    "            self._previous_interaction = self._last_interaction\n",
    "            self._last_interaction = self._enacted_interaction\n",
    "            # Call the learning mechanism\n",
    "            self.learn(self._enacted_interaction)\n",
    "            # Create the proposed dataframe\n",
    "            self.create_proposed_df()\n",
    "            self.aggregate_propositions()\n",
    "            # Decide the next enaction\n",
    "            self.decide()\n",
    "\n",
    "        # Return the next primitive action\n",
    "        self._primitive_intended_interaction = self._intended_interaction.current()\n",
    "        return self._primitive_intended_interaction.get_action()\n",
    "        \n",
    "    def learn(self, enacted_interaction):\n",
    "        \"\"\"Learn the composite interactions\"\"\"\n",
    "        # First level of composite interactions\n",
    "        self._last_composite_interaction = self.learn_composite_interaction(self._previous_interaction, enacted_interaction)\n",
    "        # Second level of composite interactions\n",
    "        self.learn_composite_interaction(self._previous_composite_interaction, enacted_interaction)\n",
    "        self.learn_composite_interaction(self._penultimate_interaction, self._last_composite_interaction)\n",
    "\n",
    "        # Higher level composite interaction made of two composite interactions\n",
    "        if self._last_composite_interaction is not None:\n",
    "            self.learn_composite_interaction(self._penultimate_composite_interaction, self._last_composite_interaction)\n",
    "\n",
    "    def learn_composite_interaction(self, pre_interaction, post_interaction):\n",
    "        \"\"\"Record or reinforce the composite interaction made of (pre_interaction, post_interaction)\"\"\"\n",
    "        if pre_interaction is None:\n",
    "            return None\n",
    "        else:\n",
    "            # If the pre-interaction exists\n",
    "            composite_interaction = CompositeInteraction(pre_interaction, post_interaction)\n",
    "            if composite_interaction.key() not in self._interactions:\n",
    "                # Add the composite interaction to memory\n",
    "                self._interactions[composite_interaction.key()] = composite_interaction\n",
    "                print(f\"Learning {composite_interaction}\")\n",
    "                return composite_interaction\n",
    "            else:\n",
    "                # Reinforce the existing composite interaction and return it\n",
    "                self._interactions[composite_interaction.key()].reinforce()\n",
    "                print(f\"Reinforcing {self._interactions[composite_interaction.key()]}\")\n",
    "                return self._interactions[composite_interaction.key()]\n",
    "\n",
    "    def create_proposed_df(self):\n",
    "        \"\"\"Create the proposed dataframe from the activated interactions\"\"\"\n",
    "        # The list of activated interactions that match the current context\n",
    "        activated_interactions = [i for i in self._interactions.values() if i.get_length() > 1 \n",
    "                                  and i.pre_interaction in self._last_composite_interaction.get_post_interactions()]\n",
    "        data = {'activated': [i.key() for i in activated_interactions],\n",
    "                'weight': [i.weight for i in activated_interactions],\n",
    "                'actions': [i.post_interaction.get_actions() for i in activated_interactions],\n",
    "                'intention': [i.post_interaction.key() for i in activated_interactions],\n",
    "                'valence': [i.post_interaction.get_valence() for i in activated_interactions],\n",
    "                'decision': [i.post_interaction.get_decision() for i in activated_interactions],\n",
    "                'pre': [i.post_interaction.pre_key() for i in activated_interactions],\n",
    "                'length': [i.post_interaction.get_length() for i in activated_interactions],\n",
    "                }\n",
    "        activated_df = pd.DataFrame(data).astype(self._default_df.dtypes)  # Force the same types in case activated_df is empty\n",
    "\n",
    "        # Create the proposed dataframe\n",
    "        self.proposed_df = pd.concat([self._default_df, activated_df], ignore_index=True).sort_values(by='decision', ascending=True).reset_index(drop=True)\n",
    "\n",
    "        # Calculate the proclivity of each proposition\n",
    "        self.proposed_df['proclivity'] = self.proposed_df['weight'] * self.proposed_df['valence']\n",
    "\n",
    "        # Compute the probability of each propositions\n",
    "        # self.proposed_df['probability'] = self.proposed_df['weight'] / self.proposed_df.groupby('actions')['weight'].transform('sum')\n",
    "        self.proposed_df['probability'] = self.proposed_df.groupby('intention')['weight'].transform('sum') / self.proposed_df.groupby('actions')['weight'].transform('sum')\n",
    "\n",
    "    def aggregate_propositions(self):\n",
    "        \"\"\"Aggregate the proclivity\"\"\"\n",
    "        # Aggregate the proclivity for each decision\n",
    "        grouped_df = self.proposed_df.groupby('decision').agg({'proclivity': 'sum', 'actions': 'first', # 'action': 'first', \n",
    "                                                               'length': 'first', 'intention': 'first', 'pre': 'first'}).reset_index()\n",
    "        # For each proposed composite decision \n",
    "        for index, proposed in grouped_df[grouped_df['length'] > 1].iterrows():\n",
    "            # print(f\"Index {index}, actions {proposition['actions']}, intention {proposition['intention']}\")\n",
    "            # Find shorter decisions that start with the same sequence \n",
    "            for _, shorter in self.proposed_df[self.proposed_df.apply(lambda row: proposed['actions'].startswith(row['actions']) \n",
    "                                                                      and row['length'] < proposed['length'], axis=1)].iterrows():\n",
    "                # Add the proclivity of the shorter decisions\n",
    "                grouped_df.loc[index, 'proclivity'] += shorter['proclivity']\n",
    "                # print(f\"Decision {proposed['decision']} recieves {shorter['proclivity']} from shorter {shorter['intention']}\")\n",
    "        \n",
    "        # Sort by descending proclivity\n",
    "        self.decision_df = grouped_df.sort_values(by=['proclivity', 'decision'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    def decide(self):\n",
    "        \"\"\"Selects the intended_interaction at the top of the proposed dataframe\"\"\"\n",
    "        # The intended interaction is in the first row because it has been sorted by descending proclivity\n",
    "        intended_interaction_key = self.decision_df.loc[0, 'intention']\n",
    "        print(\"Intention:\", intended_interaction_key)\n",
    "        self._intended_interaction = self._interactions[intended_interaction_key]\n",
    "\n",
    "    def assimilate(self, outcome):\n",
    "        \"\"\"Process the last outcome\"\"\"\n",
    "        # Trace the previous cycle\n",
    "        primitive_enacted_interaction = self._interactions[f\"{self._primitive_intended_interaction.get_action()}{outcome}\"]\n",
    "        prediction_correct = self._primitive_intended_interaction.get_outcome() == outcome\n",
    "        print(\n",
    "            f\"Action: {self._primitive_intended_interaction.get_action()}, Predicted: {self._primitive_intended_interaction.get_outcome()}, \"\n",
    "            f\"Outcome: {outcome}, Prediction_correct: {prediction_correct}, \"\n",
    "            f\"Valence: {primitive_enacted_interaction.get_valence()}\")\n",
    "        # Follow up the enaction\n",
    "        self._enacted_interaction = self._intended_interaction.increment(primitive_enacted_interaction, self._interactions)\n",
    "        # Update the simulator\n",
    "        self.simulator.outcome(primitive_enacted_interaction)\n",
    "        if prediction_correct:\n",
    "            self.simulator.marker_color = agent_color\n",
    "        else:\n",
    "            self.simulator.marker_color = prediction_error_color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0188a-e490-4c14-b3d1-50868d0ccd3f",
   "metadata": {},
   "source": [
    "# Testons l'agent dans le Small Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59db638-c069-4589-ae5f-fe50f0fac89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea9f91bff79489e8dae3b22acb97ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciate the small loop environment\n",
    "grid = [[WALL, WALL , WALL , WALL , WALL , WALL],\n",
    "        [WALL, EMPTY, EMPTY, EMPTY, WALL , WALL],\n",
    "        [WALL, EMPTY, WALL , EMPTY, EMPTY, WALL],\n",
    "        [WALL, EMPTY, WALL , WALL , EMPTY, WALL],\n",
    "        [WALL, EMPTY, EMPTY, EMPTY, EMPTY, WALL],\n",
    "        [WALL, WALL , WALL , WALL , WALL , WALL]]\n",
    "e = SmallLoop([1, 1], 0, grid)\n",
    "\n",
    "# Instanciate the agent \n",
    "interactions = [\n",
    "    Interaction(FORWARD,0,5),\n",
    "    Interaction(FORWARD,1,-10),\n",
    "    Interaction(TURN_LEFT,0,-3),\n",
    "    Interaction(TURN_LEFT,1,-3),\n",
    "    Interaction(TURN_RIGHT,0,-3),\n",
    "    Interaction(TURN_RIGHT,1,-3),\n",
    "    Interaction(FEEL_FRONT,0,-1),\n",
    "    Interaction(FEEL_FRONT,1,-1),\n",
    "    Interaction(FEEL_LEFT,0,-1),\n",
    "    Interaction(FEEL_LEFT,1,-1),\n",
    "    Interaction(FEEL_RIGHT,0,-1),\n",
    "    Interaction(FEEL_RIGHT,1,-1)\n",
    "]\n",
    "a = Agent(interactions)\n",
    "\n",
    "# Initialize the interaction loop\n",
    "step = 0\n",
    "outcome = 0\n",
    "\n",
    "# Display\n",
    "out = Output()\n",
    "e.display(a.simulator)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e38a590-b256-433f-a1d9-100cc7b0a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(1000):\n",
    "    print(f\"Step: {step}\")\n",
    "    # step += 1\n",
    "    action = a.action()\n",
    "    outcome = e.outcome(action)\n",
    "    a.assimilate(outcome)\n",
    "    e.display(a.simulator)\n",
    "    e.save(step, a.simulator)  # Save the image file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f1d53-ab19-4118-a24b-3a740b47ee70",
   "metadata": {},
   "source": [
    "![video13](video13.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b0a5df-9f3b-4355-87af-1ecb9153658f",
   "metadata": {},
   "source": [
    "_Video 1: Exemple_\n",
    "\n",
    "La partie gauche représente l'agent dans l'environnement. \n",
    "La partie droite représente les séquences enactées du point de vue de l'agent. \n",
    "L'agent apparait en couleur magenta en cas d'erreur de prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3207837-462b-43eb-9057-55be487e145e",
   "metadata": {},
   "source": [
    "# Extrayons les séquences apprises "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2492,
   "id": "5b66c7c4-d5ed-4c8c-961e-7c5506185c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences to train the LSTM:\n",
      "Sequences to train the next token prediction Transformer:\n",
      "[] ,\n",
      "[] ,\n",
      "[] ,\n",
      "[] ,\n",
      "[] ,\n",
      "[] ,\n",
      "[] ,\n",
      "[] ,\n",
      "[] ,\n",
      "Sequences to train the seq2seq Transformer:\n",
      "Sequences to visualize the decoder:\n"
     ]
    }
   ],
   "source": [
    "# Le token End Of Sequence = 11\n",
    "EOS_TOKEN = 11\n",
    "\n",
    "def sequences(agent):\n",
    "    \"\"\"Entrain le LSTM avec toutes les interactions composite\"\"\"\n",
    "    # Pour toutes les interactions composite jusqu'a la longuer 6\n",
    "    print(\"Sequences to train the LSTM:\")\n",
    "    for l in range(2, 6):\n",
    "        x = [i.series()[:-1] for i in agent._interactions.values() if i.get_length() == l]\n",
    "        # print(\"x\", x)\n",
    "        y = [i.series()[-1] for i in agent._interactions.values() if i.get_length() == l]\n",
    "        # print(\"y\", y)\n",
    "\n",
    "    print(\"Sequences to train the next token prediction Transformer:\")\n",
    "    for l in range(2, 11):\n",
    "        print([i.series() for i in agent._interactions.values() if i.get_length() == l], \",\")\n",
    "\n",
    "    print(\"Sequences to train the seq2seq Transformer:\")\n",
    "    # print([[i.pre_interaction.series(), i.post_interaction.series()] for i in agent._interactions.values() for l in range(6, 20) if i.get_length() == l ])\n",
    "\n",
    "    print(\"Sequences to visualize the decoder:\")\n",
    "    # print([i.pre_interaction.series() for i in agent._interactions.values() for l in range(10, 18) if i.get_length() == l ])\n",
    "    # print([i.post_interaction.series() for i in agent._interactions.values() for l in range(10, 18) if i.get_length() == l ])\n",
    "\n",
    "sequences(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b39704-f1bb-46dd-9aee-a379e43aecf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403af118-e226-4e3d-8812-8b066a5a6a87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
