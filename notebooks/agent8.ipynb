{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f34084-d4b8-4160-89ad-a927a0919615",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PetiteIA/schema_mechanism/blob/master/notebooks/agent7.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec58b3-e210-4a78-9610-f6615254de29",
   "metadata": {},
   "source": [
    "# L'AGENT QUI ESPERAIT GAGNER AU COUP SUIVANT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ef995-42ce-44ba-9f3b-8bfa808b08fe",
   "metadata": {},
   "source": [
    "# Objectifs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14a47d-ca87-4080-8f84-afcd2bc83170",
   "metadata": {},
   "source": [
    "Ce tutoriel montre comment implémenter un agent qui prend une décision basée sur l'espérence de valence des deux prochains cycles d'interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16db8-1bd6-4e82-9cf6-2b37d996ffe8",
   "metadata": {},
   "source": [
    "# Préparons les classes Interaction et CompositeInteraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7e5da-0188-4924-92f4-3ecbfc2fce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install ipywidgets\n",
    "!pip install IPython.display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d018f-39ae-471d-9976-0737298bc1a0",
   "metadata": {},
   "source": [
    "Même classe Interaction que pour l'Agent7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "ca7fd8bc-9dee-4aef-a217-3ed77844e2f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, _action, _outcome, _valence):\n",
    "        self._action = _action\n",
    "        self._outcome = _outcome\n",
    "        self._valence = _valence\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_decision(self):\n",
    "        \"\"\"Return the decision key\"\"\"\n",
    "        return f\"a{self._action}\"\n",
    "\n",
    "    def get_primitive_action(self):\n",
    "        \"\"\"Return the action for compatibility with CompositeInteraction\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_outcome(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._outcome\n",
    "\n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self._action}{self._outcome}\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key. Used for compatibility with CompositeInteraction\"\"\"\n",
    "        return self.key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self._action}{self._outcome}:{self._valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.key() == other.key()\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa086c1-5987-4375-939a-aa217b6048f6",
   "metadata": {},
   "source": [
    "Même classe CompositeInteraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "ef3243d6-1de3-418e-9236-8af0287131da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CompositeInteraction:\n",
    "    \"\"\"A composite interaction is a tuple (pre_interaction, post_interaction) and a weight\"\"\"\n",
    "    def __init__(self, pre_interaction, post_interaction):\n",
    "        self.pre_interaction = pre_interaction\n",
    "        self.post_interaction = post_interaction\n",
    "        self.weight = 1\n",
    "        self.isActivated = False\n",
    "\n",
    "    def get_decision(self):\n",
    "        \"\"\"Return the sequence of decisions\"\"\"\n",
    "        # return self.key()\n",
    "        return f\"{self.pre_interaction.key()}{self.post_interaction.get_decision()}\"\n",
    "\n",
    "    def get_primitive_action(self):\n",
    "        \"\"\"Return the primite action\"\"\"\n",
    "        return self.pre_interaction.get_primitive_action()\n",
    "\n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the valence of the pre_interaction plus the valence of the post_interaction\"\"\"\n",
    "        return self.pre_interaction.get_valence() + self.post_interaction.get_valence()\n",
    "\n",
    "    def reinforce(self):\n",
    "        \"\"\"Increment the composite interaction's weight\"\"\"\n",
    "        self.weight += 1\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictionary is the string '<pre_interaction><post_interaction>'. \"\"\"\n",
    "        return f\"({self.pre_interaction.key()},{self.post_interaction.key()})\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key of the pre_interaction\"\"\"\n",
    "        return self.pre_interaction.pre_key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print the interaction in the Newick tree format (pre_interaction, post_interaction: valence) \"\"\"\n",
    "        return f\"({self.pre_interaction}, {self.post_interaction}: {self.weight})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same pre and post interactions \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return (self.pre_interaction == other.pre_interaction) and (self.post_interaction == other.post_interaction)\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152512a3-ef97-473a-91a7-9a36fe70ba33",
   "metadata": {},
   "source": [
    "# Définissons l'agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d65a49-9e8a-4834-b064-d9fff651abdc",
   "metadata": {},
   "source": [
    "Implémentons un agent qui calcule la valence attendue sur la base d'une anticipation de deux pas de temps.\n",
    "\n",
    "La Figure 1 illustre ce mécanisme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e95995-8077-46ed-a03d-e318623cb82b",
   "metadata": {},
   "source": [
    "![Agent5](img/Figure_1_Agent7_3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742aecbc-5e94-4cc6-bf5d-1b6c12bde814",
   "metadata": {},
   "source": [
    "Figure 1: L'agent calcule une valence attendue pour chaque interaction proposée avant de les aggréger.\n",
    "\n",
    "Les interactions proposées sont aggrégées par leur décision $d$.\n",
    "La valence attendue $\\mathbb{E}(V_d)$ est calculée en fonction de la valence attendue de chaque interaction pouvant résulter de cette décision et leurs probabilités de survenir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c689e-8a92-472d-9374-5ebf75be2392",
   "metadata": {},
   "source": [
    "## Calcul de la valence attendue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b1b654-9015-465d-b997-d0e662217c88",
   "metadata": {},
   "source": [
    "La valence attendue (_expected valence_) $\\mathbb{E}(V_d)$ est la somme des valence des interactions pouvant résulter de cette décision multipliées par la probabilité d'enacter chacune de ces interaction :\n",
    "\n",
    "$\\displaystyle \\mathbb{E}(V_d) = \\sum_{i \\in I_d} v_{i} \\cdot \\hat{p}_{i} $\n",
    "\n",
    "Ici nous simplifions ce calcul en ne considérant que deux interactions pouvant résulter de la décision $d$ :\n",
    "* L'interaction qui consiste à enacter entièrement la décision $d$.\n",
    "* L'interaction qui résulte d'un échec de la première interaction primitive de la décision $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0957e7b6-d560-4e4e-a8bf-0a7b62817a7a",
   "metadata": {},
   "source": [
    "# Examinons l'agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "id": "bf545ea9-a6b5-4c2a-8179-4145894e26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, _interactions):\n",
    "        \"\"\" Initialize our agent \"\"\"\n",
    "        self._interactions = {interaction.key(): interaction for interaction in _interactions}\n",
    "        self._composite_interactions = {}\n",
    "        self._intended_interaction = self._interactions[\"00\"]\n",
    "        self._last_interaction = None\n",
    "        self._previous_interaction = None\n",
    "        self._penultimate_interaction = None\n",
    "        self._last_composite_interaction = None\n",
    "        self._previous_composite_interaction = None\n",
    "        # Create a dataframe of default primitive interactions\n",
    "        default_interactions = [interaction for interaction in _interactions if interaction.get_outcome() == 0]\n",
    "        data = {'activated': [np.nan] * len(default_interactions),\n",
    "                'weight': [0] * len(default_interactions),\n",
    "                'action': [i.get_primitive_action() for i in default_interactions],\n",
    "                'interaction': [i.key() for i in default_interactions],\n",
    "                'valence': [i.get_valence() for i in default_interactions],\n",
    "                'decision': [i.get_decision() for i in default_interactions],\n",
    "                'proclivity': [0] * len(default_interactions)}\n",
    "        self.primitive_df = pd.DataFrame(data)\n",
    "        # Store the selection dataframe as a class attribute so we can display it in the notebook\n",
    "        self.proposed_df = None\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\"Implement the agent's policy\"\"\"\n",
    "        # Memorize the context\n",
    "        self._previous_composite_interaction = self._last_composite_interaction\n",
    "        self._penultimate_interaction = self._previous_interaction\n",
    "        self._previous_interaction = self._last_interaction\n",
    "        self._last_interaction = self._interactions[f\"{self._intended_interaction.get_action()}{_outcome}\"]\n",
    "\n",
    "        # tracing the previous cycle\n",
    "        print(\n",
    "            f\"Action: {self._intended_interaction.get_action()}, Prediction: {self._intended_interaction.get_outcome()}, \"\n",
    "            f\"Outcome: {_outcome}, Prediction_correct: {self._intended_interaction.get_outcome() == _outcome}, \"\n",
    "            f\"Valence: {self._last_interaction.get_valence()}\")\n",
    "\n",
    "        # Call the learning mechanism\n",
    "        self.learn()\n",
    "\n",
    "        # Create the proposed dataframe\n",
    "        self.create_proposed_df()\n",
    "        self.aggregate_propositions()\n",
    "\n",
    "        # Select the intended primitive interaction\n",
    "        self.decide()\n",
    "\n",
    "        return self._intended_interaction.get_action()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Learn the composite interactions\"\"\"\n",
    "        # First level of composite interactions\n",
    "        self._last_composite_interaction = self.learn_composite_interaction(self._previous_interaction,\n",
    "                                                                            self._last_interaction)\n",
    "        # Second level of composite interactions\n",
    "        self.learn_composite_interaction(self._previous_composite_interaction, self._last_interaction)\n",
    "        self.learn_composite_interaction(self._penultimate_interaction, self._last_composite_interaction)\n",
    "\n",
    "    def learn_composite_interaction(self, pre_interaction, post_interaction):\n",
    "        \"\"\"Record or reinforce the composite interaction made of (pre_interaction, post_interaction)\"\"\"\n",
    "        if pre_interaction is None:\n",
    "            return None\n",
    "        else:\n",
    "            # If the pre interaction exist\n",
    "            composite_interaction = CompositeInteraction(pre_interaction, post_interaction)\n",
    "            if composite_interaction.key() not in self._composite_interactions:\n",
    "                # Add the composite interaction to memory\n",
    "                self._composite_interactions[composite_interaction.key()] = composite_interaction\n",
    "                print(f\"Learning {composite_interaction}\")\n",
    "                return composite_interaction\n",
    "            else:\n",
    "                # Reinforce the existing composite interaction and return it\n",
    "                self._composite_interactions[composite_interaction.key()].reinforce()\n",
    "                print(f\"Reinforcing {self._composite_interactions[composite_interaction.key()]}\")\n",
    "                return self._composite_interactions[composite_interaction.key()]\n",
    "\n",
    "    def create_proposed_df(self):\n",
    "        \"\"\"Create the proposed dataframe from the activated interactions\"\"\"\n",
    "        # The list of activated interaction that match the current context\n",
    "        activated_keys = [composite_interaction.key() for composite_interaction in\n",
    "                          self._composite_interactions.values()\n",
    "                          if composite_interaction.pre_interaction == self._last_interaction or\n",
    "                          composite_interaction.pre_interaction == self._last_composite_interaction]\n",
    "        data = {'activated': activated_keys,\n",
    "                'weight': [self._composite_interactions[k].weight for k in activated_keys],\n",
    "                'action': [self._composite_interactions[k].post_interaction.get_primitive_action() for k in activated_keys],\n",
    "                'interaction': [self._composite_interactions[k].post_interaction.pre_key() for k in activated_keys],\n",
    "                'valence': [self._composite_interactions[k].post_interaction.get_valence() for k in activated_keys],\n",
    "                'decision': [self._composite_interactions[k].post_interaction.get_decision() for k in activated_keys],\n",
    "                }\n",
    "        activated_df = pd.DataFrame(data)\n",
    "\n",
    "        # Create the selection dataframe from the primitive and the activated dataframes\n",
    "        self.proposed_df = pd.concat([self.primitive_df, activated_df], ignore_index=True)\n",
    "\n",
    "        # Compute the proclivity for each proposition\n",
    "        self.proposed_df['proclivity'] = self.proposed_df['weight'] * self.proposed_df['valence']\n",
    "\n",
    "    def aggregate_propositions(self):\n",
    "        \"\"\"Aggregate the proclivity\"\"\"\n",
    "        # Compute the proclivity for each action\n",
    "        grouped_df = self.proposed_df.groupby('decision').agg({'proclivity': 'sum'}).reset_index()\n",
    "        self.proposed_df = self.proposed_df.merge(grouped_df, on='decision', suffixes=('', '_agg'))\n",
    "        # Sort by descending order of proclivity\n",
    "        self.proposed_df = self.proposed_df.sort_values(by=['proclivity_agg', 'decision'], ascending=[False, False])\n",
    "\n",
    "        # Find the most probable primitive interaction for each action\n",
    "        max_weight_df = self.proposed_df.loc[self.proposed_df.groupby('decision')['weight'].idxmax(), ['decision', 'interaction']].reset_index(\n",
    "            drop=True)\n",
    "        max_weight_df.columns = ['decision', 'intended']\n",
    "        self.proposed_df = self.proposed_df.merge(max_weight_df, on='decision')\n",
    "        \n",
    "    def decide(self):\n",
    "        \"\"\"Selects the intended interaction from the proposed dataframe\"\"\"\n",
    "        # Find the row that has the highest proclivity\n",
    "        max_index = self.proposed_df['proclivity_agg'].idxmax()\n",
    "        # Find the intended interaction in the row that has the highest proclivity\n",
    "        intended_interaction_key = self.proposed_df.loc[max_index, ['intended']].values[0]\n",
    "        self._intended_interaction = self._interactions[intended_interaction_key]\n",
    "        print(\"Intended\", self._intended_interaction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afad0a0-b639-4168-8a62-c97a323b67e8",
   "metadata": {},
   "source": [
    "## Implémentons l'environnement de la petite boucle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15624ba-33e2-4b0c-825c-6a01195d7ad6",
   "metadata": {},
   "source": [
    "On crée l'environnement Small Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "id": "c153ef60-2a2c-4cfe-9896-92b47ba234e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from ipywidgets import Button, HBox,VBox, Output\n",
    "from IPython.display import display\n",
    "\n",
    "FORWARD = 0\n",
    "TURN_LEFT = 1\n",
    "TURN_RIGHT = 2\n",
    "FEEL_FRONT = 3\n",
    "FEEL_LEFT = 4\n",
    "FEEL_RIGHT = 5\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "FEELING = 2\n",
    "BUMPING = 3\n",
    "\n",
    "class SmallLoop():\n",
    "    def __init__(self, poX, poY, direction):\n",
    "        self.grid = np.array([\n",
    "            [1, 0, 0, 0, 0, 1]\n",
    "        ])\n",
    "        self.maze = self.grid.copy()\n",
    "        self.poX = poX\n",
    "        self.poY = poY\n",
    "        self.direction = direction\n",
    "        self.cmap = ListedColormap(['white', 'green', 'yellow', 'red'])\n",
    "        self.norm = BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5], self.cmap.N)\n",
    "\n",
    "    def outcome(self, action):\n",
    "        # print('before:', self.agent_position.strPosition(), action_dcit[action])\n",
    "        self.maze[:,:] = self.grid\n",
    "        result = 0\n",
    "        \n",
    "        if action == FORWARD:  # move forward\n",
    "            # print('the action is move forward')\n",
    "            # print(str(self.position.pointX)+': '+ str(self.position.pointY)+ ' ' +self.direction, action)\n",
    "        \n",
    "            if self.direction == LEFT:\n",
    "                if self.maze[self.poX][self.poY - 1] == 0:\n",
    "                    self.poY -= 1\n",
    "                else:\n",
    "                    self.maze[self.poX][self.poY - 1] = BUMPING\n",
    "                    result = 1\n",
    "            elif self.direction == DOWN:\n",
    "                if self.maze[self.poX + 1][self.poY] == 0:\n",
    "                    self.poX += 1\n",
    "                else:\n",
    "                    self.maze[self.poX + 1][self.poY] = BUMPING\n",
    "                    result = 1\n",
    "            elif self.direction == RIGHT:\n",
    "                if self.maze[self.poX][self.poY + 1] == 0:\n",
    "                    self.poY += 1\n",
    "                else:\n",
    "                    self.maze[self.poX][self.poY + 1] = BUMPING\n",
    "                    result = 1\n",
    "            elif self.direction == UP:\n",
    "                if self.maze[self.poX - 1][self.poY] == 0:\n",
    "                    self.poX -= 1\n",
    "                else:\n",
    "                    self.maze[self.poX - 1][self.poY] = BUMPING\n",
    "                    result = 1\n",
    "            # print(str(self.position.pointX)+': '+ str(self.position.pointY)+ ' ' +self.direction, action)\n",
    "        elif action == TURN_RIGHT:\n",
    "            if self.direction == LEFT:\n",
    "                self.direction = UP\n",
    "            elif self.direction == DOWN:\n",
    "                self.direction = LEFT\n",
    "            elif self.direction == RIGHT:\n",
    "                self.direction = DOWN\n",
    "            elif self.direction == UP:\n",
    "                self.direction = RIGHT\n",
    "        elif action == TURN_LEFT:\n",
    "            if self.direction == LEFT:\n",
    "                self.direction = RIGHT  # DOWN\n",
    "            elif self.direction == DOWN:\n",
    "                self.direction = RIGHT\n",
    "            elif self.direction == RIGHT:\n",
    "                self.direction = LEFT  # UP\n",
    "            elif self.direction == UP:\n",
    "                self.direction = LEFT\n",
    "        elif action == FEEL_FRONT:\n",
    "            if self.direction == LEFT:\n",
    "                if self.maze[self.poX][self.poY - 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY - 1] = FEELING\n",
    "            elif self.direction == DOWN:\n",
    "                if self.maze[self.poX + 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX + 1][self.poY] = FEELING\n",
    "            elif self.direction == RIGHT:\n",
    "                if self.maze[self.poX][self.poY + 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY + 1] = FEELING\n",
    "            elif self.direction == UP:\n",
    "                if self.maze[self.poX - 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX - 1][self.poY] = FEELING\n",
    "        elif action == FEEL_LEFT:\n",
    "            if self.direction == LEFT:\n",
    "                if self.maze[self.poX + 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX + 1][self.poY] = FEELING\n",
    "            elif self.direction == DOWN:\n",
    "                if self.maze[self.poX][self.poY + 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY + 1] = FEELING\n",
    "            elif self.direction == RIGHT:\n",
    "                if self.maze[self.poX - 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX - 1][self.poY] = FEELING\n",
    "            elif self.direction == UP:\n",
    "                if self.maze[self.poX][self.poY - 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY - 1] = FEELING\n",
    "        elif action == FEEL_RIGHT:\n",
    "            if self.direction == LEFT:\n",
    "                if self.maze[self.poX - 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX - 1][self.poY] = FEELING\n",
    "            elif self.direction == DOWN:\n",
    "                if self.maze[self.poX][self.poY - 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY - 1] = FEELING\n",
    "            elif self.direction == RIGHT:\n",
    "                if self.maze[self.poX + 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX + 1][self.poY] = FEELING\n",
    "            elif self.direction == UP:\n",
    "                if self.maze[self.poX][self.poY + 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY + 1] = FEELING\n",
    "        print(f\"Line: {self.poX}, Column: {self.poY}, direction: {self.direction}\")\n",
    "        # return self.position,\n",
    "\n",
    "        return result  \n",
    "    \n",
    "    def display(self):\n",
    "        out.clear_output(wait=True)\n",
    "        with out:\n",
    "            fig, ax = plt.subplots()\n",
    "            # ax.set_xticks([])\n",
    "            # ax.set_yticks([])\n",
    "            # ax.axis('off')\n",
    "            # ax.imshow(self.maze, cmap='Greens', vmin=0, vmax=2)\n",
    "            ax.imshow(self.maze, cmap=self.cmap, norm=self.norm)\n",
    "            if self.direction == LEFT:\n",
    "                # Y is column and X is line\n",
    "                plt.scatter(self.poY, self.poX, s=400, marker='<')\n",
    "            elif self.direction == DOWN:\n",
    "                plt.scatter(self.poY, self.poX, s=400, marker='v')\n",
    "            elif self.direction == RIGHT:\n",
    "                plt.scatter(self.poY, self.poX, s=400, marker='>')\n",
    "            else: # UP\n",
    "                plt.scatter(self.poY, self.poX, s=400, marker='^')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0188a-e490-4c14-b3d1-50868d0ccd3f",
   "metadata": {},
   "source": [
    "On instancie l'agent et l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "id": "e59db638-c069-4589-ae5f-fe50f0fac89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6f8f2750c449ff84b46351ba3a9a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciate the small loop environment\n",
    "e = SmallLoop(0, 1, 0)\n",
    "\n",
    "# Instanciate the agent \n",
    "interactions = [\n",
    "    Interaction(0,0,5),\n",
    "    Interaction(0,1,-10),\n",
    "    Interaction(1,0,-6),\n",
    "    Interaction(1,1,-6),\n",
    "    # Interaction(2,0,-2),\n",
    "    # Interaction(2,1,-2),\n",
    "    Interaction(3,0,-1),\n",
    "    Interaction(3,1,-1)\n",
    "]\n",
    "a = Agent(interactions)\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0\n",
    "\n",
    "# Display\n",
    "out = Output()\n",
    "e.display()\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "id": "6e38a590-b256-433f-a1d9-100cc7b0a191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 62\n",
      "Action: 3, Prediction: 1, Outcome: 1, Prediction_correct: True, Valence: -1\n",
      "Reinforcing (31:-1, 31:-1: 43)\n",
      "Reinforcing ((31:-1, 31:-1: 43), 31:-1: 41)\n",
      "Reinforcing (31:-1, (31:-1, 31:-1: 43): 41)\n",
      "Intended 31:-1\n",
      "Line: 0, Column: 4, direction: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activated</th>\n",
       "      <th>weight</th>\n",
       "      <th>action</th>\n",
       "      <th>interaction</th>\n",
       "      <th>valence</th>\n",
       "      <th>decision</th>\n",
       "      <th>proclivity</th>\n",
       "      <th>proclivity_agg</th>\n",
       "      <th>intended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(31,(31,10))</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>-7</td>\n",
       "      <td>31a1</td>\n",
       "      <td>-7</td>\n",
       "      <td>-7</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(31,(10,30))</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-7</td>\n",
       "      <td>10a3</td>\n",
       "      <td>-7</td>\n",
       "      <td>-7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00</td>\n",
       "      <td>5</td>\n",
       "      <td>a0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(31,01)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>01</td>\n",
       "      <td>-10</td>\n",
       "      <td>a0</td>\n",
       "      <td>-10</td>\n",
       "      <td>-10</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-6</td>\n",
       "      <td>a1</td>\n",
       "      <td>0</td>\n",
       "      <td>-12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(31,10)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-6</td>\n",
       "      <td>a1</td>\n",
       "      <td>-6</td>\n",
       "      <td>-12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>((31,31),10)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-6</td>\n",
       "      <td>a1</td>\n",
       "      <td>-6</td>\n",
       "      <td>-12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(31,(01,10))</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>01</td>\n",
       "      <td>-16</td>\n",
       "      <td>01a1</td>\n",
       "      <td>-16</td>\n",
       "      <td>-16</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(31,(31,31))</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>-2</td>\n",
       "      <td>31a3</td>\n",
       "      <td>-82</td>\n",
       "      <td>-82</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>a3</td>\n",
       "      <td>0</td>\n",
       "      <td>-84</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(31,31)</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>a3</td>\n",
       "      <td>-43</td>\n",
       "      <td>-84</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>((31,31),31)</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>a3</td>\n",
       "      <td>-41</td>\n",
       "      <td>-84</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       activated  weight  action interaction  valence decision  proclivity  \\\n",
       "0   (31,(31,10))       1       3          31       -7     31a1          -7   \n",
       "1   (31,(10,30))       1       1          10       -7     10a3          -7   \n",
       "2            NaN       0       0          00        5       a0           0   \n",
       "3        (31,01)       1       0          01      -10       a0         -10   \n",
       "4            NaN       0       1          10       -6       a1           0   \n",
       "5        (31,10)       1       1          10       -6       a1          -6   \n",
       "6   ((31,31),10)       1       1          10       -6       a1          -6   \n",
       "7   (31,(01,10))       1       0          01      -16     01a1         -16   \n",
       "8   (31,(31,31))      41       3          31       -2     31a3         -82   \n",
       "9            NaN       0       3          30       -1       a3           0   \n",
       "10       (31,31)      43       3          31       -1       a3         -43   \n",
       "11  ((31,31),31)      41       3          31       -1       a3         -41   \n",
       "\n",
       "    proclivity_agg intended  \n",
       "0               -7       31  \n",
       "1               -7       10  \n",
       "2              -10       01  \n",
       "3              -10       01  \n",
       "4              -12       10  \n",
       "5              -12       10  \n",
       "6              -12       10  \n",
       "7              -16       01  \n",
       "8              -82       31  \n",
       "9              -84       31  \n",
       "10             -84       31  \n",
       "11             -84       31  "
      ]
     },
     "execution_count": 1074,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "action = a.action(outcome)\n",
    "e.display()\n",
    "outcome = e.outcome(action)\n",
    "a.proposed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a140b-952b-4b55-9a84-89792a145acd",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be10adae-f3cc-4f22-bb7a-f9e65caf66a1",
   "metadata": {},
   "source": [
    "## Implémentons l'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "3f0bf369-48ef-48d7-a92b-3c7ef8dc1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, _interactions):\n",
    "        \"\"\" Initialize our agent \"\"\"\n",
    "        self._interactions = {interaction.key(): interaction for interaction in _interactions}\n",
    "        self._composite_interactions = {}\n",
    "        self._intended_interaction = self._interactions[\"00\"]\n",
    "        self._last_interaction = None\n",
    "        self._previous_interaction = None\n",
    "        self._penultimate_interaction = None\n",
    "        self._last_composite_interaction = None\n",
    "        self._previous_composite_interaction = None\n",
    "        # Create a dataframe of default primitive interactions\n",
    "        default_interactions = [interaction for interaction in _interactions if interaction.get_outcome() == 0]\n",
    "        data = {'proposed': [i.key() for i in default_interactions],\n",
    "                'E(Vi)': [0.] * len(default_interactions),\n",
    "                'action': [i.get_action() for i in default_interactions],\n",
    "                'E(Va)': [0.] * len(default_interactions),\n",
    "                'interaction': [i.key() for i in default_interactions],\n",
    "                'weight': [0] * len(default_interactions)}\n",
    "        self.primitive_df = pd.DataFrame(data)\n",
    "        # Store the selection dataframe as a class attribute so we can display it in the notebook\n",
    "        self.selection_df = None\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\"Implement the agent's policy\"\"\"\n",
    "        # Memorize the context\n",
    "        self._previous_composite_interaction = self._last_composite_interaction\n",
    "        self._penultimate_interaction = self._previous_interaction\n",
    "        self._previous_interaction = self._last_interaction\n",
    "        self._last_interaction = self._interactions[f\"{self._intended_interaction.get_action()}{_outcome}\"]\n",
    "\n",
    "        # tracing the previous cycle\n",
    "        print(\n",
    "            f\"Action: {self._intended_interaction.get_action()}, Prediction: {self._intended_interaction.get_outcome()}, \"\n",
    "            f\"Outcome: {_outcome}, Prediction_correct: {self._intended_interaction.get_outcome() == _outcome}, \"\n",
    "            f\"Valence: {self._last_interaction.get_valence()}\")\n",
    "\n",
    "        # Call the learning mechanism\n",
    "        self.learn()\n",
    "\n",
    "        # Calculate the proposed dataframe\n",
    "        self.calculate_proposed_df()\n",
    "\n",
    "        # Select the intended primitive interaction\n",
    "        self.select_intended_interaction()\n",
    "\n",
    "        return self._intended_interaction.get_action()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Learn the composite interactions\"\"\"\n",
    "        # First level of composite interactions\n",
    "        self._last_composite_interaction = self.learn_composite_interaction(self._previous_interaction,\n",
    "                                                                            self._last_interaction)\n",
    "        # Second level of composite interactions\n",
    "        self.learn_composite_interaction(self._previous_composite_interaction, self._last_interaction)\n",
    "        self.learn_composite_interaction(self._penultimate_interaction, self._last_composite_interaction)\n",
    "\n",
    "    def learn_composite_interaction(self, pre_interaction, post_interaction):\n",
    "        if pre_interaction is None:\n",
    "            return None\n",
    "        else:\n",
    "            # Record or reinforce the first level composite interaction\n",
    "            composite_interaction = CompositeInteraction(pre_interaction, post_interaction)\n",
    "            if composite_interaction.key() not in self._composite_interactions:\n",
    "                self._composite_interactions[composite_interaction.key()] = composite_interaction\n",
    "                print(f\"Learning {composite_interaction}\")\n",
    "                return composite_interaction\n",
    "            else:\n",
    "                self._composite_interactions[composite_interaction.key()].reinforce()\n",
    "                print(f\"Reinforcing {self._composite_interactions[composite_interaction.key()]}\")\n",
    "                # Retrieve the existing composite interaction\n",
    "                return self._composite_interactions[composite_interaction.key()]\n",
    "\n",
    "    def calculate_proposed_df(self):\n",
    "        \"\"\"Select the action that has the highest expected valence\"\"\"\n",
    "\n",
    "        # The activated composite interactions\n",
    "        activated_keys = [composite_interaction.key() for composite_interaction in self._composite_interactions.values()\n",
    "                          if composite_interaction.pre_interaction == self._last_interaction or\n",
    "                          composite_interaction.pre_interaction == self._last_composite_interaction]\n",
    "\n",
    "        # Create the dataframe of sequences\n",
    "        series_df = pd.DataFrame(columns=['proposed', 'weight', 'a_t', 'i_t', 'a_t+1', 'i_t+1'])\n",
    "        for k in activated_keys:\n",
    "            new_row = {'proposed': self._composite_interactions[k].post_interaction.key(),\n",
    "                       'weight': self._composite_interactions[k].weight,\n",
    "                       'a_t': self._composite_interactions[k].post_interaction.get_primitive_action()}\n",
    "            if type(self._composite_interactions[k].post_interaction) == Interaction:\n",
    "                new_row['i_t'] = self._composite_interactions[k].post_interaction.key()\n",
    "            else:\n",
    "                new_row['i_t'] = self._composite_interactions[k].post_interaction.pre_interaction.key()\n",
    "                new_row['a_t+1'] = self._composite_interactions[k].post_interaction.post_interaction.get_primitive_action()\n",
    "                new_row['i_t+1'] = self._composite_interactions[k].post_interaction.post_interaction.key()\n",
    "            series_df = pd.concat([series_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        # print(series_df)\n",
    "\n",
    "        # The probability P(it|at)\n",
    "        total_by_i = series_df.groupby([\"a_t\", \"i_t\"], as_index=False)[\"weight\"].sum().rename(columns={\"weight\": \"i_weight\"})\n",
    "        total_by_a = series_df.groupby(\"a_t\", as_index=False)[\"weight\"].sum().rename(columns={\"weight\": \"a_weight\"})\n",
    "        p_t_df = pd.merge(total_by_i, total_by_a, on=\"a_t\")\n",
    "        p_t_df['P(it|at)'] = p_t_df['i_weight'] / p_t_df['a_weight']\n",
    "        # print(p_t_df[['a_t', 'i_t', 'P(it|at)']])\n",
    "\n",
    "        # The probability P(it+1|it, at+1)\n",
    "        series_filtered = series_df.dropna(subset=[\"i_t+1\"])\n",
    "        total_by_i = series_filtered.groupby([\"i_t\", \"a_t+1\", \"i_t+1\"], as_index=False)[\"weight\"].sum().rename(columns={\"weight\": \"t+1_weight\"})\n",
    "        total_by_a = series_filtered.groupby([\"i_t\", \"a_t+1\"], as_index=False)[\"weight\"].sum().rename(columns={\"weight\": \"t_weight\"})\n",
    "        p_t1_df = pd.merge(total_by_i, total_by_a, on=[\"i_t\", \"a_t+1\"])\n",
    "        p_t1_df['P(it+1|it, at+1)'] = p_t1_df['t+1_weight'] / p_t1_df['t_weight']\n",
    "        # print(p_t1_df[['i_t', 'a_t+1', 'i_t+1', 'P(it+1|it, at+1)']])  # [['i_t', 'a_t+1', 'i_t+1', 'P(it+1|it, at+1)']]\n",
    "\n",
    "        # Create the dataframe of proposed interactions\n",
    "        data = {'proposed': [self._composite_interactions[k].post_interaction.key() for k in activated_keys],\n",
    "                'E(Vi)': [0.] * len(activated_keys),\n",
    "                'action': [self._composite_interactions[k].post_interaction.get_primitive_action() for k in\n",
    "                           activated_keys],\n",
    "                'E(Va)': [0.] * len(activated_keys),\n",
    "                'weight': [self._composite_interactions[k].weight for k in activated_keys],\n",
    "                'interaction': [self._composite_interactions[k].post_interaction.pre_key() for k in activated_keys]\n",
    "                }\n",
    "        expected_df = pd.DataFrame(data)\n",
    "        # Add default interactions\n",
    "        expected_df = pd.concat([self.primitive_df, expected_df], ignore_index=True)\n",
    "\n",
    "        # Remove the post_interaction that have a negative valence\n",
    "        for i, k in expected_df[\"proposed\"].items():\n",
    "            if k in self._composite_interactions and self._composite_interactions[k].post_interaction.get_valence() < 0:\n",
    "                expected_df.at[i, \"proposed\"] = self._composite_interactions[k].pre_interaction.key()\n",
    "\n",
    "        # Remove the interactions that are the beginning of a longer interaction\n",
    "        to_remove = set()\n",
    "        for i, k1 in expected_df[\"proposed\"].items():\n",
    "            for j, k2 in expected_df[\"proposed\"].items():\n",
    "                if i not in to_remove and j not in to_remove and i != j:\n",
    "                    if k1 in self._composite_interactions:\n",
    "                        s1 = pd.Series(self._composite_interactions[k1].get_primitive_series())\n",
    "                    else:\n",
    "                        s1 = pd.Series(k1)\n",
    "                    if k2 in self._composite_interactions:\n",
    "                        s2 = pd.Series(self._composite_interactions[k2].get_primitive_series())\n",
    "                    else:\n",
    "                        s2 = pd.Series(k2)\n",
    "                    if len(s1) <= len(s2):\n",
    "                        if s1.equals(s2.iloc[:len(s1)]):\n",
    "                            # print(f\"Remove {s1.tolist()} from {i} to {j} weight {expected_df.at[i, 'weight']}\")\n",
    "                            to_remove.add(i)  # Mark the sequence to be removed\n",
    "                            expected_df.at[j, \"weight\"] += expected_df.at[i, \"weight\"]\n",
    "                        elif s2.equals(s1.iloc[:len(s2)]):\n",
    "                            # print(f\"Remove {s2.tolist()} from {j} to {i} weight {expected_df.at[j, 'weight']}\")\n",
    "                            to_remove.add(j)  # Mark the sequence to be removed\n",
    "                            expected_df.at[i, \"weight\"] += expected_df.at[j, \"weight\"]\n",
    "        expected_df = expected_df.drop(index=to_remove)\n",
    "        # Compute the expected valence of interactions\n",
    "        for i, k in expected_df[\"proposed\"].items():\n",
    "            if k in self._interactions:\n",
    "                first_row = p_t_df.loc[p_t_df[\"i_t\"] == k].head(1)  # [\"P(it|at)\"].values[0]\n",
    "                p = first_row[\"P(it|at)\"].values[0] if not first_row.empty else 0\n",
    "                # print(f\"E(vi) of {k} probability {p}\")\n",
    "                expected_df.at[i, \"E(Vi)\"] = p * self._interactions[k].get_valence()\n",
    "            else:\n",
    "                k1 = self._composite_interactions[k].pre_interaction.key()\n",
    "                p1 = p_t_df.loc[p_t_df[\"i_t\"] == k1].head(1)[\"P(it|at)\"].values[0]\n",
    "                k2 = self._composite_interactions[k].post_interaction.key()\n",
    "                p2 = p_t1_df.loc[p_t1_df[\"i_t+1\"] == k2].head(1)[\"P(it+1|it, at+1)\"].values[0]\n",
    "                expected_df.at[i, \"E(Vi)\"] = p1 * (self._interactions[k1].get_valence()\n",
    "                                                   + p2 * self._interactions[k2].get_valence())\n",
    "        # The sum expected valence per action\n",
    "        expected_df[\"E(Va)\"] = expected_df.groupby(\"action\")[\"E(Vi)\"].transform(\"sum\")\n",
    "        # The sum weight per action\n",
    "        # expected_df[\"weight\"] = expected_df.groupby(\"action\")[\"weight\"].transform(\"sum\")\n",
    "\n",
    "        # Find the most probable outcome for each action\n",
    "        max_weight_df = expected_df.loc[expected_df.groupby('action')['weight'].idxmax(), ['action', 'interaction']].reset_index(\n",
    "            drop=True)\n",
    "        max_weight_df.columns = ['action', 'intended']\n",
    "        expected_df = expected_df.merge(max_weight_df, on='action')\n",
    "\n",
    "        # Store the dataframe for printing\n",
    "        self.selection_df = expected_df.copy()\n",
    "\n",
    "    def select_intended_interaction(self):\n",
    "        \"\"\"Selects the intended interaction from the proposed dataframe\"\"\"\n",
    "        # Find the first row that has the highest proclivity\n",
    "        max_index = self.selection_df['E(Va)'].idxmax()\n",
    "        intended_interaction_key = self.selection_df.loc[max_index, ['intended']].values[0]\n",
    "        print(f\"Intended Max E(Va) {intended_interaction_key}\")\n",
    "        self._intended_interaction = self._interactions[intended_interaction_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019f138-a218-44b2-964a-8095fbe891a7",
   "metadata": {},
   "source": [
    "# PRELIMINARY EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f2924-f597-4d34-bc96-3602dc4460b7",
   "metadata": {},
   "source": [
    "## Let's create Environment6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c94c8-e36a-41cd-ad69-976849f8ced9",
   "metadata": {},
   "source": [
    "The agent has two possible actions: move to the left or move to the right. \n",
    "The environment returns outcome 1 when the agent bumps into a light green wall, and then the wall turns dark green until the agent moves away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "cf9a38a7-78b8-4590-90c4-4ce903b26876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import Output\n",
    "from IPython.display import display\n",
    "\n",
    "class Environment6:\n",
    "    \"\"\" The grid \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize the grid \"\"\"\n",
    "        self.grid = np.array([[1, 0, 0, 1]])\n",
    "        self.position = 1\n",
    "\n",
    "    def outcome(self, action):\n",
    "        \"\"\"Take the action and generate the next outcome \"\"\"\n",
    "        if action == 0:\n",
    "            # Move left\n",
    "            if self.position > 1:\n",
    "                # No bump\n",
    "                self.position -= 1\n",
    "                self.grid[0, 3] = 1\n",
    "                outcome = 0\n",
    "            elif self.grid[0, 0] == 1:\n",
    "                # First bump\n",
    "                outcome = 1\n",
    "                self.grid[0, 0] = 2\n",
    "            else:\n",
    "                # Subsequent bumps\n",
    "                outcome = 0\n",
    "        else:\n",
    "            # Move right\n",
    "            if self.position < 2:\n",
    "                # No bump\n",
    "                self.position += 1\n",
    "                self.grid[0, 0] = 1\n",
    "                outcome = 0\n",
    "            elif self.grid[0, 3] == 1:\n",
    "                # First bump\n",
    "                outcome = 1\n",
    "                self.grid[0, 3] = 2\n",
    "            else:\n",
    "                # Subsequent bumps\n",
    "                outcome = 0\n",
    "        return outcome  \n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display the grid\"\"\"\n",
    "        out.clear_output(wait=True)\n",
    "        with out:\n",
    "            fig, ax = plt.subplots()\n",
    "            # Hide the ticks\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # Display the grid\n",
    "            ax.imshow(self.grid, cmap='Greens', vmin=0, vmax=2)\n",
    "            plt.scatter(self.position, 0, s=1000)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82801853-2aef-44a7-8acd-ae2d621048bb",
   "metadata": {},
   "source": [
    "## Run the agent in Environment6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "6f6d76df-89e4-4ab3-a327-afa813a27325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the agent in Environment6\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1)\n",
    "]\n",
    "a = Agent(interactions)\n",
    "e = Environment6()\n",
    "\n",
    "# Output widget for displaying the plot\n",
    "out = Output()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf97eb-7aca-4157-ba8e-3e97f2b85ae1",
   "metadata": {},
   "source": [
    "Run the simulation step by step to see the Proposed DataFrame. Use `Ctrl+Enter` to run the cell bellow and stay on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "id": "5b044b4d-162e-4491-929c-a73eb584dcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f95447ce5b43078e0f569432b136af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1\n",
      "Reinforcing (10:-1, 10:-1: 2)\n",
      "Learning ((00:-1, 10:-1: 2), 10:-1: 1)\n",
      "Learning (00:-1, (10:-1, 10:-1: 2): 1)\n",
      "Intended Max proclivity 00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proposed</th>\n",
       "      <th>E(Vi)</th>\n",
       "      <th>action</th>\n",
       "      <th>E(Va)</th>\n",
       "      <th>interaction</th>\n",
       "      <th>weight</th>\n",
       "      <th>intended</th>\n",
       "      <th>weight_sum</th>\n",
       "      <th>proclivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(00,01)</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>00</td>\n",
       "      <td>5</td>\n",
       "      <td>00</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  proposed  E(Vi)  action  E(Va) interaction  weight intended  weight_sum  \\\n",
       "0  (00,01)   -0.5       0   -0.5          00       5       00           5   \n",
       "1       10   -1.0       1   -1.0          10       3       10           3   \n",
       "\n",
       "   proclivity  \n",
       "0        -2.5  \n",
       "1        -3.0  "
      ]
     },
     "execution_count": 979,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "e.display()\n",
    "display(out)\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.selection_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97002cd7-f66c-435b-9752-698fc3862c1a",
   "metadata": {},
   "source": [
    "Observe that, on step 8, the composite interaction (00, 01) is proposed but its expected valence equals 0 and it is not selected.\n",
    "\n",
    "After Step 18, however the agent alternates the sequences (00, 01) and (10, 11) that gives the best average valence the agent can get in this environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183c22a-51ab-419b-ac70-48642c6cfd15",
   "metadata": {},
   "source": [
    "## Let's create Environment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f775a8a-db72-44fb-852c-948bd091fee2",
   "metadata": {},
   "source": [
    "In Environment7, the agent has two possible actions: move forward or turn 180°.\n",
    "Like Environment6, Environment7 return 1 only when the agent bumps into a wall once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "8efa5b75-21ca-437b-83ec-150cd4a964ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment7:\n",
    "    \"\"\" The grid \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize the grid and the agent's pose \"\"\"\n",
    "        self.grid = np.array([[1, 0, 0, 1]])\n",
    "        self.position = 1\n",
    "        self.direction = 0\n",
    "\n",
    "    def outcome(self, action):\n",
    "        \"\"\"Take the action and generate the next outcome \"\"\"\n",
    "        if action == 0:\n",
    "            # Move forward\n",
    "            if self.direction == 0:\n",
    "                # Move to the left\n",
    "                if self.position > 1:\n",
    "                    # No bump\n",
    "                    self.position -= 1\n",
    "                    self.grid[0, 3] = 1\n",
    "                    outcome = 0\n",
    "                elif self.grid[0, 0] == 1:\n",
    "                    # First bump\n",
    "                    outcome = 1\n",
    "                    self.grid[0, 0] = 2\n",
    "                else:\n",
    "                    # Subsequent bumps\n",
    "                    outcome = 0\n",
    "            else:\n",
    "                # Move to the right\n",
    "                if self.position < 2:\n",
    "                    # No bump\n",
    "                    self.position += 1\n",
    "                    self.grid[0, 0] = 1\n",
    "                    outcome = 0\n",
    "                elif self.grid[0, 3] == 1:\n",
    "                    # First bump\n",
    "                    outcome = 1\n",
    "                    self.grid[0, 3] = 2\n",
    "                else:\n",
    "                    # Subsequent bumps\n",
    "                    outcome = 0\n",
    "        else:\n",
    "            # Turn 180°\n",
    "            outcome = 0\n",
    "            if self.direction == 0:\n",
    "                self.direction = 1\n",
    "            else:\n",
    "                self.direction = 0\n",
    "        return outcome  \n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display the grid\"\"\"\n",
    "        out.clear_output(wait=True)\n",
    "        with out:\n",
    "            fig, ax = plt.subplots()\n",
    "            # Hide the ticks\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # Display the grid\n",
    "            ax.imshow(self.grid, cmap='Greens', vmin=0, vmax=2)\n",
    "            if self.direction == 0:\n",
    "                # Display agent to the left\n",
    "                plt.scatter(self.position, 0, s=1000, marker='<')\n",
    "            else:\n",
    "                # Display agent to the right\n",
    "                plt.scatter(self.position, 0, s=1000, marker='>')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c101e5-2dad-4e57-a52e-eb1f7d634564",
   "metadata": {},
   "source": [
    "## Test the Agent in Environment7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "74c423e8-de20-479c-84f7-8d64b3985433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a new agent\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1)\n",
    "]\n",
    "a = Agent(interactions)\n",
    "e = Environment7()\n",
    "\n",
    "# Output widget for displaying the plot\n",
    "out = Output()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "id": "ca83faa7-49ae-40c2-b7b7-dee94eb6ec8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f95447ce5b43078e0f569432b136af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1\n",
      "Reinforcing (00:-1, 00:-1: 3)\n",
      "Learning ((10:-1, 00:-1: 2), 00:-1: 1)\n",
      "Learning (10:-1, (00:-1, 00:-1: 3): 1)\n",
      "Intended Max proclivity 00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proposed</th>\n",
       "      <th>E(Vi)</th>\n",
       "      <th>action</th>\n",
       "      <th>E(Va)</th>\n",
       "      <th>interaction</th>\n",
       "      <th>weight</th>\n",
       "      <th>intended</th>\n",
       "      <th>weight_sum</th>\n",
       "      <th>proclivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>00</td>\n",
       "      <td>6</td>\n",
       "      <td>00</td>\n",
       "      <td>10</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>01</td>\n",
       "      <td>4</td>\n",
       "      <td>00</td>\n",
       "      <td>10</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  proposed  E(Vi)  action  E(Va) interaction  weight intended  weight_sum  \\\n",
       "0       00   -0.6       0   -0.2          00       6       00          10   \n",
       "1       10   -1.0       1   -1.0          10       3       10           3   \n",
       "2       01    0.4       0   -0.2          01       4       00          10   \n",
       "\n",
       "   proclivity  \n",
       "0        -2.0  \n",
       "1        -3.0  \n",
       "2        -2.0  "
      ]
     },
     "execution_count": 976,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "e.display()\n",
    "display(out)\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.selection_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfdadd6-468e-4fb9-9718-7224d695c24c",
   "metadata": {},
   "source": [
    "Observe that Agent7 does not manage to learn to obtain the optimum avarage valence in Environment7\n",
    "\n",
    "This is because it selects the next action that has the highest expecte valence but it does not take into account the proposal weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50d96e-4f12-4531-a122-9d13d8bd8639",
   "metadata": {},
   "source": [
    "# ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf06ea-5925-4590-80dd-f9284f4306a6",
   "metadata": {},
   "source": [
    "Create Agent7 that computes the proclivity of proposed interactions by multiplying the expectet valence with the weight, and select the proposed interaction that has the highest proclivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "2e150c60-5315-4a49-822f-2681fb22a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent7(Agent):\n",
    "    def select_intended_interaction(self):\n",
    "        \"\"\"Selects the intended interaction from the proposed dataframe\"\"\"\n",
    "        # Modify to compute the proclivity and select the action that has the hiest proclivity\n",
    "        # Find the first row that has the highest proclivity\n",
    "        max_index = self.selection_df['E(Va)'].idxmax()\n",
    "        intended_interaction_key = self.selection_df.loc[max_index, ['intended']].values[0]\n",
    "        print(f\"Intended Max E(Va) {intended_interaction_key}\")\n",
    "        self._intended_interaction = self._interactions[intended_interaction_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "3ab0a978-5e55-4edd-bed0-8f249fa1f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent7(Agent):\n",
    "    def select_intended_interaction(self):\n",
    "        # The sum weight per action\n",
    "        grouped_df = self.selection_df.groupby('action').agg({'weight': 'sum'}).reset_index()\n",
    "        self.selection_df = self.selection_df.merge(grouped_df, on='action', suffixes=('', '_sum'))\n",
    "        # self.selection_df[\"sum_weight\"] = self.selection_df.groupby(\"action\")[\"weight\"].transform(\"sum\")\n",
    "\n",
    "        # Compute the proclivity\n",
    "        self.selection_df['proclivity'] = self.selection_df[\"weight_sum\"] * self.selection_df['E(Va)']\n",
    "\n",
    "        # Select the action that has the highest proclivity\n",
    "        max_index = self.selection_df['proclivity'].idxmax()\n",
    "        intended_interaction_key = self.selection_df.loc[max_index, ['intended']].values[0]\n",
    "        print(f\"Intended Max proclivity {intended_interaction_key}\")\n",
    "        self._intended_interaction = self._interactions[intended_interaction_key]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4fec51-19aa-447d-a456-893249acc994",
   "metadata": {},
   "source": [
    "## Test your Agent7 in Environment7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "id": "7db78d74-6fd8-4132-8c7d-d201a4084fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a new agent\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1)\n",
    "]\n",
    "a = Agent7(interactions)\n",
    "e = Environment7()\n",
    "\n",
    "# Output widget for displaying the plot\n",
    "out = Output()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "id": "0159e4d4-f851-432b-8127-63403d7a45c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f95447ce5b43078e0f569432b136af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1\n",
      "Reinforcing (00:-1, 01:1: 2)\n",
      "Learning ((10:-1, 00:-1: 1), 01:1: 1)\n",
      "Learning (10:-1, (00:-1, 01:1: 2): 1)\n",
      "Intended Max proclivity 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proposed</th>\n",
       "      <th>E(Vi)</th>\n",
       "      <th>action</th>\n",
       "      <th>E(Va)</th>\n",
       "      <th>interaction</th>\n",
       "      <th>weight</th>\n",
       "      <th>intended</th>\n",
       "      <th>weight_sum</th>\n",
       "      <th>proclivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>00</td>\n",
       "      <td>3</td>\n",
       "      <td>00</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  proposed  E(Vi)  action  E(Va) interaction  weight intended  weight_sum  \\\n",
       "0       10    0.0       1    0.0          10       0       10           0   \n",
       "1       00   -1.0       0   -1.0          00       3       00           3   \n",
       "\n",
       "   proclivity  \n",
       "0         0.0  \n",
       "1        -3.0  "
      ]
     },
     "execution_count": 972,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "e.display()\n",
    "display(out)\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.selection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7fcb67-1fe1-47ad-817e-4ad9ba905e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
