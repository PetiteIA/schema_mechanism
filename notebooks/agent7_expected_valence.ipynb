{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f34084-d4b8-4160-89ad-a927a0919615",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PetiteIA/schema_mechanism/blob/master/notebooks/agent7.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec58b3-e210-4a78-9610-f6615254de29",
   "metadata": {},
   "source": [
    "# THE AGENT WHO SAW TOMORROW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ef995-42ce-44ba-9f3b-8bfa808b08fe",
   "metadata": {},
   "source": [
    "# Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14a47d-ca87-4080-8f84-afcd2bc83170",
   "metadata": {},
   "source": [
    "Upon completing this lab, you will be able to implement a developmental agent that selects an action based on a two-step-ahead anticipation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16db8-1bd6-4e82-9cf6-2b37d996ffe8",
   "metadata": {},
   "source": [
    "# Define the necessary classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17704e-ca60-417b-8d7e-b8f77cfa09a0",
   "metadata": {},
   "source": [
    "Ensure the required packages are installed if they aren't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7e5da-0188-4924-92f4-3ecbfc2fce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install ipywidgets\n",
    "!pip install IPython.display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44155c4a-d038-40b3-be33-e8cf77c5f976",
   "metadata": {},
   "source": [
    "We keep improving the Interaction class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "e96c3637-4161-49aa-9f1d-0d3342de92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, action, outcome, valence):\n",
    "        self._action = action\n",
    "        self._outcome = outcome\n",
    "        self._valence = valence\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_primitive_action(self):\n",
    "        \"\"\"Return the action for compatibility with CompositeInteraction\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_outcome(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._outcome\n",
    "\n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self._action}{self._outcome}\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key. Used for compatibility with CompositeInteraction\"\"\"\n",
    "        return self.key()\n",
    "\n",
    "    def get_primitive_series(self):\n",
    "        \"\"\"\"Return the key in a list\"\"\"\n",
    "        return [self.key()]\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self._action}{self._outcome}:{self._valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.key() == other.key()\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa086c1-5987-4375-939a-aa217b6048f6",
   "metadata": {},
   "source": [
    "We keep improving the CompositeInteraction class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "ef3243d6-1de3-418e-9236-8af0287131da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeInteraction:\n",
    "    \"\"\"A composite interaction is a tuple (pre_interaction, post_interaction) and a weight\"\"\"\n",
    "\n",
    "    def __init__(self, pre_interaction, post_interaction):\n",
    "        self.pre_interaction = pre_interaction\n",
    "        self.post_interaction = post_interaction\n",
    "        self.weight = 1\n",
    "        self.isActivated = False\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action of the pre interaction\"\"\"\n",
    "        return self.key()\n",
    "        # return self.pre_interaction.get_action()\n",
    "\n",
    "    def get_primitive_action(self):\n",
    "        \"\"\"Return the primite action\"\"\"\n",
    "        return self.pre_interaction.get_primitive_action()\n",
    "\n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the valence of the pre_interaction plus the valence of the post_interaction\"\"\"\n",
    "        return self.pre_interaction.get_valence() + self.post_interaction.get_valence()\n",
    "\n",
    "    def reinforce(self):\n",
    "        \"\"\"Increment the composite interaction's weight\"\"\"\n",
    "        self.weight += 1\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictionary is the string '<pre_interaction><post_interaction>'. \"\"\"\n",
    "        return f\"({self.pre_interaction.key()},{self.post_interaction.key()})\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key of the pre_interaction\"\"\"\n",
    "        return self.pre_interaction.pre_key()\n",
    "\n",
    "    def get_primitive_series(self):\n",
    "        \"\"\"\"Return the list of primitive keys\"\"\"\n",
    "        return self.pre_interaction.get_primitive_series() + self.post_interaction.get_primitive_series()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print the interaction in the Newick tree format (pre_interaction, post_interaction: valence) \"\"\"\n",
    "        return f\"({self.pre_interaction}, {self.post_interaction}: {self.weight})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same pre and post interactions \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return (self.pre_interaction == other.pre_interaction) and (self.post_interaction == other.post_interaction)\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_valence_series(self):\n",
    "        \"\"\"\"Return the list of valences of primitive interactions\"\"\"\n",
    "        return self.pre_interaction.get_valence_series() + self.post_interaction.get_valence_series()\n",
    "\n",
    "    def get_primitive_series(self):\n",
    "        \"\"\"\"Return the list of primitive keys\"\"\"\n",
    "        return self.pre_interaction.get_primitive_series() + self.post_interaction.get_primitive_series()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152512a3-ef97-473a-91a7-9a36fe70ba33",
   "metadata": {},
   "source": [
    "# Prepare the Agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d65a49-9e8a-4834-b064-d9fff651abdc",
   "metadata": {},
   "source": [
    "Let's implement Agent7 that calculates the expected valence based on a two-step anticipation. \n",
    "Figure 1 illustrates the learning mechanism and the interaction selection mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e95995-8077-46ed-a03d-e318623cb82b",
   "metadata": {},
   "source": [
    "![Agent5](img/Figure_1_Agent7_3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742aecbc-5e94-4cc6-bf5d-1b6c12bde814",
   "metadata": {},
   "source": [
    "Figure 1: Agent7 records and reinforces two levels of composite interactions:\n",
    "* First-level composite interaction $c_{t-1} = (i_{t-2}, i_{t-1})$, \n",
    "* Second-level composite interaction $((i_{t-3}, i_{t-2}), i_{t-1})$, and $(i_{t-3}, (i_{t-2}, i_{t-1}))$. \n",
    "\n",
    "The last enacted primitive interaction $i_{t-1}$ and the last enacted composite interaction $c_{t-1}$ activates previously-learned composite interactions that propose their post interaction. \n",
    "Now the post interaction may be a composite interaction.\n",
    "\n",
    "Post interactions are aggregated by their first action. The expected valence $\\mathbb{E}(Va)$ is computed for each action. The action that has the highest expected valence is selected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c689e-8a92-472d-9374-5ebf75be2392",
   "metadata": {},
   "source": [
    "## Calculate the expected Valence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b1b654-9015-465d-b997-d0e662217c88",
   "metadata": {},
   "source": [
    "An action's expected valence $\\mathbb{E}(V_a)$ is calculated as follows.\n",
    "\n",
    "We find the set $A$ of activated composite interaction that propose $a$. \n",
    "\n",
    "We find the set $S$ of the longuest sequences that are proposed by the activated composite interactions in $A$.\n",
    "Sequences that are the beginning of a longer sequence are not included in $S$. \n",
    "Final interactions that have negative valence are removed from the sequence. \n",
    "\n",
    "We compute the expected valence of action $a$ by incorporating each steps of the sequence in the formula from Agent6:\n",
    "\n",
    "$\\displaystyle \\mathbb{E}(V_a) = \\sum_{s \\in S} \\sum_{j=1}^{n_s} v_{sj} \\cdot \\prod_{k=1}^{j} p_{sk} $\n",
    "\n",
    "in which $n_s$ is the length of $s$, i.e., the number of primitive interaction of $s$, and $p_{sk}$ is the probability of successfully enacting the $k^{th}$ primitive interaction of $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cbeec7-f221-485d-bbb4-452a069fd4a6",
   "metadata": {},
   "source": [
    "The probability estimation $\\hat{p}_{s1}$ of engaging in sequence $s$ is computed as in Agent6: \n",
    "\n",
    "$\\displaystyle \\hat{p}_{s1} = \\frac{\\sum_{c \\in A_s} w_c }{\\sum_{c \\in A_1} w_c } = \\hat{p}(s|a_{s1}) $ \n",
    "\n",
    "$A_1$ is the set of activated composite interactions proposing the first action $a_{s1}$ that begins sequence $s$. $A_s \\subset A_1$ is the set of activated composite interactions that propose $s$. \n",
    "In other words, $\\hat{p}_{s1}$ is the ratio of the number of time $s$ was enacted over the number of time its first action $a_{s1}$ was selected.\n",
    "\n",
    "The probability estimation $\\hat{p}_{sk}$ of enacting the step $k \\gt 1$ of a sequence is computed similarly:\n",
    "\n",
    "$\\displaystyle \\hat{p}_{sk} = \\frac{\\sum_{c \\in A_{sk}} w_c }{\\sum_{c \\in A_{k}} w_c} = \\hat{p}(s_k|s_{k-1}, a_{sk}) $ \n",
    "\n",
    "$A_{k}$ is the set of activated composite interactions proposing the $k^{th}$ action $a_{k}$ of sequence $s$. \n",
    "$A_{sk} \\subset A_k$ is the set of activated composite interactions proposing the $k^{th}$ interaction $s_{k}$ of sequence $s$. \n",
    "In other words, $\\hat{p}_{sk}$ is the ratio of the number of time the primitive interaction $s_k$ was enacted at step $k$ over the number of time its action $a_{sk}$ was selected when trying to enact sequence $s$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be10adae-f3cc-4f22-bb7a-f9e65caf66a1",
   "metadata": {},
   "source": [
    "## Implement the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "3f0bf369-48ef-48d7-a92b-3c7ef8dc1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, _interactions):\n",
    "        \"\"\" Initialize our agent \"\"\"\n",
    "        self._interactions = {interaction.key(): interaction for interaction in _interactions}\n",
    "        self._composite_interactions = {}\n",
    "        self._intended_interaction = self._interactions[\"00\"]\n",
    "        self._last_interaction = None\n",
    "        self._previous_interaction = None\n",
    "        self._penultimate_interaction = None\n",
    "        self._last_composite_interaction = None\n",
    "        self._previous_composite_interaction = None\n",
    "        # Create a dataframe of default primitive interactions\n",
    "        default_interactions = [interaction for interaction in _interactions if interaction.get_outcome() == 0]\n",
    "        data = {'proposed': [i.key() for i in default_interactions],\n",
    "                'E(Vi)': [0.] * len(default_interactions),\n",
    "                'action': [i.get_action() for i in default_interactions],\n",
    "                'E(Va)': [0.] * len(default_interactions),\n",
    "                'interaction': [i.key() for i in default_interactions],\n",
    "                'weight': [0] * len(default_interactions)}\n",
    "        self.primitive_df = pd.DataFrame(data)\n",
    "        # Store the selection dataframe as a class attribute so we can display it in the notebook\n",
    "        self.selection_df = None\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\"Implement the agent's policy\"\"\"\n",
    "        # Memorize the context\n",
    "        self._previous_composite_interaction = self._last_composite_interaction\n",
    "        self._penultimate_interaction = self._previous_interaction\n",
    "        self._previous_interaction = self._last_interaction\n",
    "        self._last_interaction = self._interactions[f\"{self._intended_interaction.get_action()}{_outcome}\"]\n",
    "\n",
    "        # tracing the previous cycle\n",
    "        print(\n",
    "            f\"Action: {self._intended_interaction.get_action()}, Prediction: {self._intended_interaction.get_outcome()}, \"\n",
    "            f\"Outcome: {_outcome}, Prediction_correct: {self._intended_interaction.get_outcome() == _outcome}, \"\n",
    "            f\"Valence: {self._last_interaction.get_valence()}\")\n",
    "\n",
    "        # Call the learning mechanism\n",
    "        self.learn()\n",
    "\n",
    "        # Calculate the proposed dataframe\n",
    "        self.calculate_proposed_df()\n",
    "\n",
    "        # Select the intended primitive interaction\n",
    "        self.select_intended_interaction()\n",
    "\n",
    "        return self._intended_interaction.get_action()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Learn the composite interactions\"\"\"\n",
    "        # First level of composite interactions\n",
    "        self._last_composite_interaction = self.learn_composite_interaction(self._previous_interaction,\n",
    "                                                                            self._last_interaction)\n",
    "        # Second level of composite interactions\n",
    "        self.learn_composite_interaction(self._previous_composite_interaction, self._last_interaction)\n",
    "        self.learn_composite_interaction(self._penultimate_interaction, self._last_composite_interaction)\n",
    "\n",
    "    def learn_composite_interaction(self, pre_interaction, post_interaction):\n",
    "        if pre_interaction is None:\n",
    "            return None\n",
    "        else:\n",
    "            # Record or reinforce the first level composite interaction\n",
    "            composite_interaction = CompositeInteraction(pre_interaction, post_interaction)\n",
    "            if composite_interaction.key() not in self._composite_interactions:\n",
    "                self._composite_interactions[composite_interaction.key()] = composite_interaction\n",
    "                print(f\"Learning {composite_interaction}\")\n",
    "                return composite_interaction\n",
    "            else:\n",
    "                self._composite_interactions[composite_interaction.key()].reinforce()\n",
    "                print(f\"Reinforcing {self._composite_interactions[composite_interaction.key()]}\")\n",
    "                # Retrieve the existing composite interaction\n",
    "                return self._composite_interactions[composite_interaction.key()]\n",
    "\n",
    "    def calculate_proposed_df(self):\n",
    "        \"\"\"Select the action that has the highest expected valence\"\"\"\n",
    "\n",
    "        # The activated composite interactions\n",
    "        activated_keys = [composite_interaction.key() for composite_interaction in self._composite_interactions.values()\n",
    "                          if composite_interaction.pre_interaction == self._last_interaction or\n",
    "                          composite_interaction.pre_interaction == self._last_composite_interaction]\n",
    "\n",
    "        # Create the dataframe of sequences\n",
    "        series_df = pd.DataFrame(columns=['proposed', 'weight', 'a_t', 'i_t', 'a_t+1', 'i_t+1'])\n",
    "        for k in activated_keys:\n",
    "            new_row = {'proposed': self._composite_interactions[k].post_interaction.key(),\n",
    "                       'weight': self._composite_interactions[k].weight,\n",
    "                       'a_t': self._composite_interactions[k].post_interaction.get_primitive_action()}\n",
    "            if type(self._composite_interactions[k].post_interaction) == Interaction:\n",
    "                new_row['i_t'] = self._composite_interactions[k].post_interaction.key()\n",
    "            else:\n",
    "                new_row['i_t'] = self._composite_interactions[k].post_interaction.pre_interaction.key()\n",
    "                new_row['a_t+1'] = self._composite_interactions[k].post_interaction.post_interaction.get_primitive_action()\n",
    "                new_row['i_t+1'] = self._composite_interactions[k].post_interaction.post_interaction.key()\n",
    "            series_df = pd.concat([series_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        # print(series_df)\n",
    "\n",
    "        # The probability P(it|at)\n",
    "        total_by_i = series_df.groupby([\"a_t\", \"i_t\"], as_index=False)[\"weight\"].sum().rename(columns={\"weight\": \"i_weight\"})\n",
    "        total_by_a = series_df.groupby(\"a_t\", as_index=False)[\"weight\"].sum().rename(columns={\"weight\": \"a_weight\"})\n",
    "        p_t_df = pd.merge(total_by_i, total_by_a, on=\"a_t\")\n",
    "        p_t_df['P(it|at)'] = p_t_df['i_weight'] / p_t_df['a_weight']\n",
    "        # print(p_t_df[['a_t', 'i_t', 'P(it|at)']])\n",
    "\n",
    "        # The probability P(it+1|it, at+1)\n",
    "        series_filtered = series_df.dropna(subset=[\"i_t+1\"])\n",
    "        total_by_i = series_filtered.groupby([\"i_t\", \"a_t+1\", \"i_t+1\"], as_index=False)[\"weight\"].sum().rename(columns={\"weight\": \"t+1_weight\"})\n",
    "        total_by_a = series_filtered.groupby([\"i_t\", \"a_t+1\"], as_index=False)[\"weight\"].sum().rename(columns={\"weight\": \"t_weight\"})\n",
    "        p_t1_df = pd.merge(total_by_i, total_by_a, on=[\"i_t\", \"a_t+1\"])\n",
    "        p_t1_df['P(it+1|it, at+1)'] = p_t1_df['t+1_weight'] / p_t1_df['t_weight']\n",
    "        # print(p_t1_df[['i_t', 'a_t+1', 'i_t+1', 'P(it+1|it, at+1)']])  # [['i_t', 'a_t+1', 'i_t+1', 'P(it+1|it, at+1)']]\n",
    "\n",
    "        # Create the dataframe of proposed interactions\n",
    "        data = {'proposed': [self._composite_interactions[k].post_interaction.key() for k in activated_keys],\n",
    "                'E(Vi)': [0.] * len(activated_keys),\n",
    "                'action': [self._composite_interactions[k].post_interaction.get_primitive_action() for k in\n",
    "                           activated_keys],\n",
    "                'E(Va)': [0.] * len(activated_keys),\n",
    "                'weight': [self._composite_interactions[k].weight for k in activated_keys],\n",
    "                'interaction': [self._composite_interactions[k].post_interaction.pre_key() for k in activated_keys]\n",
    "                }\n",
    "        expected_df = pd.DataFrame(data)\n",
    "        # Add default interactions\n",
    "        expected_df = pd.concat([self.primitive_df, expected_df], ignore_index=True)\n",
    "\n",
    "        # Remove the post_interaction that have a negative valence\n",
    "        for i, k in expected_df[\"proposed\"].items():\n",
    "            if k in self._composite_interactions and self._composite_interactions[k].post_interaction.get_valence() < 0:\n",
    "                expected_df.at[i, \"proposed\"] = self._composite_interactions[k].pre_interaction.key()\n",
    "\n",
    "        # Remove the interactions that are the beginning of a longer interaction\n",
    "        to_remove = set()\n",
    "        for i, k1 in expected_df[\"proposed\"].items():\n",
    "            for j, k2 in expected_df[\"proposed\"].items():\n",
    "                if i not in to_remove and j not in to_remove and i != j:\n",
    "                    if k1 in self._composite_interactions:\n",
    "                        s1 = pd.Series(self._composite_interactions[k1].get_primitive_series())\n",
    "                    else:\n",
    "                        s1 = pd.Series(k1)\n",
    "                    if k2 in self._composite_interactions:\n",
    "                        s2 = pd.Series(self._composite_interactions[k2].get_primitive_series())\n",
    "                    else:\n",
    "                        s2 = pd.Series(k2)\n",
    "                    if len(s1) <= len(s2):\n",
    "                        if s1.equals(s2.iloc[:len(s1)]):\n",
    "                            # print(f\"Remove {s1.tolist()} from {i} to {j} weight {expected_df.at[i, 'weight']}\")\n",
    "                            to_remove.add(i)  # Mark the sequence to be removed\n",
    "                            expected_df.at[j, \"weight\"] += expected_df.at[i, \"weight\"]\n",
    "                        elif s2.equals(s1.iloc[:len(s2)]):\n",
    "                            # print(f\"Remove {s2.tolist()} from {j} to {i} weight {expected_df.at[j, 'weight']}\")\n",
    "                            to_remove.add(j)  # Mark the sequence to be removed\n",
    "                            expected_df.at[i, \"weight\"] += expected_df.at[j, \"weight\"]\n",
    "        expected_df = expected_df.drop(index=to_remove)\n",
    "        # Compute the expected valence of interactions\n",
    "        for i, k in expected_df[\"proposed\"].items():\n",
    "            if k in self._interactions:\n",
    "                first_row = p_t_df.loc[p_t_df[\"i_t\"] == k].head(1)  # [\"P(it|at)\"].values[0]\n",
    "                p = first_row[\"P(it|at)\"].values[0] if not first_row.empty else 0\n",
    "                # print(f\"E(vi) of {k} probability {p}\")\n",
    "                expected_df.at[i, \"E(Vi)\"] = p * self._interactions[k].get_valence()\n",
    "            else:\n",
    "                k1 = self._composite_interactions[k].pre_interaction.key()\n",
    "                p1 = p_t_df.loc[p_t_df[\"i_t\"] == k1].head(1)[\"P(it|at)\"].values[0]\n",
    "                k2 = self._composite_interactions[k].post_interaction.key()\n",
    "                p2 = p_t1_df.loc[p_t1_df[\"i_t+1\"] == k2].head(1)[\"P(it+1|it, at+1)\"].values[0]\n",
    "                expected_df.at[i, \"E(Vi)\"] = p1 * (self._interactions[k1].get_valence()\n",
    "                                                   + p2 * self._interactions[k2].get_valence())\n",
    "        # The sum expected valence per action\n",
    "        expected_df[\"E(Va)\"] = expected_df.groupby(\"action\")[\"E(Vi)\"].transform(\"sum\")\n",
    "        # The sum weight per action\n",
    "        # expected_df[\"weight\"] = expected_df.groupby(\"action\")[\"weight\"].transform(\"sum\")\n",
    "\n",
    "        # Find the most probable outcome for each action\n",
    "        max_weight_df = expected_df.loc[expected_df.groupby('action')['weight'].idxmax(), ['action', 'interaction']].reset_index(\n",
    "            drop=True)\n",
    "        max_weight_df.columns = ['action', 'intended']\n",
    "        expected_df = expected_df.merge(max_weight_df, on='action')\n",
    "\n",
    "        # Store the dataframe for printing\n",
    "        self.selection_df = expected_df.copy()\n",
    "\n",
    "    def select_intended_interaction(self):\n",
    "        \"\"\"Selects the intended interaction from the proposed dataframe\"\"\"\n",
    "        # Find the first row that has the highest proclivity\n",
    "        max_index = self.selection_df['E(Va)'].idxmax()\n",
    "        intended_interaction_key = self.selection_df.loc[max_index, ['intended']].values[0]\n",
    "        print(f\"Intended Max E(Va) {intended_interaction_key}\")\n",
    "        self._intended_interaction = self._interactions[intended_interaction_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019f138-a218-44b2-964a-8095fbe891a7",
   "metadata": {},
   "source": [
    "# PRELIMINARY EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f2924-f597-4d34-bc96-3602dc4460b7",
   "metadata": {},
   "source": [
    "## Let's create Environment6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c94c8-e36a-41cd-ad69-976849f8ced9",
   "metadata": {},
   "source": [
    "The agent has two possible actions: move to the left or move to the right. \n",
    "The environment returns outcome 1 when the agent bumps into a light green wall, and then the wall turns dark green until the agent moves away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "cf9a38a7-78b8-4590-90c4-4ce903b26876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import Output\n",
    "from IPython.display import display\n",
    "\n",
    "class Environment6:\n",
    "    \"\"\" The grid \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize the grid \"\"\"\n",
    "        self.grid = np.array([[1, 0, 0, 1]])\n",
    "        self.position = 1\n",
    "\n",
    "    def outcome(self, action):\n",
    "        \"\"\"Take the action and generate the next outcome \"\"\"\n",
    "        if action == 0:\n",
    "            # Move left\n",
    "            if self.position > 1:\n",
    "                # No bump\n",
    "                self.position -= 1\n",
    "                self.grid[0, 3] = 1\n",
    "                outcome = 0\n",
    "            elif self.grid[0, 0] == 1:\n",
    "                # First bump\n",
    "                outcome = 1\n",
    "                self.grid[0, 0] = 2\n",
    "            else:\n",
    "                # Subsequent bumps\n",
    "                outcome = 0\n",
    "        else:\n",
    "            # Move right\n",
    "            if self.position < 2:\n",
    "                # No bump\n",
    "                self.position += 1\n",
    "                self.grid[0, 0] = 1\n",
    "                outcome = 0\n",
    "            elif self.grid[0, 3] == 1:\n",
    "                # First bump\n",
    "                outcome = 1\n",
    "                self.grid[0, 3] = 2\n",
    "            else:\n",
    "                # Subsequent bumps\n",
    "                outcome = 0\n",
    "        return outcome  \n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display the grid\"\"\"\n",
    "        out.clear_output(wait=True)\n",
    "        with out:\n",
    "            fig, ax = plt.subplots()\n",
    "            # Hide the ticks\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # Display the grid\n",
    "            ax.imshow(self.grid, cmap='Greens', vmin=0, vmax=2)\n",
    "            plt.scatter(self.position, 0, s=1000)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82801853-2aef-44a7-8acd-ae2d621048bb",
   "metadata": {},
   "source": [
    "## Run the agent in Environment6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "6f6d76df-89e4-4ab3-a327-afa813a27325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the agent in Environment6\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1)\n",
    "]\n",
    "a = Agent(interactions)\n",
    "e = Environment6()\n",
    "\n",
    "# Output widget for displaying the plot\n",
    "out = Output()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf97eb-7aca-4157-ba8e-3e97f2b85ae1",
   "metadata": {},
   "source": [
    "Run the simulation step by step to see the Proposed DataFrame. Use `Ctrl+Enter` to run the cell bellow and stay on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "id": "5b044b4d-162e-4491-929c-a73eb584dcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f95447ce5b43078e0f569432b136af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1\n",
      "Reinforcing (10:-1, 10:-1: 2)\n",
      "Learning ((00:-1, 10:-1: 2), 10:-1: 1)\n",
      "Learning (00:-1, (10:-1, 10:-1: 2): 1)\n",
      "Intended Max proclivity 00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proposed</th>\n",
       "      <th>E(Vi)</th>\n",
       "      <th>action</th>\n",
       "      <th>E(Va)</th>\n",
       "      <th>interaction</th>\n",
       "      <th>weight</th>\n",
       "      <th>intended</th>\n",
       "      <th>weight_sum</th>\n",
       "      <th>proclivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(00,01)</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>00</td>\n",
       "      <td>5</td>\n",
       "      <td>00</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  proposed  E(Vi)  action  E(Va) interaction  weight intended  weight_sum  \\\n",
       "0  (00,01)   -0.5       0   -0.5          00       5       00           5   \n",
       "1       10   -1.0       1   -1.0          10       3       10           3   \n",
       "\n",
       "   proclivity  \n",
       "0        -2.5  \n",
       "1        -3.0  "
      ]
     },
     "execution_count": 979,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "e.display()\n",
    "display(out)\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.selection_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97002cd7-f66c-435b-9752-698fc3862c1a",
   "metadata": {},
   "source": [
    "Observe that, on step 8, the composite interaction (00, 01) is proposed but its expected valence equals 0 and it is not selected.\n",
    "\n",
    "After Step 18, however the agent alternates the sequences (00, 01) and (10, 11) that gives the best average valence the agent can get in this environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183c22a-51ab-419b-ac70-48642c6cfd15",
   "metadata": {},
   "source": [
    "## Let's create Environment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f775a8a-db72-44fb-852c-948bd091fee2",
   "metadata": {},
   "source": [
    "In Environment7, the agent has two possible actions: move forward or turn 180°.\n",
    "Like Environment6, Environment7 return 1 only when the agent bumps into a wall once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "8efa5b75-21ca-437b-83ec-150cd4a964ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment7:\n",
    "    \"\"\" The grid \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize the grid and the agent's pose \"\"\"\n",
    "        self.grid = np.array([[1, 0, 0, 1]])\n",
    "        self.position = 1\n",
    "        self.direction = 0\n",
    "\n",
    "    def outcome(self, action):\n",
    "        \"\"\"Take the action and generate the next outcome \"\"\"\n",
    "        if action == 0:\n",
    "            # Move forward\n",
    "            if self.direction == 0:\n",
    "                # Move to the left\n",
    "                if self.position > 1:\n",
    "                    # No bump\n",
    "                    self.position -= 1\n",
    "                    self.grid[0, 3] = 1\n",
    "                    outcome = 0\n",
    "                elif self.grid[0, 0] == 1:\n",
    "                    # First bump\n",
    "                    outcome = 1\n",
    "                    self.grid[0, 0] = 2\n",
    "                else:\n",
    "                    # Subsequent bumps\n",
    "                    outcome = 0\n",
    "            else:\n",
    "                # Move to the right\n",
    "                if self.position < 2:\n",
    "                    # No bump\n",
    "                    self.position += 1\n",
    "                    self.grid[0, 0] = 1\n",
    "                    outcome = 0\n",
    "                elif self.grid[0, 3] == 1:\n",
    "                    # First bump\n",
    "                    outcome = 1\n",
    "                    self.grid[0, 3] = 2\n",
    "                else:\n",
    "                    # Subsequent bumps\n",
    "                    outcome = 0\n",
    "        else:\n",
    "            # Turn 180°\n",
    "            outcome = 0\n",
    "            if self.direction == 0:\n",
    "                self.direction = 1\n",
    "            else:\n",
    "                self.direction = 0\n",
    "        return outcome  \n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display the grid\"\"\"\n",
    "        out.clear_output(wait=True)\n",
    "        with out:\n",
    "            fig, ax = plt.subplots()\n",
    "            # Hide the ticks\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # Display the grid\n",
    "            ax.imshow(self.grid, cmap='Greens', vmin=0, vmax=2)\n",
    "            if self.direction == 0:\n",
    "                # Display agent to the left\n",
    "                plt.scatter(self.position, 0, s=1000, marker='<')\n",
    "            else:\n",
    "                # Display agent to the right\n",
    "                plt.scatter(self.position, 0, s=1000, marker='>')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c101e5-2dad-4e57-a52e-eb1f7d634564",
   "metadata": {},
   "source": [
    "## Test the Agent in Environment7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "74c423e8-de20-479c-84f7-8d64b3985433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a new agent\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1)\n",
    "]\n",
    "a = Agent(interactions)\n",
    "e = Environment7()\n",
    "\n",
    "# Output widget for displaying the plot\n",
    "out = Output()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "id": "ca83faa7-49ae-40c2-b7b7-dee94eb6ec8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f95447ce5b43078e0f569432b136af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1\n",
      "Reinforcing (00:-1, 00:-1: 3)\n",
      "Learning ((10:-1, 00:-1: 2), 00:-1: 1)\n",
      "Learning (10:-1, (00:-1, 00:-1: 3): 1)\n",
      "Intended Max proclivity 00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proposed</th>\n",
       "      <th>E(Vi)</th>\n",
       "      <th>action</th>\n",
       "      <th>E(Va)</th>\n",
       "      <th>interaction</th>\n",
       "      <th>weight</th>\n",
       "      <th>intended</th>\n",
       "      <th>weight_sum</th>\n",
       "      <th>proclivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>00</td>\n",
       "      <td>6</td>\n",
       "      <td>00</td>\n",
       "      <td>10</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>01</td>\n",
       "      <td>4</td>\n",
       "      <td>00</td>\n",
       "      <td>10</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  proposed  E(Vi)  action  E(Va) interaction  weight intended  weight_sum  \\\n",
       "0       00   -0.6       0   -0.2          00       6       00          10   \n",
       "1       10   -1.0       1   -1.0          10       3       10           3   \n",
       "2       01    0.4       0   -0.2          01       4       00          10   \n",
       "\n",
       "   proclivity  \n",
       "0        -2.0  \n",
       "1        -3.0  \n",
       "2        -2.0  "
      ]
     },
     "execution_count": 976,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "e.display()\n",
    "display(out)\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.selection_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfdadd6-468e-4fb9-9718-7224d695c24c",
   "metadata": {},
   "source": [
    "Observe that Agent7 does not manage to learn to obtain the optimum avarage valence in Environment7\n",
    "\n",
    "This is because it selects the next action that has the highest expecte valence but it does not take into account the proposal weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50d96e-4f12-4531-a122-9d13d8bd8639",
   "metadata": {},
   "source": [
    "# ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf06ea-5925-4590-80dd-f9284f4306a6",
   "metadata": {},
   "source": [
    "Create Agent7 that computes the proclivity of proposed interactions by multiplying the expectet valence with the weight, and select the proposed interaction that has the highest proclivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "2e150c60-5315-4a49-822f-2681fb22a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent7(Agent):\n",
    "    def select_intended_interaction(self):\n",
    "        \"\"\"Selects the intended interaction from the proposed dataframe\"\"\"\n",
    "        # Modify to compute the proclivity and select the action that has the hiest proclivity\n",
    "        # Find the first row that has the highest proclivity\n",
    "        max_index = self.selection_df['E(Va)'].idxmax()\n",
    "        intended_interaction_key = self.selection_df.loc[max_index, ['intended']].values[0]\n",
    "        print(f\"Intended Max E(Va) {intended_interaction_key}\")\n",
    "        self._intended_interaction = self._interactions[intended_interaction_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "3ab0a978-5e55-4edd-bed0-8f249fa1f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent7(Agent):\n",
    "    def select_intended_interaction(self):\n",
    "        # The sum weight per action\n",
    "        grouped_df = self.selection_df.groupby('action').agg({'weight': 'sum'}).reset_index()\n",
    "        self.selection_df = self.selection_df.merge(grouped_df, on='action', suffixes=('', '_sum'))\n",
    "        # self.selection_df[\"sum_weight\"] = self.selection_df.groupby(\"action\")[\"weight\"].transform(\"sum\")\n",
    "\n",
    "        # Compute the proclivity\n",
    "        self.selection_df['proclivity'] = self.selection_df[\"weight_sum\"] * self.selection_df['E(Va)']\n",
    "\n",
    "        # Select the action that has the highest proclivity\n",
    "        max_index = self.selection_df['proclivity'].idxmax()\n",
    "        intended_interaction_key = self.selection_df.loc[max_index, ['intended']].values[0]\n",
    "        print(f\"Intended Max proclivity {intended_interaction_key}\")\n",
    "        self._intended_interaction = self._interactions[intended_interaction_key]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4fec51-19aa-447d-a456-893249acc994",
   "metadata": {},
   "source": [
    "## Test your Agent7 in Environment7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "id": "7db78d74-6fd8-4132-8c7d-d201a4084fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a new agent\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1)\n",
    "]\n",
    "a = Agent7(interactions)\n",
    "e = Environment7()\n",
    "\n",
    "# Output widget for displaying the plot\n",
    "out = Output()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "id": "0159e4d4-f851-432b-8127-63403d7a45c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f95447ce5b43078e0f569432b136af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1\n",
      "Reinforcing (00:-1, 01:1: 2)\n",
      "Learning ((10:-1, 00:-1: 1), 01:1: 1)\n",
      "Learning (10:-1, (00:-1, 01:1: 2): 1)\n",
      "Intended Max proclivity 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proposed</th>\n",
       "      <th>E(Vi)</th>\n",
       "      <th>action</th>\n",
       "      <th>E(Va)</th>\n",
       "      <th>interaction</th>\n",
       "      <th>weight</th>\n",
       "      <th>intended</th>\n",
       "      <th>weight_sum</th>\n",
       "      <th>proclivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>00</td>\n",
       "      <td>3</td>\n",
       "      <td>00</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  proposed  E(Vi)  action  E(Va) interaction  weight intended  weight_sum  \\\n",
       "0       10    0.0       1    0.0          10       0       10           0   \n",
       "1       00   -1.0       0   -1.0          00       3       00           3   \n",
       "\n",
       "   proclivity  \n",
       "0         0.0  \n",
       "1        -3.0  "
      ]
     },
     "execution_count": 972,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "e.display()\n",
    "display(out)\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.selection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7fcb67-1fe1-47ad-817e-4ad9ba905e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
