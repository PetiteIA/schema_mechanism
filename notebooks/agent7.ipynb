{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f34084-d4b8-4160-89ad-a927a0919615",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PetiteIA/schema_mechanism/blob/master/notebooks/agent7.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec58b3-e210-4a78-9610-f6615254de29",
   "metadata": {},
   "source": [
    "# THE AGENT WHO LIVED THE NEXT DAY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ef995-42ce-44ba-9f3b-8bfa808b08fe",
   "metadata": {},
   "source": [
    "# Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14a47d-ca87-4080-8f84-afcd2bc83170",
   "metadata": {},
   "source": [
    "Upon completing this lab, you will be able to implement a developmental agent that selects an action based on the anticipation of the two next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16db8-1bd6-4e82-9cf6-2b37d996ffe8",
   "metadata": {},
   "source": [
    "# Define the necessary classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17704e-ca60-417b-8d7e-b8f77cfa09a0",
   "metadata": {},
   "source": [
    "Ensure the required packages are installed if they aren't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7e5da-0188-4924-92f4-3ecbfc2fce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44155c4a-d038-40b3-be33-e8cf77c5f976",
   "metadata": {},
   "source": [
    "We keep improving the Interaction class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e96c3637-4161-49aa-9f1d-0d3342de92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, _action, _outcome, _valence):\n",
    "        self._action = _action\n",
    "        self._outcome = _outcome\n",
    "        self._valence = _valence\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_decision(self):\n",
    "        \"\"\"Return the decision key\"\"\"\n",
    "        return f\"a{self._action}\"\n",
    "\n",
    "    def get_primitive_action(self):\n",
    "        \"\"\"Return the action for compatibility with CompositeInteraction\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_outcome(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._outcome\n",
    "\n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self._action}{self._outcome}\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key. Used for compatibility with CompositeInteraction\"\"\"\n",
    "        return self.key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self._action}{self._outcome}:{self._valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.key() == other.key()\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa086c1-5987-4375-939a-aa217b6048f6",
   "metadata": {},
   "source": [
    "We keep improving the CompositeInteraction class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3243d6-1de3-418e-9236-8af0287131da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeInteraction:\n",
    "    \"\"\"A composite interaction is a tuple (pre_interaction, post_interaction) and a weight\"\"\"\n",
    "    def __init__(self, pre_interaction, post_interaction):\n",
    "        self.pre_interaction = pre_interaction\n",
    "        self.post_interaction = post_interaction\n",
    "        self.weight = 1\n",
    "        self.isActivated = False\n",
    "\n",
    "    def get_decision(self):\n",
    "        \"\"\"Return the sequence of decisions\"\"\"\n",
    "        # return self.key()\n",
    "        return f\"{self.pre_interaction.key()}{self.post_interaction.get_decision()}\"\n",
    "\n",
    "    def get_primitive_action(self):\n",
    "        \"\"\"Return the primite action\"\"\"\n",
    "        return self.pre_interaction.get_primitive_action()\n",
    "\n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the valence of the pre_interaction plus the valence of the post_interaction\"\"\"\n",
    "        return self.pre_interaction.get_valence() + self.post_interaction.get_valence()\n",
    "\n",
    "    def reinforce(self):\n",
    "        \"\"\"Increment the composite interaction's weight\"\"\"\n",
    "        self.weight += 1\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictionary is the string '<pre_interaction><post_interaction>'. \"\"\"\n",
    "        return f\"({self.pre_interaction.key()},{self.post_interaction.key()})\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key of the pre_interaction\"\"\"\n",
    "        return self.pre_interaction.pre_key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print the interaction in the Newick tree format (pre_interaction, post_interaction: valence) \"\"\"\n",
    "        return f\"({self.pre_interaction}, {self.post_interaction}: {self.weight})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same pre and post interactions \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return (self.pre_interaction == other.pre_interaction) and (self.post_interaction == other.post_interaction)\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152512a3-ef97-473a-91a7-9a36fe70ba33",
   "metadata": {},
   "source": [
    "## Define the Agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d65a49-9e8a-4834-b064-d9fff651abdc",
   "metadata": {},
   "source": [
    "Let's implement an Agent that can learn composite interaction as illustrated in Figure 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e95995-8077-46ed-a03d-e318623cb82b",
   "metadata": {},
   "source": [
    "![Agent5](img/Figure_1_Agent7.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742aecbc-5e94-4cc6-bf5d-1b6c12bde814",
   "metadata": {},
   "source": [
    "Figure 1: Agent7 records and reinforces two levels of composite interactions:\n",
    "* First-level composite interaction $c_{t-1} = (i_{t-2}, i_{t-1})$, \n",
    "* Second-level composite interaction $((i_{t-3}, i_{t-2}), i_{t-1})$, and $(i_{t-3}, (i_{t-2}, i_{t-1}))$. \n",
    "\n",
    "The last enacted primitive interaction $i_{t-1}$ and the last enacted composite interaction $c_{t-1}$ activates previously-learned composite interactions that propose their post interaction. \n",
    "Now the post interaction may be a composite interaction.\n",
    "\n",
    "Post interactions are aggregated by their action. The proclivity is computed for each action. The action that has the hiest proclivity is selected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6bcb84-69cb-4a3f-a9d7-68c4f61efd60",
   "metadata": {},
   "source": [
    "## Implement the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25144f-168f-4e7a-ab31-d8d11a7c87d1",
   "metadata": {},
   "source": [
    "* The `learn()` method is improved to learn the second-level composite interactions\n",
    "* The `create_proposed_df()` method creates the dataframe of proposed interaction used for selection\n",
    "* The `aggregate_propositions()` method aggregates the proclivities of each proposition by actions\n",
    "* The `decide()` method select the action that has the highest proclivity and computes the predicted primitive interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0bf369-48ef-48d7-a92b-3c7ef8dc1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, _interactions):\n",
    "        \"\"\" Initialize our agent \"\"\"\n",
    "        self._interactions = {interaction.key(): interaction for interaction in _interactions}\n",
    "        self._composite_interactions = {}\n",
    "        self._intended_interaction = self._interactions[\"00\"]\n",
    "        self._last_interaction = None\n",
    "        self._previous_interaction = None\n",
    "        self._penultimate_interaction = None\n",
    "        self._last_composite_interaction = None\n",
    "        self._previous_composite_interaction = None\n",
    "        # Create a dataframe of default primitive interactions\n",
    "        default_interactions = [interaction for interaction in _interactions if interaction.get_outcome() == 0]\n",
    "        data = {'activated': [np.nan] * len(default_interactions),\n",
    "                'weight': [0] * len(default_interactions),\n",
    "                'action': [i.get_primitive_action() for i in default_interactions],\n",
    "                'interaction': [i.key() for i in default_interactions],\n",
    "                'valence': [i.get_valence() for i in default_interactions],\n",
    "                'decision': [i.get_decision() for i in default_interactions],\n",
    "                'proclivity': [0] * len(default_interactions)}\n",
    "        self.primitive_df = pd.DataFrame(data)\n",
    "        # Store the selection dataframe as a class attribute so we can display it in the notebook\n",
    "        self.proposed_df = None\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\"Implement the agent's policy\"\"\"\n",
    "        # Memorize the context\n",
    "        self._previous_composite_interaction = self._last_composite_interaction\n",
    "        self._penultimate_interaction = self._previous_interaction\n",
    "        self._previous_interaction = self._last_interaction\n",
    "        self._last_interaction = self._interactions[f\"{self._intended_interaction.get_action()}{_outcome}\"]\n",
    "\n",
    "        # tracing the previous cycle\n",
    "        print(\n",
    "            f\"Action: {self._intended_interaction.get_action()}, Prediction: {self._intended_interaction.get_outcome()}, \"\n",
    "            f\"Outcome: {_outcome}, Prediction_correct: {self._intended_interaction.get_outcome() == _outcome}, \"\n",
    "            f\"Valence: {self._last_interaction.get_valence()}\")\n",
    "\n",
    "        # Call the learning mechanism\n",
    "        self.learn()\n",
    "\n",
    "        # Create the proposed dataframe\n",
    "        self.create_proposed_df()\n",
    "        self.aggregate_propositions()\n",
    "\n",
    "        # Select the intended primitive interaction\n",
    "        self.decide()\n",
    "\n",
    "        return self._intended_interaction.get_action()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Learn the composite interactions\"\"\"\n",
    "        # First level of composite interactions\n",
    "        self._last_composite_interaction = self.learn_composite_interaction(self._previous_interaction,\n",
    "                                                                            self._last_interaction)\n",
    "        # Second level of composite interactions\n",
    "        self.learn_composite_interaction(self._previous_composite_interaction, self._last_interaction)\n",
    "        self.learn_composite_interaction(self._penultimate_interaction, self._last_composite_interaction)\n",
    "\n",
    "    def learn_composite_interaction(self, pre_interaction, post_interaction):\n",
    "        \"\"\"Record or reinforce the composite interaction made of (pre_interaction, post_interaction)\"\"\"\n",
    "        if pre_interaction is None:\n",
    "            return None\n",
    "        else:\n",
    "            # If the pre interaction exist\n",
    "            composite_interaction = CompositeInteraction(pre_interaction, post_interaction)\n",
    "            if composite_interaction.key() not in self._composite_interactions:\n",
    "                # Add the composite interaction to memory\n",
    "                self._composite_interactions[composite_interaction.key()] = composite_interaction\n",
    "                print(f\"Learning {composite_interaction}\")\n",
    "                return composite_interaction\n",
    "            else:\n",
    "                # Reinforce the existing composite interaction and return it\n",
    "                self._composite_interactions[composite_interaction.key()].reinforce()\n",
    "                print(f\"Reinforcing {self._composite_interactions[composite_interaction.key()]}\")\n",
    "                return self._composite_interactions[composite_interaction.key()]\n",
    "\n",
    "    def create_proposed_df(self):\n",
    "        \"\"\"Create the proposed dataframe from the activated interactions\"\"\"\n",
    "        # The list of activated interaction that match the current context\n",
    "        activated_keys = [composite_interaction.key() for composite_interaction in\n",
    "                          self._composite_interactions.values()\n",
    "                          if composite_interaction.pre_interaction == self._last_interaction or\n",
    "                          composite_interaction.pre_interaction == self._last_composite_interaction]\n",
    "        data = {'activated': activated_keys,\n",
    "                'weight': [self._composite_interactions[k].weight for k in activated_keys],\n",
    "                'action': [self._composite_interactions[k].post_interaction.get_primitive_action() for k in activated_keys],\n",
    "                'interaction': [self._composite_interactions[k].post_interaction.pre_key() for k in activated_keys],\n",
    "                'valence': [self._composite_interactions[k].post_interaction.get_valence() for k in activated_keys],\n",
    "                'decision': [self._composite_interactions[k].post_interaction.get_decision() for k in activated_keys],\n",
    "                }\n",
    "        activated_df = pd.DataFrame(data)\n",
    "\n",
    "        # Create the selection dataframe from the primitive and the activated dataframes\n",
    "        self.proposed_df = pd.concat([self.primitive_df, activated_df], ignore_index=True)\n",
    "\n",
    "        # Compute the proclivity for each proposition\n",
    "        self.proposed_df['proclivity'] = self.proposed_df['weight'] * self.proposed_df['valence']\n",
    "\n",
    "    def aggregate_propositions(self):\n",
    "        \"\"\"Aggregate the proclivity\"\"\"\n",
    "        # Compute the proclivity for each action\n",
    "        grouped_df = self.proposed_df.groupby('action').agg({'proclivity': 'sum'}).reset_index()\n",
    "        self.proposed_df = self.proposed_df.merge(grouped_df, on='action', suffixes=('', '_agg'))\n",
    "        # Sort by descending order of proclivity\n",
    "        self.proposed_df = self.proposed_df.sort_values(by=['proclivity_agg', 'decision'], ascending=[False, False])\n",
    "        \n",
    "        # Find the most probable primitive interaction for each action\n",
    "        max_weight_df = self.proposed_df.loc[self.proposed_df.groupby('action')['weight'].idxmax(), ['action', 'interaction']].reset_index(\n",
    "            drop=True)\n",
    "        max_weight_df.columns = ['action', 'intended']\n",
    "        self.proposed_df = self.proposed_df.merge(max_weight_df, on='action')\n",
    "\n",
    "    def decide(self):\n",
    "        \"\"\"Selects the intended interaction from the proposed dataframe\"\"\"\n",
    "        # Find the row that has the highest proclivity\n",
    "        max_index = self.proposed_df['proclivity_agg'].idxmax()\n",
    "        # Find the intended interaction in the row that has the highest proclivity\n",
    "        intended_interaction_key = self.proposed_df.loc[max_index, ['intended']].values[0]\n",
    "        self._intended_interaction = self._interactions[intended_interaction_key]\n",
    "        print(\"Intended\", self._intended_interaction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f2924-f597-4d34-bc96-3602dc4460b7",
   "metadata": {},
   "source": [
    "## Let's Create Environment6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c94c8-e36a-41cd-ad69-976849f8ced9",
   "metadata": {},
   "source": [
    "The agent has two possible actions: move to the left or move to the right. \n",
    "The environment returns outcome 1 when the agent bumps into a light green wall, and then the wall turns dark green until the agent moves away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9a38a7-78b8-4590-90c4-4ce903b26876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import Output\n",
    "from IPython.display import display\n",
    "\n",
    "class Environment6:\n",
    "    \"\"\" The grid \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize the grid \"\"\"\n",
    "        self.grid = np.array([[1, 0, 0, 1]])\n",
    "        self.position = 1\n",
    "\n",
    "    def outcome(self, _action):\n",
    "        \"\"\"Take the action and generate the next outcome \"\"\"\n",
    "        if _action == 0:\n",
    "            # Move left\n",
    "            if self.position > 1:\n",
    "                # No bump\n",
    "                self.position -= 1\n",
    "                self.grid[0, 3] = 1\n",
    "                _outcome = 0\n",
    "            elif self.grid[0, 0] == 1:\n",
    "                # First bump\n",
    "                _outcome = 1\n",
    "                self.grid[0, 0] = 2\n",
    "            else:\n",
    "                # Subsequent bumps\n",
    "                _outcome = 0\n",
    "        else:\n",
    "            # Move right\n",
    "            if self.position < 2:\n",
    "                # No bump\n",
    "                self.position += 1\n",
    "                self.grid[0, 0] = 1\n",
    "                _outcome = 0\n",
    "            elif self.grid[0, 3] == 1:\n",
    "                # First bump\n",
    "                _outcome = 1\n",
    "                self.grid[0, 3] = 2\n",
    "            else:\n",
    "                # Subsequent bumps\n",
    "                _outcome = 0\n",
    "        return _outcome\n",
    "        \n",
    "    def display(self):\n",
    "        \"\"\"Display the grid\"\"\"\n",
    "        out.clear_output(wait=True)\n",
    "        with out:\n",
    "            fig, ax = plt.subplots()\n",
    "            # Hide the ticks\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # Display the grid\n",
    "            ax.imshow(self.grid, cmap='Greens', vmin=0, vmax=2)\n",
    "            plt.scatter(self.position, 0, s=1000)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019f138-a218-44b2-964a-8095fbe891a7",
   "metadata": {},
   "source": [
    "# PRELIMINARY EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82801853-2aef-44a7-8acd-ae2d621048bb",
   "metadata": {},
   "source": [
    "## Run the agent in Environment6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6d76df-89e4-4ab3-a327-afa813a27325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the agent in Environment6\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1)\n",
    "]\n",
    "a = Agent(interactions)\n",
    "e = Environment6()\n",
    "\n",
    "# Output widget for displaying the plot\n",
    "out = Output()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf97eb-7aca-4157-ba8e-3e97f2b85ae1",
   "metadata": {},
   "source": [
    "Run the simulation step by step to see the environment and the proposed DataFrame. Use `Ctrl+Enter` to run the cell bellow and stay on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b044b4d-162e-4491-929c-a73eb584dcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f74a2c64f64818af7d9595b097f959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1\n",
      "Intended 10:-1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activated</th>\n",
       "      <th>weight</th>\n",
       "      <th>action</th>\n",
       "      <th>interaction</th>\n",
       "      <th>valence</th>\n",
       "      <th>decision</th>\n",
       "      <th>proclivity</th>\n",
       "      <th>proclivity_agg</th>\n",
       "      <th>intended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>a1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>a0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activated  weight  action interaction  valence decision  proclivity  \\\n",
       "0        NaN     0.0     1.0          10     -1.0       a1        -0.0   \n",
       "1        NaN     0.0     0.0          00     -1.0       a0        -0.0   \n",
       "\n",
       "   proclivity_agg intended  \n",
       "0             0.0       10  \n",
       "1             0.0       00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "e.display()\n",
    "display(out)\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.proposed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ee7cb-e6ff-4350-ac5b-064f5c5e14b2",
   "metadata": {},
   "source": [
    "Observe that the agent does not manage to obtain a positive valence every second step, even though it learns the composite interactions (10,11) from Step 9 and (00, 01) from Step 17.\n",
    "\n",
    "We will create Agent7 that can do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50d96e-4f12-4531-a122-9d13d8bd8639",
   "metadata": {},
   "source": [
    "# ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf06ea-5925-4590-80dd-f9284f4306a6",
   "metadata": {},
   "source": [
    "Create Agent7 that aggregates the proclivity by decision rather than by action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e150c60-5315-4a49-822f-2681fb22a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent7(Agent):\n",
    "    # *** Modify to aggregate by decision rather than by action***\n",
    "    def aggregate_propositions(self):\n",
    "        \"\"\"Aggregate the proclivity\"\"\"\n",
    "        grouped_df = self.proposed_df.groupby('action').agg({'proclivity': 'sum'}).reset_index()\n",
    "        self.proposed_df = self.proposed_df.merge(grouped_df, on='action', suffixes=('', '_agg'))\n",
    "\n",
    "        # Find the most probable primitive interaction for each action\n",
    "        max_weight_df = self.proposed_df.loc[self.proposed_df.groupby('action')['weight'].idxmax(), ['action', 'interaction']].reset_index(\n",
    "            drop=True)\n",
    "        max_weight_df.columns = ['action', 'intended']\n",
    "        self.proposed_df = self.proposed_df.merge(max_weight_df, on='action')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4fec51-19aa-447d-a456-893249acc994",
   "metadata": {},
   "source": [
    "## Test your Agent7 in Environment6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74c423e8-de20-479c-84f7-8d64b3985433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a new agent\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1)\n",
    "]\n",
    "a = Agent7(interactions)\n",
    "e = Environment6()\n",
    "\n",
    "# Output widget for displaying the plot\n",
    "out = Output()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0159e4d4-f851-432b-8127-63403d7a45c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1efc7a62dc0419192d5e164e82df6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1\n",
      "Reinforcing (10:-1, 10:-1: 6)\n",
      "Reinforcing ((10:-1, 10:-1: 6), 10:-1: 3)\n",
      "Reinforcing (10:-1, (10:-1, 10:-1: 6): 3)\n",
      "Intended 00:-1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activated</th>\n",
       "      <th>weight</th>\n",
       "      <th>action</th>\n",
       "      <th>interaction</th>\n",
       "      <th>valence</th>\n",
       "      <th>decision</th>\n",
       "      <th>proclivity</th>\n",
       "      <th>proclivity_agg</th>\n",
       "      <th>intended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00</td>\n",
       "      <td>-1</td>\n",
       "      <td>a0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>a1</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(10,00)</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>00</td>\n",
       "      <td>-1</td>\n",
       "      <td>a0</td>\n",
       "      <td>-6</td>\n",
       "      <td>-10</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(10,(00,10))</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00</td>\n",
       "      <td>-2</td>\n",
       "      <td>00a1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-10</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(10,11)</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>a1</td>\n",
       "      <td>9</td>\n",
       "      <td>-10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(10,(11,00))</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11a0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(10,(11,10))</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11a1</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(10,10)</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>a1</td>\n",
       "      <td>-6</td>\n",
       "      <td>-10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>((10,10),10)</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>a1</td>\n",
       "      <td>-3</td>\n",
       "      <td>-10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(10,(10,10))</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-2</td>\n",
       "      <td>10a1</td>\n",
       "      <td>-6</td>\n",
       "      <td>-10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>((10,10),00)</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>00</td>\n",
       "      <td>-1</td>\n",
       "      <td>a0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-10</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(10,(10,00))</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-2</td>\n",
       "      <td>10a0</td>\n",
       "      <td>-4</td>\n",
       "      <td>-10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(10,(00,01))</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>00</td>\n",
       "      <td>0</td>\n",
       "      <td>00a0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       activated  weight  action interaction  valence decision  proclivity  \\\n",
       "0            NaN       0       0          00       -1       a0           0   \n",
       "1            NaN       0       1          10       -1       a1           0   \n",
       "2        (10,00)       6       0          00       -1       a0          -6   \n",
       "3   (10,(00,10))       1       0          00       -2     00a1          -2   \n",
       "4        (10,11)       9       1          11        1       a1           9   \n",
       "5   (10,(11,00))       5       1          11        0     11a0           0   \n",
       "6   (10,(11,10))       4       1          11        0     11a1           0   \n",
       "7        (10,10)       6       1          10       -1       a1          -6   \n",
       "8   ((10,10),10)       3       1          10       -1       a1          -3   \n",
       "9   (10,(10,10))       3       1          10       -2     10a1          -6   \n",
       "10  ((10,10),00)       2       0          00       -1       a0          -2   \n",
       "11  (10,(10,00))       2       1          10       -2     10a0          -4   \n",
       "12  (10,(00,01))       5       0          00        0     00a0           0   \n",
       "\n",
       "    proclivity_agg intended  \n",
       "0              -10       00  \n",
       "1              -10       11  \n",
       "2              -10       00  \n",
       "3              -10       00  \n",
       "4              -10       11  \n",
       "5              -10       11  \n",
       "6              -10       11  \n",
       "7              -10       11  \n",
       "8              -10       11  \n",
       "9              -10       11  \n",
       "10             -10       00  \n",
       "11             -10       11  \n",
       "12             -10       00  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "e.display()\n",
    "display(out)\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.proposed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f775a8a-db72-44fb-852c-948bd091fee2",
   "metadata": {},
   "source": [
    "Your Agent7 should be able to select a dicision that involves a sequence of two primitive interactions. \n",
    "After a while, it will select the sequences (00, 01) and (10,11) to obtain a positive valence every second step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba26ffc-9861-4ca2-ad88-60438920dad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6d4a13-ebd6-439a-a15b-03eaa7cb3d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
