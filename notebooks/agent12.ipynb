{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f34084-d4b8-4160-89ad-a927a0919615",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PetiteIA/schema_mechanism/blob/master/notebooks/agent12.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec58b3-e210-4a78-9610-f6615254de29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L'AGENT QUI FAISAIT SES COMPTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ef995-42ce-44ba-9f3b-8bfa808b08fe",
   "metadata": {},
   "source": [
    "# Objectifs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14a47d-ca87-4080-8f84-afcd2bc83170",
   "metadata": {},
   "source": [
    "Nous allons doter l'Agent12 d'une Long Short Term Memory (LSTM) pour améliorer le calcul de l'`expected_valence`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16db8-1bd6-4e82-9cf6-2b37d996ffe8",
   "metadata": {},
   "source": [
    "# Préparons les classes CompositeInteraction et Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d018f-39ae-471d-9976-0737298bc1a0",
   "metadata": {},
   "source": [
    "Nous conservons la même classe `CompositeInteraction` que l'Agent11 sauf que nous ajoutons la méthode `series()` qui renvoie la séquence des tokens des interactions primitive sous forme d'une liste.\n",
    "\n",
    "les tokens sont construits par `action * BASE + outcome`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4b289ae-d5c7-45d2-8b46-9446d3de00f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "ef3243d6-1de3-418e-9236-8af0287131da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeInteraction:\n",
    "    \"\"\"A composite interaction is a tuple (pre_interaction, post_interaction) and a weight\"\"\"\n",
    "    def __init__(self, pre_interaction, post_interaction):\n",
    "        self.pre_interaction = pre_interaction\n",
    "        self.post_interaction = post_interaction\n",
    "        self.weight = 1\n",
    "        self._step = 1\n",
    "\n",
    "    def get_decision(self):\n",
    "        \"\"\"Return the flatten sequence of intermediary primitive interactions terminated with the final decision\"\"\"\n",
    "        return f\"{self.pre_interaction.sequence()}{self.post_interaction.get_decision()}\"\n",
    "\n",
    "    def get_actions(self):\n",
    "        \"\"\"Return the flat sequence of the decisions of this interaction as a string\"\"\"\n",
    "        return f\"{self.pre_interaction.get_actions()}{self.post_interaction.get_actions()}\"\n",
    "    \n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the valence of the pre_interaction plus the valence of the post_interaction\"\"\"\n",
    "        return self.pre_interaction.get_valence() + self.post_interaction.get_valence()\n",
    "\n",
    "    def reinforce(self):\n",
    "        \"\"\"Increment the composite interaction's weight\"\"\"\n",
    "        self.weight += 1\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictionary is the string '<pre_interaction><post_interaction>'. \"\"\"\n",
    "        return f\"({self.pre_interaction.key()},{self.post_interaction.key()})\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key of the pre_interaction\"\"\"\n",
    "        return self.pre_interaction.key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print the interaction in the Newick tree format (pre_interaction, post_interaction: valence) \"\"\"\n",
    "        return f\"({self.pre_interaction}, {self.post_interaction}: {self.weight})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same pre and post interactions \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return (self.pre_interaction == other.pre_interaction) and (self.post_interaction == other.post_interaction)\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_length(self):\n",
    "        \"\"\"Return the length of the number of primitive interactions in this composite interaction\"\"\"\n",
    "        return self.pre_interaction.get_length() + self.post_interaction.get_length()\n",
    "\n",
    "    def increment(self, interaction, interactions):\n",
    "        \"\"\"Increment the step of the appropriate sub-interaction. Return the enacted interaction if it is over, or None if it is ongoing.\"\"\"\n",
    "        # First step \n",
    "        if self._step == 1:\n",
    "            interaction = self.pre_interaction.increment(interaction, interactions)\n",
    "            # Ongoing pre-interaction. Return None\n",
    "            if interaction is None:\n",
    "                return None\n",
    "            # Pre-interaction succeeded. Increment the step and return None\n",
    "            elif interaction == self.pre_interaction:\n",
    "                self._step = 2\n",
    "                return None\n",
    "            # Pre-interaction failed. Reset the step and return the enacted interaction\n",
    "            else:\n",
    "                self._step = 1\n",
    "                return interaction\n",
    "        # Second step\n",
    "        else:\n",
    "            interaction = self.post_interaction.increment(interaction, interactions)\n",
    "            # Ongoing post-interaction. Return None\n",
    "            if interaction is None:\n",
    "                return None\n",
    "            # Post-interaction succeeded. Reset the step and return this interaction\n",
    "            elif interaction == self.post_interaction:\n",
    "                self._step = 1\n",
    "                return self\n",
    "            # Post-interaction failed. Reset the step and return the enacted interaction\n",
    "            else:\n",
    "                self._step = 1\n",
    "                composite_interaction = CompositeInteraction(self.pre_interaction, interaction)\n",
    "                if composite_interaction.key() not in interactions:\n",
    "                    # Add the enacted composite interaction to memory\n",
    "                    interactions[composite_interaction.key()] = composite_interaction\n",
    "                    print(f\"Learning {composite_interaction}\")\n",
    "                    return composite_interaction\n",
    "                else:\n",
    "                    # Reinforce the existing composite interaction and return it\n",
    "                    interactions[composite_interaction.key()].reinforce()\n",
    "                    print(f\"Reinforcing {interactions[composite_interaction.key()]}\")\n",
    "                    return interactions[composite_interaction.key()]\n",
    "\n",
    "    def current(self):\n",
    "        \"\"\"Return the current intended primitive interaction\"\"\"\n",
    "        # Step 1: the current primitive interaction of the pre-interaction\n",
    "        if self._step == 1:\n",
    "            return self.pre_interaction.current()\n",
    "        # Step 2: The current primitive interaction of the post-interaction\n",
    "        else:\n",
    "            return self.post_interaction.current()\n",
    "\n",
    "    def sequence(self):\n",
    "        \"\"\"Return the flat sequence of primitive interactions of this composite interaction\"\"\"\n",
    "        return f\"{self.pre_interaction.sequence()}{self.post_interaction.sequence()}\"\n",
    "\n",
    "    def get_post_interactions(self):\n",
    "        \"\"\"Return the list of the hierarchy of the sub post_interactions\"\"\"\n",
    "        return [self.post_interaction] + self.post_interaction.get_post_interactions()\n",
    "\n",
    "    def series(self):\n",
    "        \"\"\"Return the series of tokens of the primitive interactions\"\"\"\n",
    "        series = self.pre_interaction.series()\n",
    "        series.extend(self.post_interaction.series())\n",
    "        return series\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf913b2-97bf-4bfb-9647-26a18ebbf549",
   "metadata": {},
   "source": [
    "Nous conservons la mêmes classe `Interaction ` que pour l'Agent10 a part la méthode `get_post_interactions()` ajoutée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "ca7fd8bc-9dee-4aef-a217-3ed77844e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, _action, _outcome, _valence):\n",
    "        self._action = _action\n",
    "        self._outcome = _outcome\n",
    "        self._valence = _valence\n",
    "        self.weight = 10\n",
    "        \n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_actions(self):\n",
    "        \"\"\"Return the action as a string for compatibilty with CompositeInteraction\"\"\"\n",
    "        return str(self._action)\n",
    "\n",
    "    def get_decision(self):\n",
    "        \"\"\"Return the decision key\"\"\"\n",
    "        return f\"{self._action}\"\n",
    "        # return f\"a{self._action}\"\n",
    "\n",
    "    def get_outcome(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._outcome\n",
    "\n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self._action}{self._outcome}\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key. Used for compatibility with CompositeInteraction\"\"\"\n",
    "        return \"\"  # self.key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self._action}{self._outcome}:{self._valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.key() == other.key()\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_length(self):\n",
    "        \"\"\"The length of the sequence of this interaction\"\"\"\n",
    "        return 1\n",
    "\n",
    "    def increment(self, interaction, interactions):\n",
    "        \"\"\"Return the enacted interaction for compatibility with composite interactions\"\"\"\n",
    "        return interaction\n",
    "\n",
    "    def current(self):\n",
    "        \"\"\"Return itself for compatibility with composite interactions\"\"\"\n",
    "        return self\n",
    "\n",
    "    def sequence(self):\n",
    "        \"\"\"Return the key. Use for compatibility with composite interactions\"\"\"\n",
    "        return self.key()\n",
    "\n",
    "    def get_post_interactions(self):\n",
    "        \"\"\"Return the empty list for compatibility with composite interactions\"\"\"\n",
    "        return []\n",
    "\n",
    "    def series(self):\n",
    "        \"\"\"Return the token of this primitive interactions in a list\"\"\"\n",
    "        return [self._action * BASE + self._outcome]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50dc0d9-f20d-4eaf-90d1-d9f14caa2409",
   "metadata": {},
   "source": [
    "# Implémententons le modèle LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "4c881251-8ebd-4aab-8e1f-ea855d7d78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.len_vocab = 12\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = 64\n",
    "\n",
    "        embedding_dim = self.len_vocab \n",
    "        # Create an embedding layer to convert token indices to dense vectors\n",
    "        self.embedding = nn.Embedding(self.len_vocab, embedding_dim )\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True)\n",
    "        \n",
    "        # Define the output fully connected layer\n",
    "        self.fc_out = nn.Linear(self.hidden_size, self.len_vocab)\n",
    "\n",
    "        self._optimizer = torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        self._loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Initialize the weights\n",
    "        # Embedding\n",
    "        # nn.init.constant_(self.embedding.weight, 0.5)  # Exemple : tous les poids à 0.5\n",
    "        # Initialisation manuelle des poids et biais du LSTM\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param)  # ou utilisez .copy_() pour valeurs fixes\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "        # Initialisation du fully connected\n",
    "        #nn.init.zeros_(self.fc_out.weight)\n",
    "        nn.init.constant_(self.fc_out.weight, 0.5)\n",
    "        nn.init.constant_(self.fc_out.bias, 0.1)\n",
    "    \n",
    "    def forward(self, input_seq, hidden_in, mem_in):\n",
    "        # Convert token indices to dense vectors\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        # input_embs = input_seq.type(torch.float32)\n",
    "\n",
    "        # Pass the embeddings through the LSTM layer\n",
    "        output, (hidden_out, mem_out) = self.lstm(input_embs, (hidden_in, mem_in))\n",
    "                \n",
    "        # Pass the LSTM output through the fully connected layer to get the final output\n",
    "        return self.fc_out(output), hidden_out, mem_out\n",
    "\n",
    "    def fit(self, inputs, targets):\n",
    "\n",
    "        input_tensor = torch.tensor(inputs) # , dtype=torch.int)\n",
    "        # print(\"input tensor\", input_tensor)\n",
    "        labels = torch.tensor(targets)\n",
    "        # print(\"label tensor\", labels)\n",
    "        \n",
    "        # Loop through each epoch\n",
    "        for epoch in range(20):    \n",
    "            # Set model to training mode\n",
    "            self.train()\n",
    "            train_acc = 0\n",
    "    \n",
    "            # Initialize hidden and memory states\n",
    "            hidden = torch.zeros(self.num_layers, input_tensor.shape[0], self.hidden_size, device=\"cpu\")\n",
    "            memory = torch.zeros(self.num_layers, input_tensor.shape[0], self.hidden_size, device=\"cpu\")\n",
    "    \n",
    "            # Forward pass through the model\n",
    "            pred, hidden, memory = self(input_tensor, hidden, memory)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = self._loss_func(pred[:, -1, :], labels)\n",
    "        \n",
    "            # Backpropagation and optimization\n",
    "            self._optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self._optimizer.step()\n",
    "    \n",
    "            # Append training loss to logger\n",
    "            # training_loss_logger.append(loss.item())\n",
    "    \n",
    "            # Calculate training accuracy\n",
    "            train_acc += (pred[:, -1, :].argmax(1) == labels).sum()\n",
    "        print(f\"acc : {train_acc/len(labels):.3f} = {train_acc}/{len(labels)} for epoch {epoch}\")\n",
    "\n",
    "    def predict(self, sequence):\n",
    "        # Construct the context sequence\n",
    "        sequence = torch.tensor(sequence, dtype=torch.float)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, sequence.shape[0], self.hidden_size, device=\"cpu\")\n",
    "        cell = torch.zeros(self.num_layers, sequence.shape[0], self.hidden_size, device=\"cpu\")\n",
    "        \n",
    "        with torch.no_grad():  # Pas de calcul de gradients en mode prédiction\n",
    "            logits, _, _ = self(sequence, h, cell)\n",
    "        ## probabilities = nn.functional.softmax(logits[0, -1, :], dim=0).tolist()\n",
    "        # Compute the probability of each outcome for each action\n",
    "        pairwise_logits = logits[0, -1, :].reshape(-1, 2)\n",
    "        probabilities = nn.functional.softmax(pairwise_logits, dim=1).flatten().tolist()\n",
    "        # print(\"probabilities\", probabilities)\n",
    "        return probabilities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0957e7b6-d560-4e4e-a8bf-0a7b62817a7a",
   "metadata": {},
   "source": [
    "# Implémentons l'agent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78505b37-4870-4370-9a4d-ba67f0d86241",
   "metadata": {},
   "source": [
    "Nous ajoutons une méthode qui estime la `expected_valence` sur la base du LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "bf545ea9-a6b5-4c2a-8179-4145894e26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, _interactions):\n",
    "        \"\"\" Initialize our agent \"\"\"\n",
    "        # Initialise le réseau de neurone\n",
    "        self._lstm = LSTM()\n",
    "        # Les intéreactions doivent être triées dans l'ordre de leurs tokens\n",
    "        self._interactions = dict(sorted({interaction.key(): interaction for interaction in _interactions}.items()))\n",
    "        self._primitive_intended_interaction = self._interactions[\"00\"]\n",
    "        self._intended_interaction = None\n",
    "\n",
    "        # The context\n",
    "        self._penultimate_interaction = None\n",
    "        self._previous_interaction = None\n",
    "        self._last_interaction = None\n",
    "        self._penultimate_composite_interaction = None\n",
    "        self._previous_composite_interaction = None\n",
    "        self._last_composite_interaction = None\n",
    "        \n",
    "        # Prepare the dataframe of proposed interactions\n",
    "        default_interactions = [interaction for interaction in _interactions if interaction.get_outcome() == 0]\n",
    "        data = {'activated': [\"\"] * len(default_interactions),\n",
    "                'weight': [0] * len(default_interactions),\n",
    "                'actions': [i.get_actions() for i in default_interactions],\n",
    "                'intention': [i.key() for i in default_interactions],\n",
    "                'valence': [i.get_valence() for i in default_interactions],\n",
    "                'decision': [i.get_decision() for i in default_interactions],\n",
    "                'length': [1] * len(default_interactions),\n",
    "                'pre': [\"\"] * len(default_interactions)} \n",
    "        self._default_df = pd.DataFrame(data)\n",
    "        self.proposed_df = None\n",
    "        self.decision_df = None\n",
    "        self.clear = True # Used to clear the display after the enacted interaction\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\"Implement the agent's policy\"\"\"\n",
    "        # Trace the previous cycle\n",
    "        primitive_enacted_interaction = self._interactions[f\"{self._primitive_intended_interaction.get_action()}{_outcome}\"]\n",
    "        print(\n",
    "            f\"Action: {self._primitive_intended_interaction.get_action()}, Prediction: {self._primitive_intended_interaction.get_outcome()}, \"\n",
    "            f\"Outcome: {_outcome}, Prediction_correct: {self._primitive_intended_interaction.get_outcome() == _outcome}, \"\n",
    "            f\"Valence: {primitive_enacted_interaction.get_valence()}\")\n",
    "\n",
    "        # Follow up the enaction\n",
    "        if self._intended_interaction is None: # First interaction cycle\n",
    "            enacted_interaction = primitive_enacted_interaction\n",
    "        else:\n",
    "            enacted_interaction = self._intended_interaction.increment(primitive_enacted_interaction, self._interactions)\n",
    "\n",
    "        # If the intended interaction is over (completely enacted or aborted)\n",
    "        if enacted_interaction is None:\n",
    "            self.clear = False\n",
    "        else:\n",
    "            self.clear = True\n",
    "            # Memorize the context\n",
    "            self._penultimate_composite_interaction = self._previous_composite_interaction\n",
    "            self._previous_composite_interaction = self._last_composite_interaction\n",
    "            self._penultimate_interaction = self._previous_interaction\n",
    "            self._previous_interaction = self._last_interaction\n",
    "            self._last_interaction = enacted_interaction\n",
    "            # Call the learning mechanism\n",
    "            self.learn(enacted_interaction)\n",
    "            self.train()\n",
    "            # Create the proposed dataframe\n",
    "            self.create_proposed_df()\n",
    "            self.aggregate_propositions()\n",
    "            # Decide the next enaction\n",
    "            self.decide()\n",
    "\n",
    "        # Return the next primitive action\n",
    "        self._primitive_intended_interaction = self._intended_interaction.current()\n",
    "        return self._primitive_intended_interaction.get_action()\n",
    "        \n",
    "    def learn(self, enacted_interaction):\n",
    "        \"\"\"Learn the composite interactions\"\"\"\n",
    "        # First level of composite interactions\n",
    "        self._last_composite_interaction = self.learn_composite_interaction(self._previous_interaction, enacted_interaction)\n",
    "        # Second level of composite interactions\n",
    "        self.learn_composite_interaction(self._previous_composite_interaction, enacted_interaction)\n",
    "        self.learn_composite_interaction(self._penultimate_interaction, self._last_composite_interaction)\n",
    "\n",
    "        # Higher level composite interaction made of two composite interactions\n",
    "        if self._last_composite_interaction is not None:\n",
    "            self.learn_composite_interaction(self._penultimate_composite_interaction, self._last_composite_interaction)\n",
    "\n",
    "    def learn_composite_interaction(self, pre_interaction, post_interaction):\n",
    "        \"\"\"Record or reinforce the composite interaction made of (pre_interaction, post_interaction)\"\"\"\n",
    "        if pre_interaction is None:\n",
    "            return None\n",
    "        else:\n",
    "            # If the pre-interaction exists\n",
    "            composite_interaction = CompositeInteraction(pre_interaction, post_interaction)\n",
    "            if composite_interaction.key() not in self._interactions:\n",
    "                # Add the composite interaction to memory\n",
    "                self._interactions[composite_interaction.key()] = composite_interaction\n",
    "                print(f\"Learning {composite_interaction}\")\n",
    "                return composite_interaction\n",
    "            else:\n",
    "                # Reinforce the existing composite interaction and return it\n",
    "                self._interactions[composite_interaction.key()].reinforce()\n",
    "                print(f\"Reinforcing {self._interactions[composite_interaction.key()]}\")\n",
    "                return self._interactions[composite_interaction.key()]\n",
    "\n",
    "    def create_proposed_df(self):\n",
    "        \"\"\"Create the proposed dataframe from the activated interactions\"\"\"\n",
    "        # The list of activated interactions that match the current context\n",
    "        activated_interactions = [i for i in self._interactions.values() if i.get_length() > 1 \n",
    "                                  and i.pre_interaction in self._last_composite_interaction.get_post_interactions()]\n",
    "        data = {'activated': [i.key() for i in activated_interactions],\n",
    "                'weight': [i.weight for i in activated_interactions],\n",
    "                'actions': [i.post_interaction.get_actions() for i in activated_interactions],\n",
    "                'intention': [i.post_interaction.key() for i in activated_interactions],\n",
    "                'valence': [i.post_interaction.get_valence() for i in activated_interactions],\n",
    "                'decision': [i.post_interaction.get_decision() for i in activated_interactions],\n",
    "                'pre': [i.post_interaction.pre_key() for i in activated_interactions],\n",
    "                'length': [i.post_interaction.get_length() for i in activated_interactions],\n",
    "                }\n",
    "        activated_df = pd.DataFrame(data).astype(self._default_df.dtypes)  # Force the same types in case activated_df is empty\n",
    "\n",
    "        # Create the proposed dataframe\n",
    "        self.proposed_df = pd.concat([self._default_df, activated_df], ignore_index=True).sort_values(by='decision', ascending=True).reset_index(drop=True)\n",
    "\n",
    "        # Calculate the proclivity of each proposition\n",
    "        self.proposed_df['proclivity'] = self.proposed_df['weight'] * self.proposed_df['valence']\n",
    "\n",
    "        # Compute the probability of each propositions\n",
    "        # self.proposed_df['probability'] = self.proposed_df['weight'] / self.proposed_df.groupby('actions')['weight'].transform('sum')\n",
    "        self.proposed_df['probability'] = self.proposed_df.groupby('intention')['weight'].transform('sum') / self.proposed_df.groupby('actions')['weight'].transform('sum')\n",
    "\n",
    "    def aggregate_propositions(self):\n",
    "        \"\"\"Aggregate the proclivity\"\"\"\n",
    "        # Aggregate the proclivity for each decision\n",
    "        grouped_df = self.proposed_df.groupby('decision').agg({'proclivity': 'sum', 'actions': 'first', # 'action': 'first', \n",
    "                                                               'length': 'first', 'intention': 'first', 'pre': 'first'}).reset_index()\n",
    "        # For each proposed composite decision \n",
    "        for index, proposed in grouped_df[grouped_df['length'] > 1].iterrows():\n",
    "            # print(f\"Index {index}, actions {proposition['actions']}, intention {proposition['intention']}\")\n",
    "            # Find shorter decisions that start with the same sequence \n",
    "            for _, shorter in self.proposed_df[self.proposed_df.apply(lambda row: proposed['actions'].startswith(row['actions']) \n",
    "                                                                      and row['length'] < proposed['length'], axis=1)].iterrows():\n",
    "                # Add the proclivity of the shorter decisions\n",
    "                grouped_df.loc[index, 'proclivity'] += shorter['proclivity']\n",
    "                # print(f\"Decision {proposed['decision']} recieves {shorter['proclivity']} from shorter {shorter['intention']}\")\n",
    "        \n",
    "        # Sort by descending proclivity\n",
    "        self.decision_df = grouped_df.sort_values(by=['proclivity', 'decision'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    def decide(self):\n",
    "        \"\"\"Selects the intended_interaction at the top of the proposed dataframe\"\"\"\n",
    "        # The intended interaction is in the first row because it has been sorted by descending proclivity\n",
    "        intended_interaction_key = self.decision_df.loc[0, 'intention']\n",
    "        print(\"Intention:\", intended_interaction_key)\n",
    "        self._intended_interaction = self._interactions[intended_interaction_key]\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the LSTM Model\"\"\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afad0a0-b639-4168-8a62-c97a323b67e8",
   "metadata": {},
   "source": [
    "# Modifions l'environnement SmallLoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "1ef6497c-c828-49c6-9288-847b23474da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"sav\"\n",
    "\n",
    "FORWARD = 0\n",
    "FEEL_FRONT = 1\n",
    "FEEL_LEFT = 2\n",
    "FEEL_RIGHT = 3\n",
    "TURN_LEFT = 4\n",
    "TURN_RIGHT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "c153ef60-2a2c-4cfe-9896-92b47ba234e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from ipywidgets import Button, HBox,VBox, Output\n",
    "from IPython.display import display\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "FEELING_EMPTY = 2\n",
    "FEELING_WALL = 3\n",
    "BUMPING = 4\n",
    "\n",
    "colors = [\"#b0b0b0\", '#b0b0b0', '#ffffff', '#535865', \"#F93943\"]  # Hidden environment\n",
    "colors = [\"#D6D6D6\", '#5C946E', '#FAE2DB', '#535865', \"#F93943\"]\n",
    "agent_color = \"#1976D2\"\n",
    "\n",
    "class SmallLoop():\n",
    "    def __init__(self, position, direction):\n",
    "        self.grid = np.array([\n",
    "            [1, 1, 1, 1, 1, 1], \n",
    "            [1, 0, 0, 0, 1, 1],\n",
    "            [1, 0, 1, 0, 0, 1],\n",
    "            [1, 0, 1, 1, 0, 1],\n",
    "            [1, 0, 0, 0, 0, 1],\n",
    "            [1, 1, 1, 1, 1, 1]\n",
    "        ])\n",
    "        self.maze = self.grid.copy()\n",
    "        self.position = np.array(position) \n",
    "        self.direction = direction\n",
    "        self.cmap = ListedColormap(colors)\n",
    "        self.norm = BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5, 4.5], self.cmap.N)\n",
    "        self.marker_size = 400\n",
    "        self.marker_map = {LEFT: '<', DOWN: 'v', RIGHT: '>', UP: '^'}\n",
    "        self.marker_color = agent_color\n",
    "        self.directions = np.array([\n",
    "            [0, -1],  # Left\n",
    "            [1, 0],   # Down\n",
    "            [0, 1],   # Right\n",
    "            [-1, 0]   # Up\n",
    "            ])\n",
    "\n",
    "    def outcome(self, action):\n",
    "        \"\"\"Update the grid. Return the outcome of the action.\"\"\"\n",
    "        result = 0\n",
    "        # x, y = self.position\n",
    "\n",
    "        if action == FORWARD:  \n",
    "            target_position = self.position + self.directions[self.direction]\n",
    "            if self.grid[tuple(target_position)] == 0:\n",
    "                self.position[:] = target_position\n",
    "            else:\n",
    "                result = 1\n",
    "                self.maze[tuple(target_position)] = BUMPING\n",
    "        \n",
    "        elif action == TURN_RIGHT:\n",
    "            self.direction = {LEFT: UP, DOWN: LEFT, RIGHT: DOWN, UP: RIGHT}[self.direction]\n",
    "        \n",
    "        elif action == TURN_LEFT:\n",
    "            self.direction = {LEFT: DOWN, DOWN: RIGHT, RIGHT: UP, UP: LEFT}[self.direction]\n",
    "        \n",
    "        elif action == FEEL_FRONT:\n",
    "            feeling_position = self.position + self.directions[self.direction]\n",
    "            if self.grid[tuple(feeling_position)] == 0:\n",
    "                self.maze[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                result = 1\n",
    "                self.maze[tuple(feeling_position)] = FEELING_WALL\n",
    "        \n",
    "        elif action == FEEL_LEFT:\n",
    "            feeling_position = self.position + self.directions[(self.direction + 1) % 4]\n",
    "            if self.grid[tuple(feeling_position)] == 0:\n",
    "                self.maze[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                result = 1\n",
    "                self.maze[tuple(feeling_position)] = FEELING_WALL\n",
    "        \n",
    "        elif action == FEEL_RIGHT:\n",
    "            feeling_position = self.position + self.directions[self.direction - 1]\n",
    "            if self.grid[tuple(feeling_position)] == 0:\n",
    "                self.maze[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                result = 1\n",
    "                self.maze[tuple(feeling_position)] = FEELING_WALL\n",
    "\n",
    "        print(f\"Line: {self.position[0]}, Column: {self.position[1]}, direction: {self.direction}\")\n",
    "        return result  \n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the grid in the notebook\"\"\"\n",
    "        out.clear_output(wait=True)\n",
    "        with out:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.imshow(self.maze, cmap=self.cmap, norm=self.norm)\n",
    "            plt.scatter(self.position[1], self.position[0], s=self.marker_size, marker=self.marker_map[self.direction], c=self.marker_color)\n",
    "            ax.text(4.5, 0, f\"{step:>3}\", fontsize=12, color='White')\n",
    "            plt.show()\n",
    "    \n",
    "    def save(self, step):\n",
    "        \"\"\"Save the display as a PNG file\"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.axis('off')\n",
    "        ax.imshow(self.maze, cmap=self.cmap, norm=self.norm)\n",
    "        plt.scatter(self.position[1], self.position[0], s=self.marker_size, marker=self.marker_map[self.direction], c=self.marker_color)\n",
    "        ax.text(4.5, 0, f\"{step:>4}\", fontsize=12, color='White')\n",
    "        plt.savefig(f\"{save_dir}/{step:04}.png\", bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    def clear(self, clear):\n",
    "        \"\"\"Clear the grid display\"\"\"\n",
    "        if clear:\n",
    "            self.maze[:, :] = self.grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0188a-e490-4c14-b3d1-50868d0ccd3f",
   "metadata": {},
   "source": [
    "# Testons l'agent dans le Small Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "e59db638-c069-4589-ae5f-fe50f0fac89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc20c2b66b7411383451d739a7f46f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciate the small loop environment\n",
    "e = SmallLoop([1, 1], 0)\n",
    "\n",
    "# Instanciate the agent \n",
    "interactions = [\n",
    "    Interaction(FORWARD,0,5),\n",
    "    Interaction(FORWARD,1,-10),\n",
    "    Interaction(TURN_LEFT,0,-3),\n",
    "    Interaction(TURN_LEFT,1,-3),\n",
    "    Interaction(TURN_RIGHT,0,-3),\n",
    "    Interaction(TURN_RIGHT,1,-3),\n",
    "    Interaction(FEEL_FRONT,0,-1),\n",
    "    Interaction(FEEL_FRONT,1,-1),\n",
    "    Interaction(FEEL_LEFT,0,-1),\n",
    "    Interaction(FEEL_LEFT,1,-1),\n",
    "    Interaction(FEEL_RIGHT,0,-1),\n",
    "    Interaction(FEEL_RIGHT,1,-1)\n",
    "]\n",
    "a = Agent(interactions)\n",
    "\n",
    "# Initialize the interaction loop\n",
    "step = 0\n",
    "outcome = 0\n",
    "\n",
    "# Display\n",
    "out = Output()\n",
    "e.display()\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "6e38a590-b256-433f-a1d9-100cc7b0a191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: 5\n",
      "Intention: 00\n",
      "Line: 1, Column: 1, direction: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activated</th>\n",
       "      <th>weight</th>\n",
       "      <th>actions</th>\n",
       "      <th>intention</th>\n",
       "      <th>valence</th>\n",
       "      <th>decision</th>\n",
       "      <th>length</th>\n",
       "      <th>pre</th>\n",
       "      <th>proclivity</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>-3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>-3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activated  weight actions intention  valence decision  length pre  \\\n",
       "0                 0       0        00        5        0       1       \n",
       "1                 0       1        10       -1        1       1       \n",
       "2                 0       2        20       -1        2       1       \n",
       "3                 0       3        30       -1        3       1       \n",
       "4                 0       4        40       -3        4       1       \n",
       "5                 0       5        50       -3        5       1       \n",
       "\n",
       "   proclivity  probability  \n",
       "0           0          NaN  \n",
       "1           0          NaN  \n",
       "2           0          NaN  \n",
       "3           0          NaN  \n",
       "4           0          NaN  \n",
       "5           0          NaN  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step: {step}\")\n",
    "step += 1\n",
    "action = a.action(outcome)\n",
    "e.display()\n",
    "e.save(step)  # Save the image file \n",
    "e.clear(a.clear)  # Clear the display grid if the current composite enaction is over\n",
    "outcome = e.outcome(action)\n",
    "a.proposed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e6f49-9090-42c3-b4db-9309e6c59518",
   "metadata": {},
   "source": [
    "# Créons l'Agent 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "2cad4af5-863c-4629-8766-f3695e5ed5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent12(Agent):\n",
    "    def train(self):\n",
    "        \"\"\"Entrain le LSTM avec toutes les interactions composite\"\"\"\n",
    "        # Pour toutes les interactions composite jusqu'a la longuer 4\n",
    "        for l in range(2, 5):\n",
    "            x = [i.series()[:-1] for i in self._interactions.values() if i.get_length() == l]\n",
    "            print(\"x\", x)\n",
    "            y = [i.series()[-1] for i in self._interactions.values() if i.get_length() == l]\n",
    "            print(\"y\", y)\n",
    "            if len(x) > 0:\n",
    "                self._lstm.fit(x, y)\n",
    "\n",
    "    def create_expected_df(self, sequence):\n",
    "        \"\"\"Create the dataframe of expected valence per interaction\"\"\"\n",
    "        # On prédit les probabilités\n",
    "        probabilities = self._model.predict([sequence])\n",
    "        # Le dataframe qui donne les expected valence pour chaque interaction\n",
    "        expected_df = pd.DataFrame({\n",
    "            'interaction': [i.key() for i in self._interactions.values() if i.get_length() == 1],\n",
    "            'action': [i.action for i in self._interactions.values() if i.get_length() == 1],\n",
    "            'outcome': [i.outcome for i in self._interactions.values() if i.get_length() == 1],\n",
    "            'valence': [i.valence for i in self._interactions.values() if i.get_length() == 1],\n",
    "            'probability': probabilities})\n",
    "        expected_df['expected_valence'] = expected_df['valence'] * expected_df['probability']\n",
    "        return expected_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50993b35-38e3-47b9-ab81-2ef1a1e13651",
   "metadata": {},
   "source": [
    "# Testons l'Agent12 dans le Small Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "5840f69c-8a47-4930-b0de-1ac7867a6edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e88fd4627a14215b9138d011015785d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciate the small loop environment\n",
    "e = SmallLoop([1, 1], 0)\n",
    "\n",
    "# Instanciate the agent \n",
    "interactions = [\n",
    "    Interaction(FORWARD,0,5),\n",
    "    Interaction(FORWARD,1,-10),\n",
    "    Interaction(TURN_LEFT,0,-3),\n",
    "    Interaction(TURN_LEFT,1,-3),\n",
    "    Interaction(TURN_RIGHT,0,-3),\n",
    "    Interaction(TURN_RIGHT,1,-3),\n",
    "    Interaction(FEEL_FRONT,0,-1),\n",
    "    Interaction(FEEL_FRONT,1,-1),\n",
    "    Interaction(FEEL_LEFT,0,-1),\n",
    "    Interaction(FEEL_LEFT,1,-1),\n",
    "    Interaction(FEEL_RIGHT,0,-1),\n",
    "    Interaction(FEEL_RIGHT,1,-1)\n",
    "]\n",
    "a = Agent12(interactions)\n",
    "\n",
    "# Initialize the interaction loop\n",
    "step = 0\n",
    "outcome = 0\n",
    "\n",
    "# Display\n",
    "out = Output()\n",
    "e.display()\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "75432201-d234-4fd8-bf11-c1bfe9ce5547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 12\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: 5\n",
      "Learning (10:-1, 00:5: 1)\n",
      "Learning ((00:5, 10:-1: 1), 00:5: 1)\n",
      "Learning (00:5, (10:-1, 00:5: 1): 1)\n",
      "Learning ((40:-3, 00:5: 1), (10:-1, 00:5: 1): 1)\n",
      "x [[0], [1], [1], [3], [1], [4], [1], [7], [1], [8], [0], [2]]\n",
      "y [1, 1, 3, 1, 4, 1, 7, 1, 8, 0, 2, 0]\n",
      "acc : 0.583 = 7/12 for epoch 19\n",
      "x [[0, 1], [0, 1], [1, 1], [1, 1], [1, 3], [1, 3], [3, 1], [3, 1], [1, 4], [1, 4], [4, 1], [4, 1], [1, 7], [1, 7], [7, 1], [7, 1], [1, 8], [1, 8], [8, 0], [8, 0], [0, 2], [0, 2]]\n",
      "y [1, 1, 3, 3, 1, 1, 4, 4, 1, 1, 7, 7, 1, 1, 8, 8, 0, 0, 2, 2, 0, 0]\n",
      "acc : 0.909 = 20/22 for epoch 19\n",
      "x [[0, 1, 1], [1, 1, 3], [1, 3, 1], [3, 1, 4], [1, 4, 1], [4, 1, 7], [1, 7, 1], [7, 1, 8], [1, 8, 0], [8, 0, 2]]\n",
      "y [3, 1, 4, 1, 7, 1, 8, 0, 2, 0]\n",
      "acc : 1.000 = 10/10 for epoch 19\n",
      "Intention: (10,00)\n",
      "Line: 3, Column: 1, direction: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activated</th>\n",
       "      <th>weight</th>\n",
       "      <th>actions</th>\n",
       "      <th>intention</th>\n",
       "      <th>valence</th>\n",
       "      <th>decision</th>\n",
       "      <th>length</th>\n",
       "      <th>pre</th>\n",
       "      <th>proclivity</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(00,01)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>01</td>\n",
       "      <td>-10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>-10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(00,(01,01))</td>\n",
       "      <td>1</td>\n",
       "      <td>00</td>\n",
       "      <td>(01,01)</td>\n",
       "      <td>-20</td>\n",
       "      <td>010</td>\n",
       "      <td>2</td>\n",
       "      <td>01</td>\n",
       "      <td>-20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(00,10)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(00,(10,00))</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>(10,00)</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>-3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>-3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      activated  weight actions intention  valence decision  length pre  \\\n",
       "0                     0       0        00        5        0       1       \n",
       "1       (00,01)       1       0        01      -10        0       1       \n",
       "2  (00,(01,01))       1      00   (01,01)      -20      010       2  01   \n",
       "3                     0       1        10       -1        1       1       \n",
       "4       (00,10)       1       1        10       -1        1       1       \n",
       "5  (00,(10,00))       1      10   (10,00)        4      100       2  10   \n",
       "6                     0       2        20       -1        2       1       \n",
       "7                     0       3        30       -1        3       1       \n",
       "8                     0       4        40       -3        4       1       \n",
       "9                     0       5        50       -3        5       1       \n",
       "\n",
       "   proclivity  probability  \n",
       "0           0          0.0  \n",
       "1         -10          1.0  \n",
       "2         -20          1.0  \n",
       "3           0          1.0  \n",
       "4          -1          1.0  \n",
       "5           4          1.0  \n",
       "6           0          NaN  \n",
       "7           0          NaN  \n",
       "8           0          NaN  \n",
       "9           0          NaN  "
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step: {step}\")\n",
    "step += 1\n",
    "action = a.action(outcome)\n",
    "e.display()\n",
    "e.save(step)  # Save the image file \n",
    "e.clear(a.clear)  # Clear the display grid if the current composite enaction is over\n",
    "outcome = e.outcome(action)\n",
    "a.proposed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d654bc-0f41-48f1-a587-4003d711bef0",
   "metadata": {},
   "source": [
    "# 1000 cycles d'interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15cd40-815e-4ca8-8acc-968d6c18d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(1000):\n",
    "    print(f\"Step: {step}\")\n",
    "    # step += 1\n",
    "    action = a.action(outcome)\n",
    "    e.display()\n",
    "    e.save(step)  # Save the image file \n",
    "    e.clear(a.clear)  # Clear the display grid if the current composite enaction is over\n",
    "    outcome = e.outcome(action)\n",
    "# a.proposed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f2449d-1ae9-495e-a26e-fc6a297c4438",
   "metadata": {},
   "source": [
    "# Créons le film gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "id": "77dd51a7-b14b-42af-b6fc-c015db2ce4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v2 as imageio\n",
    "import os\n",
    "\n",
    "img_dir = f\"./{save_dir}\"\n",
    "all_files = [os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith('.png')]\n",
    "images = [imageio.imread(f) for f in all_files]\n",
    "imageio.mimsave(\"movie.gif\", images, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a034c32-737a-4a6f-9295-44bd34f703d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
