{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f34084-d4b8-4160-89ad-a927a0919615",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PetiteIA/schema_mechanism/blob/master/notebooks/agent5.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec58b3-e210-4a78-9610-f6615254de29",
   "metadata": {},
   "source": [
    "# THE AGENT WHO CHANGED HIS MIND"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ef995-42ce-44ba-9f3b-8bfa808b08fe",
   "metadata": {},
   "source": [
    "# Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14a47d-ca87-4080-8f84-afcd2bc83170",
   "metadata": {},
   "source": [
    "Upon completing this lab, you will be able to implement a developmental agent that reinforces simple behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16db8-1bd6-4e82-9cf6-2b37d996ffe8",
   "metadata": {},
   "source": [
    "## Define the Interaction class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44155c4a-d038-40b3-be33-e8cf77c5f976",
   "metadata": {},
   "source": [
    "Let's use the same interaction class as Agent4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96c3637-4161-49aa-9f1d-0d3342de92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, action, outcome, valence):\n",
    "        self.action = action\n",
    "        self.outcome = outcome\n",
    "        self.valence = valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self.action}{self.outcome}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self.action}{self.outcome}:{self.valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        return self.key() == other.key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152512a3-ef97-473a-91a7-9a36fe70ba33",
   "metadata": {},
   "source": [
    "## Define the Agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b81c3-5671-4ec6-bb7d-9db4f99c7c0b",
   "metadata": {},
   "source": [
    "The agent is initialized with the list of interactions \n",
    "\n",
    "On a new step _t+1_:\n",
    "* The interaction enacted on step _t_ is memorized in `self._last_interaction`\n",
    "* The interaction enacted on step _t-1_ is memorized in `self._previous_interaction`\n",
    "* The intended interaction `(selected action, predicted outcome)` is memorized in `self._intended_interaction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f0bf369-48ef-48d7-a92b-3c7ef8dc1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Creating our agent\"\"\"\n",
    "    def __init__(self, _interactions):\n",
    "        \"\"\" Initialize the dictionary of interactions\"\"\"\n",
    "        self._interactions = {interaction.key(): interaction for interaction in _interactions}\n",
    "        self._intended_interaction = self._interactions[\"20\"]\n",
    "        self._last_interaction = None\n",
    "        self._previous_interaction = None\n",
    "\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\" Tracing the previous cycle \"\"\"\n",
    "        self._previous_interaction = self._last_interaction\n",
    "        self._last_interaction = self._interactions[f\"{self._intended_interaction.action}{_outcome}\"]\n",
    "        print(f\"Action: {self._intended_interaction.action}, Prediction: {self._intended_interaction.outcome}, \"\n",
    "              f\"Outcome: {_outcome}, Prediction_correct: {self._intended_interaction.outcome == _outcome}, \"\n",
    "              f\"Valence: {self._last_interaction.valence})\")\n",
    "\n",
    "        \"\"\" Computing the next interaction to try to enact \"\"\"\n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        intended_action = 2\n",
    "        # TODO: Implement the agent's prediction mechanism\n",
    "        intended_outcome = 0\n",
    "        # Memorize the intended interaction\n",
    "        self._last_interaction = self._interactions[f\"{intended_action}{intended_outcome}\"]\n",
    "        return intended_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1c38f-c8ed-48e5-bc6a-e7078ed3e5e4",
   "metadata": {},
   "source": [
    "## Environment1 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d65ca61-9386-4260-a406-ec766063569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment1:\n",
    "    \"\"\" In Environment 1, action 2 yields outcome 0, action 3 yields outcome 1 \"\"\"\n",
    "    def outcome(self, _action):\n",
    "        if _action == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6d7d6-5444-4778-b97a-cc5bd34c6cd2",
   "metadata": {},
   "source": [
    "## Environment2 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91ae8fba-72aa-4194-be5a-2f27a1637e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment2:\n",
    "    \"\"\" In Environment 2, action 2 yields outcome 1, action 3 yields outcome 0 \"\"\"\n",
    "    def outcome(self, _action):\n",
    "        if _action == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab7280-a559-4fc2-8668-4285aac82d8d",
   "metadata": {},
   "source": [
    "## Environment3 class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df2375-680e-4df9-a0ed-e84802aac1a4",
   "metadata": {},
   "source": [
    "Environment 3 yields outcome 1 only when the agent alternates actions 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bf9a4ba-2703-49e9-a63f-b72c08b8663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment3:\n",
    "    \"\"\" Environment 3 yields outcome 1 only when the agent alternates actions 0 and 1 \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializing Environment3 \"\"\"\n",
    "        self.previous_action = 0\n",
    "\n",
    "    def outcome(self, _action):\n",
    "        if _action == self.previous_action:\n",
    "            _outcome = 0\n",
    "        else:\n",
    "            _outcome = 1\n",
    "        self.previous_action = _action\n",
    "        return _outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372722d3-e6a3-4fb1-8c65-d58d6b825a17",
   "metadata": {},
   "source": [
    "## Environment4 class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553bc09f-31b0-4577-b153-6fe98225c892",
   "metadata": {},
   "source": [
    "Environment4 behaves like Environment1 during the first 10 cycles and then like Environment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf9630d4-7e78-450b-8940-463e2cd2f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment4:\n",
    "    \"\"\" Environm4 \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializing Environment4 \"\"\"\n",
    "        self.step = 0\n",
    "\n",
    "    def outcome(self, _action):\n",
    "        \"\"\"Take the action and generate the next outcome \"\"\"\n",
    "        self.step += 1\n",
    "        # Behave like environment1 during the first 10 steps\n",
    "        if self.step < 10:\n",
    "            if _action == 2:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1            \n",
    "        # Behave like Environment2 after the first 10 steps\n",
    "        else: \n",
    "            if _action == 2:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f49f7-9352-4938-9ee2-7d8a7bb5065a",
   "metadata": {},
   "source": [
    "## Initialize the interactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59da5f51-d5db-4cf4-8bd0-036ec11eaad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = [\n",
    "    Interaction(2,0,-1),\n",
    "    Interaction(2,1,1),\n",
    "    Interaction(3,0,-1),\n",
    "    Interaction(3,1,1),\n",
    "    Interaction(4,0,-1),\n",
    "    Interaction(5,1,1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d25cea-a183-45c5-a624-1345aace245a",
   "metadata": {},
   "source": [
    "Interactions are initialized with their action, their outcome, and their valence:\n",
    "\n",
    "|| outcome 0 | outcome 1|\n",
    "|---|---|---|\n",
    "| action 2| -1 | 1 |\n",
    "| action 3 | -1 | 1 |\n",
    "| action 4 | -1 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095ec84-3fcd-455c-bb3b-f9a72980048e",
   "metadata": {},
   "source": [
    "## Instantiate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33066d34-7f06-4b38-97b8-d7d5adbb237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent(interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16279c4-936b-4f37-a0c9-ca4b77609adf",
   "metadata": {},
   "source": [
    "## Instantiate the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17b3c8c8-0cbd-4f64-a97c-61519ad24dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Environment4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ce510-9438-47d1-ac88-ab79c063d592",
   "metadata": {},
   "source": [
    "## Test run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f6d76df-89e4-4ab3-a327-afa813a27325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n"
     ]
    }
   ],
   "source": [
    "outcome = 0\n",
    "for i in range(30):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310417af-f0ed-44c2-b99c-9e374bd9eabf",
   "metadata": {},
   "source": [
    "# AGENT5 DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef084288-168a-4a54-9934-5adf87e16a05",
   "metadata": {},
   "source": [
    "Implémentons l'Agent5 qui va prédire la probabilité de chaque outcome pour chaque action possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87058ae-6a93-4936-9c25-ed71fc92a189",
   "metadata": {},
   "source": [
    "## Créons le réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172732f1-0462-427b-ac80-bb4469e23302",
   "metadata": {},
   "source": [
    "Le modèle a trois entrées: previous_action, previous_outcome, action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74196251-d38e-43bd-beba-3c97dc76f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 6)\n",
    "        # Apply He Initialization recommended for ReLU\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "        self.fc2 = nn.Linear(6, 2)\n",
    "        # Apply Xavier initialisation recommended for linear activation\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)  # Biases are usually set to zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))  # Apply non-linearity\n",
    "        return self.fc2(x)  # Logits (CrossEntropyLoss handles softmax)\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c09ae4-2781-4b91-9a87-9ef7b73b8b83",
   "metadata": {},
   "source": [
    "# Définisson l'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f804222b-c3e5-4228-9ba0-ce6fc57b5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "class Agent5:\n",
    "    \"\"\"Creating our agent\"\"\"\n",
    "    def __init__(self, _interactions):\n",
    "        \"\"\" Initialize the dictionary of interactions\"\"\"\n",
    "        # Initialise le réseau de neurone\n",
    "        self._model = Model()\n",
    "        self._criterion = nn.CrossEntropyLoss()  # Cross-entropy for classification\n",
    "        self._optimizer = optim.SGD(model.parameters(), lr=0.3)  # SGD optimizer\n",
    "        \n",
    "        self._interactions = {interaction.key(): interaction for interaction in _interactions}\n",
    "        self._intended_interaction = self._interactions[\"20\"]\n",
    "        self._last_interaction = None\n",
    "        self._previous_interaction = None\n",
    "        # Store the selection dataframe as a class attribute so we can display it in the notebook\n",
    "        self.selection_df = None\n",
    "\n",
    "\n",
    "    def fit(self, inputs, targets):\n",
    "        \"\"\"La fonction d'apprentissage\"\"\"\n",
    "        input_tensor = torch.tensor(inputs, dtype=torch.float)\n",
    "        # input_tensor = torch.randn_like(input_tensor) * 0.01 (voir si le modèle apprend des tendances)\n",
    "        target_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "        labels = torch.nn.functional.one_hot(target_tensor, num_classes=2).to(torch.float)\n",
    "        #labels = torch.argmax(target_tensor, dim=1)  # Convert one-hot to class indices\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = self._model(input_tensor)  # Forward pass\n",
    "        loss = self._criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        self._optimizer.step()  # Update weights\n",
    "\n",
    "        # Check accuracy (we expect 100% accuracy)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        accuracy = (predictions == target_tensor).float().mean().item()\n",
    "\n",
    "        print(f\"Loss: {loss.item():.6f}, Accuracy: {accuracy * 100:.0f}%\")\n",
    "\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        \"\"\"La fonction de prediction\"\"\"\n",
    "        input_tensor = torch.tensor(inputs, dtype=torch.float)\n",
    "        outputs = self._model(input_tensor)\n",
    "        print(\"prediction\", torch.argmax(outputs, dim=1))\n",
    "        return torch.softmax(outputs, dim=1) \n",
    "        \n",
    "    \n",
    "    def action(self, _outcome):\n",
    "        \"\"\" Tracing the previous cycle \"\"\"\n",
    "        self._previous_interaction = self._last_interaction\n",
    "        self._last_interaction = self._interactions[f\"{self._intended_interaction.action}{_outcome}\"]\n",
    "        print(f\"Action: {self._intended_interaction.action}, Prediction: {self._intended_interaction.outcome}, \"\n",
    "              f\"Outcome: {_outcome}, Prediction_correct: {self._intended_interaction.outcome == _outcome}, \"\n",
    "              f\"Valence: {self._last_interaction.valence})\")\n",
    "\n",
    "        \"\"\" Computing the next interaction to try to enact \"\"\"\n",
    "        # Entraine le réseau de neurone avec les informations du dernier cycle d'interaction\n",
    "        if self._previous_interaction is not None:\n",
    "            self.fit([[self._previous_interaction.action, self._previous_interaction.outcome, self._intended_interaction.action]], [outcome])\n",
    "\n",
    "        # Prédit les résultats pour les différentes actions\n",
    "        probabilities = self.predict([[self._intended_interaction.action, outcome, 2], [self._intended_interaction.action, outcome, 3]])\n",
    "        # print(predictions)\n",
    "        \n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        # Le dataframe pour trouver la meilleure expected valence\n",
    "        data = {'action': [2, 2, 3, 3],\n",
    "                'outcome': [0, 1, 0, 1],\n",
    "                'valence': [self._interactions[i].valence for i in [\"20\", \"21\", \"30\", \"31\"]],\n",
    "                'probability': probabilities.flatten().tolist()}\n",
    "        self.selection_df = pd.DataFrame(data)\n",
    "        self.selection_df['expected_valence'] = self.selection_df['valence'] * self.selection_df['probability']\n",
    "        print(self.selection_df)\n",
    "\n",
    "        # Aggregate by action\n",
    "        grouped_df = self.selection_df.groupby('action').agg({'expected_valence': 'sum'}).reset_index()\n",
    "        # Sort by descending order of expected valence\n",
    "        grouped_df = grouped_df.sort_values(by=['expected_valence'], ascending=[False]).reset_index(drop=True)\n",
    "        print(grouped_df)\n",
    "        # Select the action that has the higest expected valence\n",
    "        intended_action = grouped_df.loc[0, 'action']\n",
    "\n",
    "        # TODO: Implement the agent's prediction mechanism\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        intended_outcome = predictions.tolist()[intended_action - 2]\n",
    "        \n",
    "        # Memorize the intended interaction\n",
    "        self._intended_interaction = self._interactions[f\"{intended_action}{intended_outcome}\"]\n",
    "        return intended_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b3f5c-f48c-45ad-bf1d-4a703bac64af",
   "metadata": {},
   "source": [
    "## Test your Agent5 in Environment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d8025780-7d39-442d-9600-b40641e6d6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.860700         -0.860700\n",
      "1       2        1        1     0.139300          0.139300\n",
      "2       3        0       -1     0.759602         -0.759602\n",
      "3       3        1        1     0.240398          0.240398\n",
      "   action  expected_valence\n",
      "0       3         -0.519203\n",
      "1       2         -0.721401\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 1.425457, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n",
      "Action: 3, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.192568, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.978569         -0.978569\n",
      "1       2        1        1     0.021431          0.021431\n",
      "2       3        0       -1     0.958934         -0.958934\n",
      "3       3        1        1     0.041066          0.041066\n",
      "   action  expected_valence\n",
      "0       3         -0.917868\n",
      "1       2         -0.957138\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "a = Agent5(interactions)\n",
    "e = Environment1()\n",
    "outcome = 0\n",
    "for i in range(20):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02f401-3a78-4ced-9b8c-3b602c7be482",
   "metadata": {},
   "source": [
    "## Test your Agent5 in Environment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "925fb5f1-1eb6-4dee-a59b-fd6384e123d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.397219         -0.397219\n",
      "1       2        1        1     0.602781          0.602781\n",
      "2       3        0       -1     0.380046         -0.380046\n",
      "3       3        1        1     0.619954          0.619954\n",
      "   action  expected_valence\n",
      "0       3          0.239907\n",
      "1       2          0.205563\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 0.967462, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n",
      "Action: 3, Prediction: 1, Outcome: 0, Prediction_correct: False, Valence: -1)\n",
      "Loss: 1.055318, Accuracy: 0%\n",
      "prediction tensor([1, 1])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.364659         -0.364659\n",
      "1       2        1        1     0.635341          0.635341\n",
      "2       3        0       -1     0.348082         -0.348082\n",
      "3       3        1        1     0.651918          0.651918\n",
      "   action  expected_valence\n",
      "0       3          0.303837\n",
      "1       2          0.270682\n"
     ]
    }
   ],
   "source": [
    "a = Agent5(interactions)\n",
    "e = Environment2()\n",
    "outcome = 0\n",
    "for i in range(20):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4ebe13-605e-4068-ab57-06416f898790",
   "metadata": {},
   "source": [
    "## Test your Agent5 in Environment3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "835595f7-512e-46aa-b140-3e2b6c43eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: 1)\n",
      "Loss: 3.933289, Accuracy: 0%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.907749         -0.907749\n",
      "1       2        1        1     0.092251          0.092251\n",
      "2       3        0       -1     0.960380         -0.960380\n",
      "3       3        1        1     0.039620          0.039620\n",
      "   action  expected_valence\n",
      "0       2         -0.815497\n",
      "1       3         -0.920760\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.096788, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n",
      "Action: 2, Prediction: 0, Outcome: 0, Prediction_correct: True, Valence: -1)\n",
      "Loss: 0.019773, Accuracy: 100%\n",
      "prediction tensor([0, 0])\n",
      "   action  outcome  valence  probability  expected_valence\n",
      "0       2        0       -1     0.980421         -0.980421\n",
      "1       2        1        1     0.019579          0.019579\n",
      "2       3        0       -1     0.991958         -0.991958\n",
      "3       3        1        1     0.008042          0.008042\n",
      "   action  expected_valence\n",
      "0       2         -0.960842\n",
      "1       3         -0.983917\n"
     ]
    }
   ],
   "source": [
    "a = Agent5(interactions)\n",
    "e = Environment3()\n",
    "outcome = 0\n",
    "for i in range(20):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc2539-8682-4ad2-9840-e5a4a51c0630",
   "metadata": {},
   "source": [
    "## Test your Agent5 in Environment4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb0749-f161-464e-b8dc-255b7b098045",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent5(interactions)\n",
    "e = Environment4()\n",
    "outcome = 0\n",
    "for i in range(20):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc8226-b4ac-49c7-b1a8-9b17adc19964",
   "metadata": {},
   "source": [
    "## Test your Agent5 with interactions that have other valences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01175431-ead3-480e-b8e3-e20ee9321379",
   "metadata": {},
   "source": [
    "Replace the valences of interactions with your choice in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fdfd38-0d4e-48a3-ab0f-87177c2da97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose different valence of interactions\n",
    "interactions = [\n",
    "    Interaction(0,0,1),\n",
    "    Interaction(0,1,0),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1),\n",
    "    Interaction(2,0,-1),\n",
    "    Interaction(2,1,1)\n",
    "]\n",
    "# Run the agent\n",
    "a = Agent5(interactions)\n",
    "e = Environment4()\n",
    "outcome = 0\n",
    "for i in range(20):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1c321-c33f-497e-8219-a0904ab6ebab",
   "metadata": {},
   "source": [
    "## Test your agent in the Turtle environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71630824-e023-49b7-a143-6ffccae07209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install the turtle environment\n",
    "!pip3 install ColabTurtle\n",
    "from ColabTurtle.Turtle import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f35913a4-4426-4e50-ba74-6790e579fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initialize the turtle environment\n",
    "\n",
    "BORDER_WIDTH = 20\n",
    "\n",
    "class ColabTurtleEnvironment:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Creating the Turtle window \"\"\"\n",
    "        bgcolor(\"lightGray\")\n",
    "        penup()\n",
    "        goto(window_width() / 2, window_height()/2)\n",
    "        face(0)\n",
    "        pendown()\n",
    "        color(\"green\")\n",
    "\n",
    "    def outcome(self, action):\n",
    "        \"\"\" Enacting an action and returning the outcome \"\"\"\n",
    "        _outcome = 0\n",
    "        for i in range(10):\n",
    "            # _outcome = 0\n",
    "            if action == 0:\n",
    "                # move forward\n",
    "                forward(10)\n",
    "            elif action == 1:\n",
    "                # rotate left\n",
    "                left(4)\n",
    "                forward(2)\n",
    "            elif action == 2:\n",
    "                # rotate right\n",
    "                right(4)\n",
    "                forward(2)\n",
    "\n",
    "            # Bump on screen edge and return outcome 1\n",
    "            if xcor() < BORDER_WIDTH:\n",
    "                goto(BORDER_WIDTH, ycor())\n",
    "                _outcome = 1\n",
    "            if xcor() > window_width() - BORDER_WIDTH:\n",
    "                goto(window_width() - BORDER_WIDTH, ycor())\n",
    "                _outcome = 1\n",
    "            if ycor() < BORDER_WIDTH:\n",
    "                goto(xcor(), BORDER_WIDTH)\n",
    "                _outcome = 1\n",
    "            if ycor() > window_height() - BORDER_WIDTH:\n",
    "                goto(xcor(), window_height() -BORDER_WIDTH)\n",
    "                _outcome = 1\n",
    "\n",
    "            # Change color\n",
    "            if _outcome == 0:\n",
    "                color(\"green\")\n",
    "            else:\n",
    "                # Finit l'interaction\n",
    "                color(\"red\")\n",
    "                # if action == 0:\n",
    "                #     break\n",
    "                if action == 1:\n",
    "                    for j in range(10):\n",
    "                        left(4)\n",
    "                elif action == 2:\n",
    "                    for j in range(10):\n",
    "                        right(4)\n",
    "                break\n",
    "\n",
    "        return _outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4b183-a761-47c8-a3a5-957467dd5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run the turtle environment\n",
    "initializeTurtle()\n",
    "\n",
    "# Parameterize the rendering\n",
    "bgcolor(\"lightGray\")\n",
    "penup()\n",
    "goto(window_width() / 2, window_height()/2)\n",
    "face(0)\n",
    "pendown()\n",
    "color(\"green\")\n",
    "speed(10)\n",
    "\n",
    "# Some valences to avoid bumping into walls\n",
    "interactions = [\n",
    "    Interaction(0,0,3),\n",
    "    Interaction(0,1,-3),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,-1),\n",
    "    Interaction(2,0,-2),\n",
    "    Interaction(2,1,-2)\n",
    "]\n",
    "\n",
    "a = Agent5(interactions)\n",
    "e = ColabTurtleEnvironment()\n",
    "\n",
    "outcome = 0\n",
    "for i in range(50):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15a134-34a6-4039-9b43-f8fd76d93b5e",
   "metadata": {},
   "source": [
    "## Report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e661c15-c1ed-4512-a1d9-0fbdb8f91879",
   "metadata": {},
   "source": [
    "Explain what you programmed and what results you observed. Export this document as PDF including your code, the traces you obtained, and your explanations below (no more than a few paragraphs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc0899-eee2-404a-baf6-47741067e7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
