{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f34084-d4b8-4160-89ad-a927a0919615",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PetiteIA/schema_mechanism/blob/master/notebooks/agent6.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec58b3-e210-4a78-9610-f6615254de29",
   "metadata": {},
   "source": [
    "# THE AGENT WHO FOLLOWED ANCIENT CLUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ef995-42ce-44ba-9f3b-8bfa808b08fe",
   "metadata": {},
   "source": [
    "# Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14a47d-ca87-4080-8f84-afcd2bc83170",
   "metadata": {},
   "source": [
    "Upon completing this lab, you will be able to implement a developmental agent that learns sequences of interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16db8-1bd6-4e82-9cf6-2b37d996ffe8",
   "metadata": {},
   "source": [
    "## Define the necessary classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44155c4a-d038-40b3-be33-e8cf77c5f976",
   "metadata": {},
   "source": [
    "Let's improve the Interaction class a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "e96c3637-4161-49aa-9f1d-0d3342de92d4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, action, outcome, valence):\n",
    "        self._action = action\n",
    "        self._outcome = outcome\n",
    "        self._valence = valence\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_outcome(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._outcome\n",
    "\n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self._action}{self._outcome}\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        return self.key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self._action}{self._outcome}:{self._valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.key() == other.key()\n",
    "        else:\n",
    "            return False            "
   ]
  },
  {
   "cell_type": "raw",
   "id": "be99690f-4199-4438-afdd-13c7ee0b65fe",
   "metadata": {},
   "source": [
    "Let's improve the CompositeInteraction class a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "ef3243d6-1de3-418e-9236-8af0287131da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CompositeInteraction:\n",
    "    \"\"\"A composite interaction is a tuple (pre_interaction, post_interaction) and a weight\"\"\"\n",
    "    def __init__(self, pre_interaction, post_interaction):\n",
    "        self.pre_interaction = pre_interaction\n",
    "        self.post_interaction = post_interaction\n",
    "        self.weight = 1\n",
    "        self.isActivated = False\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action of the post interaction\"\"\"\n",
    "        return self.post_interaction.get_action()\n",
    "    \n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the valence of the pre_interaction plus the valence of the post_interaction\"\"\"\n",
    "        return self.pre_interaction.get_valence() + self.post_interaction.get_valence()\n",
    "\n",
    "    def reinforce(self):\n",
    "        \"\"\"Increment the composite interaction's weight\"\"\"\n",
    "        self.weight += 1\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictionary is the string '<pre_interaction><post_interaction>'. \"\"\"\n",
    "        return f\"({self.pre_interaction.key()},{self.post_interaction.key()})\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key of the pre_interaction\"\"\"\n",
    "        return self.pre_interaction.pre_key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print the interaction in the Newick tree format (pre_interaction, post_interaction: valence) \"\"\"\n",
    "        return f\"({self.pre_interaction}, {self.post_interaction}: {self.weight})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same pre and post interactions \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return (self.pre_interaction == other.pre_interaction) and (self.post_interaction == other.post_interaction)\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152512a3-ef97-473a-91a7-9a36fe70ba33",
   "metadata": {},
   "source": [
    "## Define the Agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b81c3-5671-4ec6-bb7d-9db4f99c7c0b",
   "metadata": {},
   "source": [
    "We will use a Pandas DataFrame to compute the selection of the next intended interaction and to predict its most likely outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7e5da-0188-4924-92f4-3ecbfc2fce5f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d65a49-9e8a-4834-b064-d9fff651abdc",
   "metadata": {},
   "source": [
    "Let's implement a base Agent that has the functionnalities of Agent5 implemented using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "3f0bf369-48ef-48d7-a92b-3c7ef8dc1daf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, _interactions):\n",
    "        \"\"\" Initialize our agent \"\"\"\n",
    "        self._interactions = {interaction.key(): interaction for interaction in _interactions}\n",
    "        self._composite_interactions = {}\n",
    "        self._intended_interaction = self._interactions[\"00\"]\n",
    "        self._last_interaction = None\n",
    "        self._previous_interaction = None\n",
    "        self._penultimate_interaction = None\n",
    "        self._last_composite_interaction = None\n",
    "        self._previous_composite_interaction = None\n",
    "        # Create a dataframe of default primitive interactions \n",
    "        default_interactions = [interaction for interaction in _interactions if interaction.get_outcome() == 0]\n",
    "        data = {'post_action': [i.get_action() for i in default_interactions],\n",
    "                'weight': [0] * len(default_interactions),\n",
    "                'proclivity': [0] * len(default_interactions),\n",
    "                'post_interaction': [i.key() for i in default_interactions]}\n",
    "        self.primitive_df = pd.DataFrame(data)\n",
    "        # Store the selection dataframe as a class attribute so we can display it in the notebook\n",
    "        self.selection_df = None\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\"Implement the agent's policy\"\"\"\n",
    "        # tracing the previous cycle\n",
    "        self._previous_composite_interaction = self._last_composite_interaction\n",
    "        self._penultimate_interaction = self._previous_interaction\n",
    "        self._previous_interaction = self._last_interaction\n",
    "        self._last_interaction = self._interactions[f\"{self._intended_interaction.get_action()}{_outcome}\"]\n",
    "        print(f\"Action: {self._intended_interaction.get_action()}, Prediction: {self._intended_interaction.get_outcome()}, \"\n",
    "              f\"Outcome: {_outcome}, Prediction_correct: {self._intended_interaction.get_outcome() == _outcome}, \"\n",
    "              f\"Valence: {self._last_interaction.get_valence()}\")\n",
    "\n",
    "        # Call the learning mechanism\n",
    "        self.learn()\n",
    "        \n",
    "        # Create a dataframe from the activated composite interactions \n",
    "        activated_keys = [composite_interaction.key() for composite_interaction in self._composite_interactions.values() \n",
    "                          if composite_interaction.pre_interaction == self._last_interaction or \n",
    "                          composite_interaction.pre_interaction == self._last_composite_interaction]\n",
    "        data = {'composite': activated_keys,\n",
    "                'weight': [self._composite_interactions[k].weight for k in activated_keys],\n",
    "                'post_valence': [self._composite_interactions[k].post_interaction.get_valence() for k in activated_keys],\n",
    "                'post_action': [self._composite_interactions[k].post_interaction.get_action() for k in activated_keys],\n",
    "                'post_interaction': [self._composite_interactions[k].post_interaction.pre_key() for k in activated_keys]\n",
    "                }\n",
    "        activated_df = pd.DataFrame(data)\n",
    "\n",
    "        # Create the selection dataframe from the primitive and the activated dataframes\n",
    "        df = pd.concat([self.primitive_df, activated_df], ignore_index=True)\n",
    "\n",
    "        # Compute the proclivity for each action\n",
    "        df['proclivity'] = df['weight'] * df['post_valence']\n",
    "        grouped_df = df.groupby('post_action').agg({'proclivity': 'sum'}).reset_index()\n",
    "        df = df.merge(grouped_df, on='post_action', suffixes=('', '_sum'))\n",
    "\n",
    "        # Find the most probable outcome for each action\n",
    "        max_weight_df = df.loc[df.groupby('post_action')['weight'].idxmax(), ['post_action', 'post_interaction']].reset_index(drop=True)\n",
    "        max_weight_df.columns = ['post_action', 'intended']\n",
    "        df = df.merge(max_weight_df, on='post_action')\n",
    "\n",
    "        # Find the first row that has the highest proclivity\n",
    "        max_index = df['proclivity_sum'].idxmax()\n",
    "        intended_interaction_key = df.loc[max_index, ['intended']].values[0]\n",
    "        self._intended_interaction = self._interactions[intended_interaction_key]\n",
    "        print(\"Intended\", self._intended_interaction)\n",
    "\n",
    "        # Store the selection dataframe for printing\n",
    "        self.selection_df = df.copy()\n",
    "        \n",
    "        return self._intended_interaction.get_action()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Recording previous composite interaction\"\"\"\n",
    "        if self._previous_interaction is not None:\n",
    "            composite_interaction = CompositeInteraction(self._previous_interaction, self._last_interaction)\n",
    "            if composite_interaction.key() not in self._composite_interactions:\n",
    "                self._composite_interactions[composite_interaction.key()] = composite_interaction\n",
    "                print(f\"Learning {composite_interaction}\")\n",
    "                self._last_composite_interaction = composite_interaction\n",
    "            else:\n",
    "                self._composite_interactions[composite_interaction.key()].reinforce()\n",
    "                print(f\"Reinforcing {self._composite_interactions[composite_interaction.key()]}\")\n",
    "                self._last_composite_interaction = self._composite_interactions[composite_interaction.key()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019f138-a218-44b2-964a-8095fbe891a7",
   "metadata": {},
   "source": [
    "# PRELIMINARY EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f2924-f597-4d34-bc96-3602dc4460b7",
   "metadata": {},
   "source": [
    "Let's test this agent in Environment4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "5bf9a4ba-2703-49e9-a63f-b72c08b8663c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Environment4:\n",
    "    \"\"\" Environment4 \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializing Environment4 \"\"\"\n",
    "        self.step = 0\n",
    "\n",
    "    def outcome(self, _action):\n",
    "        \"\"\"Take the action and generate the next outcome \"\"\"\n",
    "        self.step += 1\n",
    "        # Behave like environment1 during the first 10 steps\n",
    "        if self.step < 10:\n",
    "            if _action == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1            \n",
    "        # Behave like Environment2 after the first 10 steps\n",
    "        else: \n",
    "            if _action == 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ce510-9438-47d1-ac88-ab79c063d592",
   "metadata": {},
   "source": [
    "Initialize the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "6f6d76df-89e4-4ab3-a327-afa813a27325",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate a new agent\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1),\n",
    "    Interaction(2,0,-1),\n",
    "    Interaction(2,1,1)\n",
    "]\n",
    "a = Agent(interactions)\n",
    "e = Environment4()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf97eb-7aca-4157-ba8e-3e97f2b85ae1",
   "metadata": {},
   "source": [
    "Run the simulation step by step to see the Selection DataFrame. Use `Ctrl+Enter` to run the cell bellow and stay on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "5b044b4d-162e-4491-929c-a73eb584dcd1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction_correct: True, Valence: 1\n",
      "Reinforcing (01:1, 01:1: 7)\n",
      "Intended 01:1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>composite</th>\n",
       "      <th>weight</th>\n",
       "      <th>post_valence</th>\n",
       "      <th>post_action</th>\n",
       "      <th>proclivity</th>\n",
       "      <th>proclivity_sum</th>\n",
       "      <th>intended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(01,01)</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  composite  weight  post_valence  post_action  proclivity  proclivity_sum  \\\n",
       "0       NaN       0           NaN            0         NaN             7.0   \n",
       "1       NaN       0           NaN            1         NaN             0.0   \n",
       "2       NaN       0           NaN            2         NaN             0.0   \n",
       "3   (01,01)       7           1.0            0         7.0             7.0   \n",
       "\n",
       "  intended  \n",
       "0       01  \n",
       "1       10  \n",
       "2       20  \n",
       "3       01  "
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.selection_df[['composite', 'weight', 'post_valence', 'post_action', 'proclivity', 'proclivity_sum', 'intended']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9daa278-434e-43ce-8520-8fabef69cd83",
   "metadata": {},
   "source": [
    "Observe the Selection DataFrame above as you run the agent step by step. \n",
    "Each activated composite interaction proposes its post_action with proclity equals to the composite interaction's weight multiplied by the post interactions' valence. \n",
    "\n",
    "The proclivities are summed for each action. The action that has the highest `proclivity_sum` is selected.\n",
    "\n",
    "* During the first 10 steps, the composite interaction (11,11) is progressively reinforced as the agent learns that, in the context of intreaction 11, it can again enact interation 11.\n",
    "* After Step 10, the agent learns the composite interaction (01,01), which tells that, in the context of interaction, 01, the agent can again enact 01."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ef47c-b298-41cd-9372-aac913a3e7f8",
   "metadata": {},
   "source": [
    "Let's define Environment5 in which the agent must perform the same action twice in a row in order to get an outcome of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "6570094a-821a-4974-b2db-882e3d298167",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Environment6:\n",
    "    \"\"\" Environm4 \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializing Environment4 \"\"\"\n",
    "        self._previous_action = 0\n",
    "        self._last_action = 0\n",
    "\n",
    "    def outcome(self, _action):\n",
    "        \"\"\"Take the action and generate the next outcome \"\"\"\n",
    "        if self._previous_action == action:\n",
    "            # If same action then outcome 0\n",
    "            outcome = 0\n",
    "        else:\n",
    "            # If different action then outcome 1\n",
    "            outcome = 1\n",
    "        self._previous_action = self._last_action\n",
    "        self._last_action = action\n",
    "        return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "id": "e8b13bbb-222e-44f8-af27-1ffa755ea999",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Environment7:\n",
    "    \"\"\" Environment5 \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializing Environment4 \"\"\"\n",
    "        self._previous_action = 0\n",
    "        self._last_action = 0\n",
    "\n",
    "    def outcome(self, _action):\n",
    "        \"\"\"Take the action and generate the next outcome \"\"\"\n",
    "        if self._last_action == action and self._previous_action != action:\n",
    "            # If same action twice only\n",
    "            outcome = 1\n",
    "        else:\n",
    "            # If different action then outcome 0\n",
    "            outcome = 0\n",
    "        self._previous_action = self._last_action\n",
    "        self._last_action = action\n",
    "        return outcome     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "id": "1673e65d-ac97-45c3-b69a-c78f871ec380",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Environment5:\n",
    "    \"\"\" Environment5 \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializing Environment4 \"\"\"\n",
    "        self._penultimate_action =0\n",
    "        self._previous_action = 0\n",
    "        self._last_action = 0\n",
    "\n",
    "    def outcome(self, _action):\n",
    "        \"\"\"Take the action and generate the next outcome \"\"\"\n",
    "        if action == self._last_action and action == self._previous_action:\n",
    "            # If same action twice only\n",
    "            outcome = 0\n",
    "        else:\n",
    "            # If different action then outcome 0\n",
    "            outcome = 1\n",
    "        self._penultimate_action = self._previous_action\n",
    "        self._previous_action = self._last_action\n",
    "        self._last_action = action\n",
    "        return outcome     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "91a00992-7b54-433f-a2a0-027c905a56cc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate a new agent\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1)\n",
    "]\n",
    "a = Agent(interactions)\n",
    "e = Environment5()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "id": "d9817644-f5e8-4935-a4ee-bac1dc04ccbd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction_correct: True, Valence: 1\n",
      "Reinforcing (11:1, 01:1: 4)\n",
      "Intended 01:1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>composite</th>\n",
       "      <th>weight</th>\n",
       "      <th>post_valence</th>\n",
       "      <th>post_action</th>\n",
       "      <th>proclivity</th>\n",
       "      <th>proclivity_sum</th>\n",
       "      <th>intended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(01,01)</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(01,00)</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  composite  weight  post_valence  post_action  proclivity  proclivity_sum  \\\n",
       "0       NaN       0           NaN            0         NaN             0.0   \n",
       "1       NaN       0           NaN            1         NaN             0.0   \n",
       "2   (01,01)       3           1.0            0         3.0             0.0   \n",
       "3   (01,00)       3          -1.0            0        -3.0             0.0   \n",
       "\n",
       "  intended  \n",
       "0       01  \n",
       "1       10  \n",
       "2       01  \n",
       "3       01  "
      ]
     },
     "execution_count": 1020,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.selection_df[['composite', 'weight', 'post_valence', 'post_action', 'proclivity', 'proclivity_sum', 'intended']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d304f1-f549-40b2-8737-b4b627e35b2e",
   "metadata": {},
   "source": [
    "Observe that this agent cannot get a positive valence each time. \n",
    "To do so, he would need to select an action base on the last two previous interactions. \n",
    "We are going to design Agent6 that can do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50d96e-4f12-4531-a122-9d13d8bd8639",
   "metadata": {},
   "source": [
    "# ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199faee9-a546-4d86-8e09-484bf2212523",
   "metadata": {},
   "source": [
    "Implement Agent6 that learns a higher level of composite interactions as shown in Figure 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e95995-8077-46ed-a03d-e318623cb82b",
   "metadata": {},
   "source": [
    "![Agent5](img/Figure_1_Agent6.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742aecbc-5e94-4cc6-bf5d-1b6c12bde814",
   "metadata": {},
   "source": [
    "Figure 1: Agent6 records and reinforces two levels of composite interactions:\n",
    "* First-level composite interaction $c_{t-1} = (i_{t-2}, i_{t-1}: weight)$, \n",
    "* Second-level composite interaction $(c_{t-2}, i_{t-1}: weight)$.\n",
    "\n",
    "The last enacted primitive interaction $i_{t-1}$ and the last enacted composite interaction $c_{t-1}$ activates previously-learned composite interactions that propose the action of their post interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e150c60-5315-4a49-822f-2681fb22a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent6(Agent):\n",
    "    def learn(self):\n",
    "        # Recording previous composite interaction\n",
    "        if self._previous_interaction is not None:\n",
    "            # Record or reinforce the first level composite interaction\n",
    "            composite_interaction = CompositeInteraction(self._previous_interaction, self._last_interaction)\n",
    "            if composite_interaction.key() not in self._composite_interactions:\n",
    "                self._composite_interactions[composite_interaction.key()] = composite_interaction\n",
    "                print(f\"Learning {composite_interaction}\")\n",
    "                self._last_composite_interaction = composite_interaction\n",
    "            else:\n",
    "                self._composite_interactions[composite_interaction.key()].reinforce()\n",
    "                print(f\"Reinforcing {self._composite_interactions[composite_interaction.key()]}\")\n",
    "                self._last_composite_interaction = self._composite_interactions[composite_interaction.key()]\n",
    "            # Record or reinforce the second level composite interaction\n",
    "            if self._previous_composite_interaction is not None:\n",
    "                composite_interaction_2 = CompositeInteraction(self._previous_composite_interaction, self._last_interaction)\n",
    "                if composite_interaction_2.key() not in self._composite_interactions:\n",
    "                    self._composite_interactions[composite_interaction_2.key()] = composite_interaction_2\n",
    "                    print(f\"Learning {composite_interaction_2}\")\n",
    "                else:\n",
    "                    self._composite_interactions[composite_interaction_2.key()].reinforce()\n",
    "                    print(f\"Reinforcing {self._composite_interactions[composite_interaction_2.key()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4fec51-19aa-447d-a456-893249acc994",
   "metadata": {},
   "source": [
    "## Test your Agent6 in Environment5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c423e8-de20-479c-84f7-8d64b3985433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a new agent\n",
    "interactions = [\n",
    "    Interaction(0,0,-1),\n",
    "    Interaction(0,1,1),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,1),\n",
    "\n",
    "]\n",
    "a = Agent6(interactions)\n",
    "e = Environment5()\n",
    "\n",
    "# Run the interaction loop\n",
    "step = 0\n",
    "outcome = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159e4d4-f851-432b-8127-63403d7a45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Step {step}\")\n",
    "step += 1\n",
    "action = a.action(outcome)\n",
    "outcome = e.outcome(action)\n",
    "a.selection_df[['composite', 'weight', 'post_valence', 'post_action', 'proclivity', 'proclivity_sum', 'intended']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1e1cc-570b-459f-bbf1-31e1d074f1e4",
   "metadata": {},
   "source": [
    "Observe that Agent6 obtains a positive valence after Step 29."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7fa66-bc71-44bf-85ff-716842a24aad",
   "metadata": {},
   "source": [
    "## Test your Agent6 in the turtle environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "id": "991de95f-e2d4-4638-a7f8-55de36eb3617",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ColabTurtle in c:\\users\\ogeorgeon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "# @title Install the turtle environment\n",
    "!pip3 install ColabTurtle\n",
    "from ColabTurtle.Turtle import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "id": "f35913a4-4426-4e50-ba74-6790e579fb80",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @title Initialize the turtle environment\n",
    "\n",
    "BORDER_WIDTH = 20\n",
    "\n",
    "class ColabTurtleEnvironment:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Creating the Turtle window \"\"\"\n",
    "        bgcolor(\"lightGray\")\n",
    "        penup()\n",
    "        goto(window_width() / 2, window_height()/2)\n",
    "        face(0)\n",
    "        pendown()\n",
    "        color(\"green\")\n",
    "\n",
    "    def outcome(self, action):\n",
    "        \"\"\" Enacting an action and returning the outcome \"\"\"\n",
    "        _outcome = 0\n",
    "        for i in range(10):\n",
    "            # _outcome = 0\n",
    "            if action == 0:\n",
    "                # move forward\n",
    "                forward(10)\n",
    "            elif action == 1:\n",
    "                # rotate left\n",
    "                left(4)\n",
    "                forward(2)\n",
    "            elif action == 2:\n",
    "                # rotate right\n",
    "                right(4)\n",
    "                forward(2)\n",
    "\n",
    "            # Bump on screen edge and return outcome 1\n",
    "            if xcor() < BORDER_WIDTH:\n",
    "                goto(BORDER_WIDTH, ycor())\n",
    "                _outcome = 1\n",
    "            if xcor() > window_width() - BORDER_WIDTH:\n",
    "                goto(window_width() - BORDER_WIDTH, ycor())\n",
    "                _outcome = 1\n",
    "            if ycor() < BORDER_WIDTH:\n",
    "                goto(xcor(), BORDER_WIDTH)\n",
    "                _outcome = 1\n",
    "            if ycor() > window_height() - BORDER_WIDTH:\n",
    "                goto(xcor(), window_height() -BORDER_WIDTH)\n",
    "                _outcome = 1\n",
    "\n",
    "            # Change color\n",
    "            if _outcome == 0:\n",
    "                color(\"green\")\n",
    "            else:\n",
    "                # Finit l'interaction\n",
    "                color(\"red\")\n",
    "                # if action == 0:\n",
    "                #     break\n",
    "                if action == 1:\n",
    "                    for j in range(10):\n",
    "                        left(4)\n",
    "                elif action == 2:\n",
    "                    for j in range(10):\n",
    "                        right(4)\n",
    "                break\n",
    "\n",
    "        return _outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4b183-a761-47c8-a3a5-957467dd5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run the turtle environment\n",
    "import pandas as pd\n",
    "initializeTurtle()\n",
    "\n",
    "# Parameterize the rendering\n",
    "bgcolor(\"lightGray\")\n",
    "penup()\n",
    "goto(window_width() / 2, window_height()/2)\n",
    "face(0)\n",
    "pendown()\n",
    "color(\"green\")\n",
    "speed(10)\n",
    "\n",
    "# Some valences to avoid bumping into walls\n",
    "interactions = [\n",
    "    Interaction(0,0,3),\n",
    "    Interaction(0,1,-3),\n",
    "    Interaction(1,0,-1),\n",
    "    Interaction(1,1,-1),\n",
    "    Interaction(2,0,-2),\n",
    "    Interaction(2,1,-2)\n",
    "]\n",
    "\n",
    "a = Agent6(interactions)\n",
    "e = ColabTurtleEnvironment()\n",
    "\n",
    "outcome = 0\n",
    "for i in range(100):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15a134-34a6-4039-9b43-f8fd76d93b5e",
   "metadata": {},
   "source": [
    "## Report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e661c15-c1ed-4512-a1d9-0fbdb8f91879",
   "metadata": {},
   "source": [
    "Explain what you programmed and what results you observed. Export this document as PDF including your code, the traces you obtained, and your explanations below (no more than a few paragraphs):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
