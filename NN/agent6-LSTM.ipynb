{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f34084-d4b8-4160-89ad-a927a0919615",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PetiteIA/schema_mechanism/blob/master/notebooks/agent5-DNN.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec58b3-e210-4a78-9610-f6615254de29",
   "metadata": {},
   "source": [
    "# UTILISATION D'UNE LSTM POUR GENERER UNE SEQUENCE DE DEUX INTERACTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f079acd-1557-4e30-bf85-7432f730d1dc",
   "metadata": {},
   "source": [
    "Ce notrebook présente notre troisième agent dotté d'un LSTM. \n",
    "Nous définissons l'embedding des interaction dans la class Interaction: une dimension pour l'action et une dimension pour l'outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16db8-1bd6-4e82-9cf6-2b37d996ffe8",
   "metadata": {},
   "source": [
    "# La classe Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44155c4a-d038-40b3-be33-e8cf77c5f976",
   "metadata": {},
   "source": [
    "On définit l'embedding de chaque interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96c3637-4161-49aa-9f1d-0d3342de92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_ACTION = 2 \n",
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, action, outcome, valence):\n",
    "        self.action = action\n",
    "        self.outcome = outcome\n",
    "        self.valence = valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary. \"\"\"\n",
    "        return self.action * BASE_ACTION + self.outcome \n",
    "        # return f\"{self.action}{self.outcome}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self.action}{self.outcome}:{self.valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        return self.key() == other.key()\n",
    "\n",
    "    def embedding(self):\n",
    "        \"\"\"return a list used as the embedding of this interaction\"\"\"\n",
    "        return [self.action, self.outcome]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc0841b-61bd-4809-b733-812bbfb9cec6",
   "metadata": {},
   "source": [
    "# L'environnement SmallLoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0fd77-e435-4791-adbf-0e41255e931f",
   "metadata": {},
   "source": [
    "L'agent a deux actions possibles: aller à droite ou aller à gauche.\n",
    "\n",
    "L'environnement renvoie 1 la première fois que l'agent se cogne dans le mur qui est vert clair, et le mur devient vert foncé. \n",
    "Tant que le mur est vert foncé, l'environnement renvoie 0 jusqu'a ce que le l'agent s'éloigne et le mur redevient vert clair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70344bd8-3772-486e-bb0e-1f0c483469fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"sav\"\n",
    "\n",
    "FORWARD = 0\n",
    "FEEL_FRONT = 1\n",
    "FEEL_LEFT = 2\n",
    "FEEL_RIGHT = 3\n",
    "TURN_LEFT = 4\n",
    "TURN_RIGHT = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c27e6d36-cc73-463c-b8ba-0e5a313498f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from ipywidgets import Button, HBox,VBox, Output\n",
    "from IPython.display import display\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "FEELING_EMPTY = 2\n",
    "FEELING_WALL = 3\n",
    "BUMPING = 4\n",
    "\n",
    "colors = [\"#b0b0b0\", '#b0b0b0', '#ffffff', '#535865', \"#F93943\"]  # Hidden environment\n",
    "colors = [\"#D6D6D6\", '#5C946E', '#FAE2DB', '#535865', \"#F93943\"]\n",
    "agent_color = \"#1976D2\"\n",
    "\n",
    "class SmallLoop():\n",
    "    def __init__(self, position, direction):\n",
    "        self.grid = np.array([\n",
    "            [1, 1, 1, 1, 1, 1], \n",
    "            [1, 0, 0, 0, 1, 1],\n",
    "            [1, 0, 1, 0, 0, 1],\n",
    "            [1, 0, 1, 1, 0, 1],\n",
    "            [1, 0, 0, 0, 0, 1],\n",
    "            [1, 1, 1, 1, 1, 1]\n",
    "        ])\n",
    "        self.maze = self.grid.copy()\n",
    "        self.position = np.array(position) \n",
    "        self.direction = direction\n",
    "        self.cmap = ListedColormap(colors)\n",
    "        self.norm = BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5, 4.5], self.cmap.N)\n",
    "        self.marker_size = 400\n",
    "        self.marker_map = {LEFT: '<', DOWN: 'v', RIGHT: '>', UP: '^'}\n",
    "        self.marker_color = agent_color\n",
    "        self.directions = np.array([\n",
    "            [0, -1],  # Left\n",
    "            [1, 0],   # Down\n",
    "            [0, 1],   # Right\n",
    "            [-1, 0]   # Up\n",
    "            ])\n",
    "\n",
    "    def outcome(self, action):\n",
    "        \"\"\"Update the grid. Return the outcome of the action.\"\"\"\n",
    "        result = 0\n",
    "        # x, y = self.position\n",
    "\n",
    "        if action == FORWARD:  \n",
    "            target_position = self.position + self.directions[self.direction]\n",
    "            if self.grid[tuple(target_position)] == 0:\n",
    "                self.position[:] = target_position\n",
    "            else:\n",
    "                result = 1\n",
    "                self.maze[tuple(target_position)] = BUMPING\n",
    "        \n",
    "        elif action == TURN_RIGHT:\n",
    "            self.direction = {LEFT: UP, DOWN: LEFT, RIGHT: DOWN, UP: RIGHT}[self.direction]\n",
    "        \n",
    "        elif action == TURN_LEFT:\n",
    "            self.direction = {LEFT: DOWN, DOWN: RIGHT, RIGHT: UP, UP: LEFT}[self.direction]\n",
    "        \n",
    "        elif action == FEEL_FRONT:\n",
    "            feeling_position = self.position + self.directions[self.direction]\n",
    "            if self.grid[tuple(feeling_position)] == 0:\n",
    "                self.maze[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                result = 1\n",
    "                self.maze[tuple(feeling_position)] = FEELING_WALL\n",
    "        \n",
    "        elif action == FEEL_LEFT:\n",
    "            feeling_position = self.position + self.directions[(self.direction + 1) % 4]\n",
    "            if self.grid[tuple(feeling_position)] == 0:\n",
    "                self.maze[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                result = 1\n",
    "                self.maze[tuple(feeling_position)] = FEELING_WALL\n",
    "        \n",
    "        elif action == FEEL_RIGHT:\n",
    "            feeling_position = self.position + self.directions[self.direction - 1]\n",
    "            if self.grid[tuple(feeling_position)] == 0:\n",
    "                self.maze[tuple(feeling_position)] = FEELING_EMPTY\n",
    "            else:\n",
    "                result = 1\n",
    "                self.maze[tuple(feeling_position)] = FEELING_WALL\n",
    "\n",
    "        print(f\"Line: {self.position[0]}, Column: {self.position[1]}, direction: {self.direction}\")\n",
    "        return result  \n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the grid in the notebook\"\"\"\n",
    "        out.clear_output(wait=True)\n",
    "        with out:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.imshow(self.maze, cmap=self.cmap, norm=self.norm)\n",
    "            plt.scatter(self.position[1], self.position[0], s=self.marker_size, marker=self.marker_map[self.direction], c=self.marker_color)\n",
    "            ax.text(4.5, 0, f\"{step:>3}\", fontsize=12, color='White')\n",
    "            plt.show()\n",
    "    \n",
    "    def save(self, step):\n",
    "        \"\"\"Save the display as a PNG file\"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.axis('off')\n",
    "        ax.imshow(self.maze, cmap=self.cmap, norm=self.norm)\n",
    "        plt.scatter(self.position[1], self.position[0], s=self.marker_size, marker=self.marker_map[self.direction], c=self.marker_color)\n",
    "        ax.text(4.5, 0, f\"{step:>4}\", fontsize=12, color='White')\n",
    "        plt.savefig(f\"{save_dir}/{step:04}.png\", bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    def clear(self, clear):\n",
    "        \"\"\"Clear the grid display\"\"\"\n",
    "        if clear:\n",
    "            self.maze[:, :] = self.grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310417af-f0ed-44c2-b99c-9e374bd9eabf",
   "metadata": {},
   "source": [
    "# AGENT LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef084288-168a-4a54-9934-5adf87e16a05",
   "metadata": {},
   "source": [
    "Implémentons l'Agent3 qui va prédire la probabilité des prochains tokens d'une séquence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87058ae-6a93-4936-9c25-ed71fc92a189",
   "metadata": {},
   "source": [
    "## Créons le modèle de LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172732f1-0462-427b-ac80-bb4469e23302",
   "metadata": {},
   "source": [
    "Le modèle a deux entrées: previous_interaction, last_interaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "74196251-d38e-43bd-beba-3c97dc76f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.len_vocab = 12\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = 64\n",
    "\n",
    "        embedding_dim = 2  # self.len_vocab \n",
    "        # Create an embedding layer to convert token indices to dense vectors\n",
    "        # self.embedding = nn.Embedding(self.len_vocab, embedding_dim )\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True) # , dropout=0.5)\n",
    "        \n",
    "        # Define the output fully connected layer\n",
    "        self.fc_out = nn.Linear(self.hidden_size, self.len_vocab)\n",
    "\n",
    "        self._optimizer = torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        self._loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Initialize the weights\n",
    "        # Embedding\n",
    "        # nn.init.constant_(self.embedding.weight, 0.5)  # Exemple : tous les poids à 0.5\n",
    "        # Initialisation manuelle des poids et biais du LSTM\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param)  # ou utilisez .copy_() pour valeurs fixes\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "        # Initialisation du fully connected\n",
    "        #nn.init.zeros_(self.fc_out.weight)\n",
    "        nn.init.constant_(self.fc_out.weight, 0.5)\n",
    "        nn.init.constant_(self.fc_out.bias, 0.1)\n",
    "    \n",
    "    def forward(self, input_seq, hidden_in, mem_in):\n",
    "        # Convert token indices to dense vectors\n",
    "        # input_embs = self.embedding(input_seq)\n",
    "        input_embs = input_seq.type(torch.float32)\n",
    "\n",
    "        # Pass the embeddings through the LSTM layer\n",
    "        output, (hidden_out, mem_out) = self.lstm(input_embs, (hidden_in, mem_in))\n",
    "                \n",
    "        # Pass the LSTM output through the fully connected layer to get the final output\n",
    "        return self.fc_out(output), hidden_out, mem_out\n",
    "\n",
    "    def fit(self, inputs, targets):\n",
    "\n",
    "        input_tensor = torch.tensor(inputs) # , dtype=torch.int)\n",
    "        # print(\"input tensor\", input_tensor)\n",
    "        labels = torch.tensor(targets)\n",
    "        # print(\"label tensor\", labels)\n",
    "        \n",
    "        # Loop through each epoch\n",
    "        for epoch in range(20):    \n",
    "            # Set model to training mode\n",
    "            self.train()\n",
    "            train_acc = 0\n",
    "    \n",
    "            # Initialize hidden and memory states\n",
    "            hidden = torch.zeros(self.num_layers, input_tensor.shape[0], self.hidden_size, device=\"cpu\")\n",
    "            memory = torch.zeros(self.num_layers, input_tensor.shape[0], self.hidden_size, device=\"cpu\")\n",
    "    \n",
    "            # Forward pass through the model\n",
    "            pred, hidden, memory = self(input_tensor, hidden, memory)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = self._loss_func(pred[:, -1, :], labels)\n",
    "        \n",
    "            # Backpropagation and optimization\n",
    "            self._optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self._optimizer.step()\n",
    "    \n",
    "            # Append training loss to logger\n",
    "            # training_loss_logger.append(loss.item())\n",
    "    \n",
    "            # Calculate training accuracy\n",
    "            train_acc += (pred[:, -1, :].argmax(1) == labels).sum()\n",
    "        print(f\"acc : {train_acc/len(labels):.3f} = {train_acc}/{len(labels)} for epoch {epoch}\")\n",
    "\n",
    "    def predict(self, sequence):\n",
    "        # Construct the context sequence\n",
    "        sequence = torch.tensor(sequence, dtype=torch.int)\n",
    "\n",
    "        h = torch.zeros(self.num_layers, sequence.shape[0], self.hidden_size, device=\"cpu\")\n",
    "        cell = torch.zeros(self.num_layers, sequence.shape[0], self.hidden_size, device=\"cpu\")\n",
    "        \n",
    "        with torch.no_grad():  # Pas de calcul de gradients en mode prédiction\n",
    "            logits, _, _ = self(sequence, h, cell)\n",
    "        ## probabilities = nn.functional.softmax(logits[0, -1, :], dim=0).tolist()\n",
    "        # Compute the probability of each outcome for each action\n",
    "        pairwise_logits = logits[0, -1, :].reshape(-1, 2)\n",
    "        probabilities = nn.functional.softmax(pairwise_logits, dim=1).flatten().tolist()\n",
    "        # print(\"probabilities\", probabilities)\n",
    "        return probabilities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c09ae4-2781-4b91-9a87-9ef7b73b8b83",
   "metadata": {},
   "source": [
    "# Définisson l'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "f804222b-c3e5-4228-9ba0-ce6fc57b5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Creating our agent\"\"\"\n",
    "    def __init__(self, _interactions):\n",
    "        \"\"\" Initialize the dictionary of interactions\"\"\"\n",
    "        # Initialise le réseau de neurone\n",
    "        self._model = LSTM()\n",
    "        \n",
    "        self._interactions = dict(sorted({interaction.key(): interaction for interaction in _interactions}.items()))\n",
    "        self._intended_interaction = list(self._interactions.values())[0]\n",
    "        self._last_interaction = None\n",
    "        self._previous_interaction = None\n",
    "        self._penultimate_interaction = None\n",
    "        # Le dataframe pour mémoriser les séquences d'interactions\n",
    "        self.sequences_df = pd.DataFrame({\n",
    "            'i1': pd.Series(dtype='int'),\n",
    "            'i2': pd.Series(dtype='int'),\n",
    "            'i3': pd.Series(dtype='int'),\n",
    "            'action': pd.Series(dtype='int'),\n",
    "            'valence': pd.Series(dtype='int'),\n",
    "            'count': pd.Series(dtype='int'),\n",
    "            'proclivity': pd.Series(dtype='int'),\n",
    "        })\n",
    "        self.expected_df = None\n",
    "    \n",
    "    def action(self, _outcome):\n",
    "        \"\"\" Tracing the previous cycle \"\"\"\n",
    "        self._penultimate_interaction = self._previous_interaction \n",
    "        self._previous_interaction = self._last_interaction\n",
    "        self._last_interaction = self._interactions[self._intended_interaction.action * BASE_ACTION + _outcome ]\n",
    "        print(f\"Action: {self._intended_interaction.action}, Prediction: {self._intended_interaction.outcome}, \"\n",
    "              f\"Outcome: {_outcome}, Prediction_correct: {self._intended_interaction.outcome == _outcome}, \"\n",
    "              f\"Valence: {self._last_interaction.valence})\")\n",
    "\n",
    "        \"\"\" Computing the next interaction to try to enact \"\"\"\n",
    "        # Enregistre les séquences dans sequences_df et entraine le LSTM\n",
    "        self.learn()       \n",
    "\n",
    "        # Prédit les probabilités des prochaines interactions\n",
    "        self.expected_df = self.create_expected_df(self._previous_interaction, self._last_interaction)\n",
    "\n",
    "        # Sélectionne l'intended interaction\n",
    "        self._intended_interaction = self.decide()        \n",
    "\n",
    "        # Return the action\n",
    "        return self._intended_interaction.action\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Record sequences\"\"\"\n",
    "        if self._previous_interaction is not None and self._last_interaction is not None and self._penultimate_interaction is not None:\n",
    "            # Record or increment the last sequence\n",
    "            condition = ((self.sequences_df['i1'] == self._penultimate_interaction.key()) & \n",
    "                        (self.sequences_df['i2'] == self._previous_interaction.key()) & \n",
    "                        (self.sequences_df['i3'] == self._last_interaction.key()))\n",
    "            if self.sequences_df[condition].empty:\n",
    "                new_sequence = pd.DataFrame({\n",
    "                    'i1': [self._penultimate_interaction.key()], \n",
    "                    'i2': [self._previous_interaction.key()], \n",
    "                    'i3': [self._last_interaction.key()], \n",
    "                    'action': [self._last_interaction.action], \n",
    "                    'valence': [self._last_interaction.valence],\n",
    "                    'count': [1], \n",
    "                    'proclivity': [0]\n",
    "                })\n",
    "                self.sequences_df = pd.concat([self.sequences_df, new_sequence], ignore_index=True)\n",
    "            else:\n",
    "                # On incrémente le compte (pas utilisé pour l'instant)\n",
    "                self.sequences_df.loc[condition, 'count'] += 1\n",
    "            # Entraine le réseau de neurone avec les séquences enregistrées dans sequences_df\n",
    "            sequence_keys = self.sequences_df[['i1', 'i2']].values.tolist()\n",
    "            x = [[self._interactions[s[0]].embedding(), self._interactions[s[1]].embedding()] for s in sequence_keys]\n",
    "            y = self.sequences_df['i3'].tolist()\n",
    "            self._model.fit(x, y)\n",
    "\n",
    "    def create_expected_df(self, previous_interaction, last_interaction):\n",
    "        \"\"\"Create the dataframe of expected valence per interaction\"\"\"\n",
    "        if previous_interaction is not None and last_interaction is not None:\n",
    "            # On prédit les probabilités\n",
    "            probabilities = self._model.predict([[previous_interaction.embedding(), last_interaction.embedding()]])       \n",
    "            # On compte les activations\n",
    "            activated_df = self.sequences_df[(self.sequences_df['i1'] == previous_interaction.key()) & (self.sequences_df['i2'] == last_interaction.key())]\n",
    "            count_df = pd.merge(pd.DataFrame({'i3': [i for i in self._interactions]}), activated_df, on='i3', how='left')\n",
    "            count = count_df['count'].fillna(0).astype(int)\n",
    "        else: \n",
    "            probabilities = [0.5] * len(self._interactions) # Assume all interactions are equiprobable\n",
    "            count = np.zeros(len(self._interactions), dtype='int')\n",
    "        # Le dataframe qui donne les expected valence pour chaque interaction\n",
    "        expected_df = pd.DataFrame({\n",
    "            'interaction': [i for i in self._interactions],\n",
    "            'action': [i.action for i in self._interactions.values()],\n",
    "            'outcome': [i.outcome for i in self._interactions.values()],\n",
    "            'valence': [i.valence for i in self._interactions.values()],\n",
    "            'probability': probabilities, \n",
    "            'count': count})\n",
    "        expected_df['expected_valence'] = expected_df['valence'] * expected_df['probability']\n",
    "        return expected_df\n",
    "    \n",
    "    def add_next_intention(self, last_interaction, expected_df):\n",
    "        \"\"\"Add the next best action and expected valence\"\"\"\n",
    "        if last_interaction is not None:\n",
    "            expected_df['next_action'] =  np.zeros(len(self._interactions), dtype='int')\n",
    "            expected_df['next_expected_valence'] = pd.Series(dtype='float')\n",
    "            for row in expected_df.itertuples(index=True):\n",
    "                # Crée un expected_df2 sur la base de last_action et de chaque interaction anticipée\n",
    "                expected_df2 = self.create_expected_df(last_interaction, self._interactions[row.interaction])\n",
    "                expected_df2 = expected_df2.groupby('action').agg({'expected_valence': 'sum'}).reset_index()\n",
    "                # Ajoute la meilleure action suivante et son expected_valence dans expected_df\n",
    "                idxmax = expected_df2['expected_valence'].idxmax()\n",
    "                expected_df.loc[row.Index, 'next_action'] = expected_df2['action'].iloc[idxmax]\n",
    "                expected_df.loc[row.Index, 'next_expected_valence'] = expected_df2['expected_valence'].iloc[idxmax]\n",
    "\n",
    "    \n",
    "    def decide(self):\n",
    "        \"\"\"Decide the intended interaction based on the dataframe of expected valences\"\"\"\n",
    "        # On aggrege par action en sommant l'expected valence\n",
    "        action_expectation_df = self.expected_df.groupby('action').agg({'expected_valence': 'sum', 'count': 'sum'}).reset_index()\n",
    "        # On trie les actions par expected valence décroissante\n",
    "        action_expectation_df['proclivity'] = action_expectation_df['expected_valence'] * action_expectation_df['count']\n",
    "        action_expectation_df = action_expectation_df.sort_values(by=['proclivity'], ascending=[False]).reset_index(drop=True)\n",
    "        print(action_expectation_df)\n",
    "\n",
    "        # Si la plus grande expected valence est négative\n",
    "        if action_expectation_df.loc[0, 'expected_valence'] < 0:  # Note used\n",
    "            self.add_next_intention(self._last_interaction, self.expected_df)\n",
    "            # On cherche une interaction qui mène à une next_expected valence elevée\n",
    "            next_expected_df = self.expected_df[(self.expected_df['probability'] > 0.8) & (self.expected_df['next_expected_valence'] > 0.8)]\n",
    "            if not next_expected_df.empty:\n",
    "                intended_interaction = next_expected_df['interaction'].iloc[0]\n",
    "                print(f\"Intend interaction {intended_interaction} because anticipation\")\n",
    "                return self._interactions[intended_interaction]\n",
    "        \n",
    "        # On sélectionne l'action qui a l'expected valence la plus élevée \n",
    "        intended_action = action_expectation_df.loc[0, 'action']\n",
    "        # Trouve l'outcome le plus probable pour l'action sélectionnée\n",
    "        outcome_df = self.expected_df[self.expected_df['action'] == intended_action]\n",
    "        intended_outcome = outcome_df.loc[outcome_df['probability'].idxmax(), 'outcome']\n",
    "        # On construit l'intended interaction \n",
    "        return self._interactions[intended_action * BASE_ACTION + intended_outcome]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b3f5c-f48c-45ad-bf1d-4a703bac64af",
   "metadata": {},
   "source": [
    "# Testons l'agent dans le Small Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "27ea6056-d93d-4790-a53c-4cbc90861135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd135258070245b3839ea75e5bbc17d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Instanciate the small loop environment\n",
    "e = SmallLoop([1, 1], 0)\n",
    "\n",
    "# Instanciate the agent \n",
    "interactions = [\n",
    "    Interaction(FORWARD,0,5),\n",
    "    Interaction(FORWARD,1,-10),\n",
    "    Interaction(TURN_LEFT,0,-3),\n",
    "    Interaction(TURN_LEFT,1,-3),\n",
    "    Interaction(TURN_RIGHT,0,-3),\n",
    "    Interaction(TURN_RIGHT,1,-3),\n",
    "    Interaction(FEEL_FRONT,0,-1),\n",
    "    Interaction(FEEL_FRONT,1,-1),\n",
    "    Interaction(FEEL_LEFT,0,-1),\n",
    "    Interaction(FEEL_LEFT,1,-1),\n",
    "    Interaction(FEEL_RIGHT,0,-1),\n",
    "    Interaction(FEEL_RIGHT,1,-1)\n",
    "]\n",
    "a = Agent(interactions)\n",
    "\n",
    "# Initialize the interaction loop\n",
    "step = 0\n",
    "outcome = 0\n",
    "\n",
    "# Display\n",
    "out = Output()\n",
    "e.display()\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "120667c5-b435-4149-a6b2-7f86b56f9bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 178\n",
      "Action: 1, Prediction: 0, Outcome: 1, Prediction_correct: False, Valence: -1)\n",
      "acc : 0.397 = 54/136 for epoch 49\n",
      "   action  expected_valence  count  proclivity\n",
      "0       1         -1.000000      0   -0.000000\n",
      "1       2         -1.000000      0   -0.000000\n",
      "2       4         -3.000000      0   -0.000000\n",
      "3       3         -1.000000      0   -0.000000\n",
      "4       5         -3.000000      0   -0.000000\n",
      "5       0         -9.999735      1   -9.999735\n",
      "Line: 3, Column: 4, direction: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction</th>\n",
       "      <th>action</th>\n",
       "      <th>outcome</th>\n",
       "      <th>valence</th>\n",
       "      <th>probability</th>\n",
       "      <th>count</th>\n",
       "      <th>expected_valence</th>\n",
       "      <th>next_action</th>\n",
       "      <th>next_expected_valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.999824</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.042569</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.042569</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.957431</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.957431</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.086089</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.086089</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.913911</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.913911</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.987054</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.987054</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.012946</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012946</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.999105</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.997314</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.999156</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.997469</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.002531</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    interaction  action  outcome  valence  probability  count  \\\n",
       "0             0       0        0        5     0.000018      0   \n",
       "1             1       0        1      -10     0.999982      1   \n",
       "2             2       1        0       -1     0.042569      0   \n",
       "3             3       1        1       -1     0.957431      0   \n",
       "4             4       2        0       -1     0.086089      0   \n",
       "5             5       2        1       -1     0.913911      0   \n",
       "6             6       3        0       -1     0.987054      0   \n",
       "7             7       3        1       -1     0.012946      0   \n",
       "8             8       4        0       -3     0.999105      0   \n",
       "9             9       4        1       -3     0.000895      0   \n",
       "10           10       5        0       -3     0.999156      0   \n",
       "11           11       5        1       -3     0.000844      0   \n",
       "\n",
       "    expected_valence  next_action  next_expected_valence  \n",
       "0           0.000088            1                   -1.0  \n",
       "1          -9.999824            1                   -1.0  \n",
       "2          -0.042569            1                   -1.0  \n",
       "3          -0.957431            3                   -1.0  \n",
       "4          -0.086089            1                   -1.0  \n",
       "5          -0.913911            3                   -1.0  \n",
       "6          -0.987054            3                   -1.0  \n",
       "7          -0.012946            2                   -1.0  \n",
       "8          -2.997314            1                   -1.0  \n",
       "9          -0.002686            3                   -1.0  \n",
       "10         -2.997469            2                   -1.0  \n",
       "11         -0.002531            3                   -1.0  "
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step: {step}\")\n",
    "step += 1\n",
    "action = a.action(outcome)\n",
    "e.display()\n",
    "e.save(step)  # Save the image file \n",
    "e.clear(True)  \n",
    "outcome = e.outcome(action)\n",
    "#a.sequences_df\n",
    "a.expected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae66437-afb3-472d-b046-1a498e6dee84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
